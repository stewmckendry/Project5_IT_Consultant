{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: Setup and Imports** <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import pytest\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Shared Functions From Project Folder** <a id=\"1.1\"></a>\n",
    "We will import the shared functions from the project folder that we built for IT Consultant Report Reviewer with ReAct. This is done to keep the notebook clean and organized. The functions are used for data loading, preprocessing, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liammckendry/spacy_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Adjust the path if your notebook is in /notebooks and src/ is a sibling\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(os.path.join(project_root))\n",
    "\n",
    "# Core OpenAI API\n",
    "from src.models.openai_interface import call_openai_with_tracking\n",
    "\n",
    "# ReAct reasoning agent and tools\n",
    "from src.server.react_agent import (\n",
    "    ReActConsultantAgent,\n",
    "    run_react_loop_check_withTool,\n",
    "    dispatch_tool_action, \n",
    "    select_best_tool_with_llm,\n",
    "    run_react_loop_for_rfp_eval\n",
    ")\n",
    "\n",
    "# Proposal orchestration (ToT + ReAct hybrid)\n",
    "#from src.server.report_review_runner import (\n",
    "#    summarize_and_score_section\n",
    "#)\n",
    "\n",
    "# Prompt builders and tool descriptions\n",
    "from src.server.prompt_builders import (\n",
    "    build_tool_hints,\n",
    "    format_tool_catalog_for_prompt\n",
    ")\n",
    "\n",
    "# Proposal scoring and analysis\n",
    "from src.models.scoring import summarize_and_score_section\n",
    "\n",
    "# LLM-based section tools (for evaluation scoring)\n",
    "from src.models.section_tools_llm import (\n",
    "    should_cite,\n",
    "    auto_fill_gaps_with_research,\n",
    "    upgrade_section_with_research,\n",
    "    make_text_coherent,\n",
    "    generate_final_summary,\n",
    "    format_upgraded_sections\n",
    ")\n",
    "\n",
    "# File loading utility (later use for multi-proposal)\n",
    "from src.utils.file_loader import load_scenario_data\n",
    "\n",
    "# Text processing (optional for parsing requirements/proposals)\n",
    "#from src.utils.text_processing import (\n",
    "#    split_report_into_sections,\n",
    "#    map_section_to_canonical,\n",
    "#    guess_canonical_section_with_llm\n",
    "#)\n",
    "\n",
    "# Visualization (optional for tool analysis)\n",
    "from src.utils.visualization import (\n",
    "    print_tool_usage,\n",
    "    plot_tool_usage\n",
    ")\n",
    "\n",
    "# Exporting to markdown + PDF\n",
    "#from src.utils.export_utils import (\n",
    "#    export_report_to_markdown,\n",
    "#    export_report_to_markdown_and_pdf,\n",
    "#    show_agent_memory\n",
    "#)\n",
    "\n",
    "# Basic tool logic\n",
    "from src.utils.tools.tools_basic import (\n",
    "    check_guideline_dynamic,\n",
    "    keyword_match_in_section,\n",
    "    search_report,\n",
    "    generate_client_questions\n",
    ")\n",
    "\n",
    "# Web tools\n",
    "from src.utils.tools.tools_web import (\n",
    "    search_web,\n",
    "    search_serpapi,\n",
    "    search_wikipedia,\n",
    "    search_arxiv,\n",
    "    should_search_arxiv\n",
    ")\n",
    "\n",
    "# NLP tools\n",
    "from src.utils.tools.tools_nlp import (\n",
    "    check_for_jargon,\n",
    "    check_readability,\n",
    "    analyze_tone_textblob,\n",
    "    extract_named_entities\n",
    ")\n",
    "\n",
    "# Reasoning tools\n",
    "from src.utils.tools.tools_reasoning import (\n",
    "    pick_tool_by_intent,\n",
    "    pick_tool_by_intent_fuzzy,\n",
    "    categorize_tools_by_priority,\n",
    "    analyze_math_question,\n",
    ")\n",
    "\n",
    "# LLM-based tools (LLM - thought generator)\n",
    "from src.models.tot_agent import generate_thoughts_openai\n",
    "\n",
    "from src.utils.tools.tool_embeddings import (\n",
    "    build_tool_embeddings, \n",
    "    suggest_tools_by_embedding\n",
    ")\n",
    "\n",
    "from src.models.openai_embeddings import get_openai_embedding\n",
    "\n",
    "from src.utils.tools.tool_catalog_RFP import tool_catalog\n",
    "\n",
    "from src.utils.export_utils import export_proposal_report_to_markdown, convert_markdown_to_pdf\n",
    "\n",
    "from src.server.proposal_eval import evaluate_proposal\n",
    "\n",
    "from src.server.multi_agent_rfpevalrunner import run_multi_proposal_evaluation\n",
    "\n",
    "from src.server.final_eval_summary import generate_final_comparison_summary\n",
    "\n",
    "from src.utils.export_utils import save_markdown_and_pdf\n",
    "\n",
    "from src.utils.file_loader import load_proposals_from_folder, load_rfp_criteria, load_default_scenario, list_available_scenarios\n",
    "\n",
    "from src.utils.logging_utils import (\n",
    "    setup_logging, \n",
    "    display_log, \n",
    "    log_phase, \n",
    "    tool_stats, \n",
    "    thought_score_stats, \n",
    "    openai_call_counter\n",
    ")\n",
    "\n",
    "from src.utils.logging_reports import finalize_evaluation_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logging setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_magic\n",
    "import logging\n",
    "\n",
    "@register_line_magic\n",
    "def loglevel(level):\n",
    "    \"\"\"Set the log level for the ProposalEvaluator logger.\"\"\"\n",
    "    logger = logging.getLogger(\"ProposalEvaluator\")\n",
    "    level = level.upper()\n",
    "    if level in (\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"):\n",
    "        logger.setLevel(getattr(logging, level))\n",
    "        print(f\"üîß Log level set to {level}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid log level: {level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:32:41] [INFO] üìå Logging initialized\n",
      "[18:32:41] [INFO] üìå Log file: logs/eval_run_2025-04-01_18-32-41.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìé Handler: StreamHandler\n",
      "üìé Handler: FileHandler\n",
      "/Users/liammckendry/Project5_IT_Consultant/notebooks/logs/eval_run_2025-04-01_18-32-41.log\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"logs/eval_run_{timestamp}.log\"\n",
    "logger = setup_logging(log_filename)\n",
    "\n",
    "# Print log handlers (stream: console/notebook, file: file)\n",
    "for h in logger.handlers:\n",
    "    print(f\"üìé Handler: {type(h).__name__}\")\n",
    "\n",
    "# Display the log file path\n",
    "print(Path(log_filename).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Log level set to DEBUG\n",
      "Current log level: DEBUG\n"
     ]
    }
   ],
   "source": [
    "%loglevel DEBUG\n",
    "print(f\"Current log level: {logging.getLevelName(logger.level)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Embeddings of Tool Catalog** <a id=\"1.1\"></a>\n",
    "First, we need to load the tool catalog and create text embeddings for each tool. This will allow us to compare the user's query with the tools available in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:32:41] [INFO] üìå ‚öôÔ∏è Generating tool embeddings...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'CreateEmbeddingResponse' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# One-time setup\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tool_embeddings = \u001b[43mbuild_tool_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_catalog\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project5_IT_Consultant/src/utils/tools/tool_embeddings.py:28\u001b[39m, in \u001b[36mbuild_tool_embeddings\u001b[39m\u001b[34m(tool_catalog, cache_path)\u001b[39m\n\u001b[32m     26\u001b[39m     examples = meta.get(\u001b[33m\"\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     27\u001b[39m     text = description + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(examples)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     embedding = \u001b[43mget_openai_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     tool_embeddings[name] = embedding\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project5_IT_Consultant/src/models/openai_embeddings.py:14\u001b[39m, in \u001b[36mget_openai_embedding\u001b[39m\u001b[34m(text, model)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_openai_embedding\u001b[39m(text, model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-ada-002\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     10\u001b[39m     response = client.embeddings.create(\n\u001b[32m     11\u001b[39m         model=model,\n\u001b[32m     12\u001b[39m         \u001b[38;5;28minput\u001b[39m=[text]\n\u001b[32m     13\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mlog_openai_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.data[\u001b[32m0\u001b[39m].embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project5_IT_Consultant/src/utils/logging_utils.py:58\u001b[39m, in \u001b[36mlog_openai_call\u001b[39m\u001b[34m(prompt, response, source, prompt_tokens, completion_tokens)\u001b[39m\n\u001b[32m     54\u001b[39m openai_prompt_token_usage_by_source[source] += prompt_tokens\n\u001b[32m     55\u001b[39m openai_completion_token_usage_by_source[source] += completion_tokens\n\u001b[32m     56\u001b[39m logger.debug(\n\u001b[32m     57\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ OpenAI call #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenai_call_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m... -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m... \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(Prompt tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Completion tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: 'CreateEmbeddingResponse' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# One-time setup\n",
    "tool_embeddings = build_tool_embeddings(tool_catalog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Functions** <a id=\"1\"></a>\n",
    "\n",
    "Only new ones built for RFP Evaluation (ToT+ReACT Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_evaluation_report(results):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Formats a full evaluation report from a list of Tree of Thought (ToT) results into a markdown string.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): A list of evaluation results, where each result is a dictionary containing:\n",
    "        - criterion (str): The evaluation criterion.\n",
    "        - score (int): The score assigned to the criterion (1‚Äì10).\n",
    "        - reasoning_path (list of str): The reasoning path (thoughts) generated during the evaluation.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a markdown report string with a title.\n",
    "    2. Iterates through the `results` list:\n",
    "        - Extracts the criterion, score, and reasoning path for each result.\n",
    "        - Appends the criterion, score, and reasoning path to the report in markdown format.\n",
    "        - Accumulates the total score for calculating the average.\n",
    "    3. Computes the average score across all criteria.\n",
    "    4. Appends the overall average score to the report.\n",
    "    5. Returns the formatted markdown report string.\n",
    "\n",
    "    Returns:\n",
    "    - str: A markdown-formatted string representing the evaluation report.\n",
    "    \"\"\"\n",
    "    report = \"# üìÑ Proposal Evaluation Report\\n\\n\"\n",
    "    total_score = 0\n",
    "\n",
    "    for result in results:\n",
    "        criterion = result[\"criterion\"]\n",
    "        score = result[\"score\"]\n",
    "        thoughts = result[\"reasoning_path\"]\n",
    "        total_score += score\n",
    "\n",
    "        report += f\"## {criterion}\\n\"\n",
    "        report += f\"**Score**: {score}/10\\n\\n\"\n",
    "        report += \"**Reasoning Path:**\\n\\n\"\n",
    "        for i, t in enumerate(thoughts, 1):\n",
    "            report += f\"{i}. {t}\\n\\n\"\n",
    "        report += \"\\n\"\n",
    "\n",
    "    avg_score = round(total_score / len(results), 2)\n",
    "    report += f\"---\\n\\n**üßæ Overall Average Score:** {avg_score}/10\\n\"\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_report_to_markdown_tot(report_text, filename=\"proposal_evaluation_report.md\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Exports a Tree of Thought (ToT) evaluation report to a markdown (.md) file.\n",
    "\n",
    "    Parameters:\n",
    "    - report_text (str): The content of the evaluation report in markdown format.\n",
    "    - filename (str): The name of the markdown file to save the report to. Default is \"proposal_evaluation_report.md\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Opens the specified file in write mode with UTF-8 encoding.\n",
    "    2. Writes the provided `report_text` to the file.\n",
    "    3. Prints a confirmation message with the file path.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value. It saves the report to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "    print(f\"‚úÖ Markdown report saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def export_tot_report_to_markdown_and_pdf(\n",
    "    report_md_text,\n",
    "    markdown_file=\"proposal_evaluation_report.md\",\n",
    "    pdf_file=\"proposal_evaluation_report.pdf\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Exports a Tree of Thought (ToT) evaluation report (provided as a markdown string) to both `.md` and `.pdf` formats.\n",
    "\n",
    "    Parameters:\n",
    "    - report_md_text (str): The content of the evaluation report in markdown format.\n",
    "    - markdown_file (str): The name of the markdown file to save the report to. Default is \"proposal_evaluation_report.md\".\n",
    "    - pdf_file (str): The name of the PDF file to save the report to. Default is \"proposal_evaluation_report.pdf\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Ensures the output directory exists (`../outputs/` relative to the current working directory).\n",
    "    2. Saves the markdown content to a `.md` file in the output directory.\n",
    "    3. Converts the markdown content to HTML using the `markdown` library.\n",
    "    4. Wraps the HTML content in a basic HTML template with styling.\n",
    "    5. Saves the HTML content to a temporary `.html` file in the output directory.\n",
    "    6. Uses Playwright to render the HTML file and export it as a PDF.\n",
    "    7. Handles any exceptions during the PDF export process and logs an error message if it fails.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value. It saves the report to `.md` and `.pdf` files in the output directory.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_proposal_content_with_llm(proposal, criterion, top_thoughts=None, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Scores how well the proposal meets a specific RFP criterion, optionally guided by Tree of Thought (ToT) thoughts.\n",
    "\n",
    "    Parameters:\n",
    "    - proposal (str): The text of the vendor proposal being evaluated.\n",
    "    - criterion (str): The evaluation criterion against which the proposal is being assessed.\n",
    "    - top_thoughts (list of str, optional): A list of key thoughts or considerations generated during the evaluation process. Default is None.\n",
    "    - model (str): The name of the OpenAI model to use for scoring. Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that includes the proposal text, evaluation criterion, and optionally, key thoughts.\n",
    "    2. Sends the prompt to the OpenAI API using the `call_openai_with_tracking` function.\n",
    "    3. Parses the response to extract the score and explanation.\n",
    "    4. If parsing fails, defaults to a fallback score of 5 and a generic explanation.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - score (int): The score assigned to the proposal, ranging from 1 to 10.\n",
    "        - explanation (str): The reasoning behind the assigned score.\n",
    "    \"\"\"\n",
    "    thoughts_text = \"\"\n",
    "    if top_thoughts:\n",
    "        thoughts_text = \"\\nHere are some important considerations:\\n\" + \"\\n\".join(f\"- {t}\" for t in top_thoughts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are evaluating a vendor proposal on the criterion: **{criterion}**.\n",
    "\n",
    "Proposal:\n",
    "\\\"\\\"\\\"\n",
    "{proposal}\n",
    "\\\"\\\"\\\"\n",
    "{thoughts_text}\n",
    "\n",
    "Based on the proposal and the evaluation criteria above, assign a score from 1 to 10.\n",
    "\n",
    "Respond in this format:\n",
    "Score: X\n",
    "Explanation: (why this score)\n",
    "\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = call_openai_with_tracking(messages, model=model, temperature=0)\n",
    "\n",
    "    try:\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        score_line = next((l for l in lines if \"Score:\" in l), \"Score: 5\")\n",
    "        explanation = next((l for l in lines if \"Explanation:\" in l), \"Explanation: No explanation found.\")\n",
    "        score = int(score_line.split(\":\")[1].strip())\n",
    "        explanation = explanation.split(\":\", 1)[1].strip()\n",
    "        return score, explanation\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to parse score: {str(e)}\")\n",
    "        return 5, \"Failed to parse explanation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_proposal_evaluation(results, overall_score, swot_summary):\n",
    "    \"\"\"\n",
    "    Nicely prints the full evaluation for a proposal, including:\n",
    "    - Criterion-level scores and reasoning\n",
    "    - ToT thoughts\n",
    "    - Tools used and results\n",
    "    - Final overall score\n",
    "    - SWOT summary\n",
    "    \"\"\"\n",
    "    print(\"\\n====================\")\n",
    "    print(\"üìä PROPOSAL EVALUATION\")\n",
    "    print(\"====================\\n\")\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"üîπ Criterion: {result['criterion']}\")\n",
    "        print(f\"üìà Score: {result['proposal_score']}/10\")\n",
    "        print(f\"üß† Thoughts:\")\n",
    "        for t in result[\"reasoning_path\"]:\n",
    "            print(f\"   ‚Ä¢ {t}\")\n",
    "        print(\"üõ†Ô∏è Tools Used:\")\n",
    "        for tool in result[\"triggered_tools\"]:\n",
    "            print(f\"   ‚Ä¢ {tool['tool']}: {tool['result']}\")\n",
    "        print(f\"üó£Ô∏è Explanation: {result['proposal_explanation']}\")\n",
    "        print(\"\\n--------------------\\n\")\n",
    "\n",
    "    print(f\"‚úÖ OVERALL SCORE: {overall_score}/10\\n\")\n",
    "    print(\"üìã SWOT ASSESSMENT:\\n\")\n",
    "    print(swot_summary)\n",
    "    print(\"\\n====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Load RFP Data** <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use Case ---\n",
    "use_case = \"\"\"\n",
    "A public sector organization is seeking a new cloud-based Electronic Health Record (EHR) system. \n",
    "They've issued an RFP to multiple technology vendors. Each proposal must address:\n",
    "- Functional fit to healthcare workflows\n",
    "- Technical architecture\n",
    "- Cost structure\n",
    "- Implementation timeline\n",
    "- Vendor experience\n",
    "- Risk management\n",
    "\n",
    "As a first step, we want our AI agent to evaluate a single proposal using Tree of Thought reasoning.\n",
    "\"\"\"\n",
    "\n",
    "# --- Evaluation Criteria ---\n",
    "rfp_criteria = [\n",
    "    \"Solution Fit\",\n",
    "    \"Technical Architecture\",\n",
    "    \"Cost\",\n",
    "    \"Implementation Timeline\",\n",
    "    \"Vendor Experience\",\n",
    "    \"Risk Management\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfp_path = \"../data/rfps/sample_rfp.txt\"\n",
    "rfp_criteria = load_rfp_criteria(rfp_path)\n",
    "print(\"‚úÖ RFP Criteria Loaded:\")\n",
    "print(rfp_criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load proposals from folder\n",
    "dummy_proposals = load_proposals_from_folder(\"../data/proposals/\")\n",
    "\n",
    "print(f\"Loaded {len(dummy_proposals)} proposals.\")\n",
    "print(dummy_proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfp_path = \"../data/rfps/sample_rfp.txt\"\n",
    "rfp_criteria = load_rfp_criteria(rfp_path)\n",
    "print(\"‚úÖ RFP Criteria Loaded:\")\n",
    "print(rfp_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 4: Single Agent - Evaluate One Proposal** <a id=\"4\"></a>\n",
    "**Run Hybrid ToT + ReAct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run evaluation for single proposal\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    results_a, overall_score_a, swot_summary_a = evaluate_proposal(proposal_a, rfp_criteria)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Execution skipped. Set `execute_cell = True` to run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Print single proposal report after getting results:\n",
    "    md_path = export_proposal_report_to_markdown(results_a, overall_score_a, swot_summary_a, proposal_name=\"VendorA\")\n",
    "    pdf_path = convert_markdown_to_pdf(md_path)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {md_path}\")\n",
    "    print(f\"‚úÖ Saved: {pdf_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Execution skipped. Set `execute_cell = True` to run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 5: Multi-Agent - Evaluate Multiple Proposals** <a id=\"5\"></a>\n",
    "**Display Evaluation Metrics & Export to MD, HTML and PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple proposals with multi-agent evaluation\n",
    "execute_cell = True\n",
    "bypass_existing_file = True  # Set to True to bypass the existing file and re-run evaluation\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"../outputs/proposal_eval_reports/all_vendor_evaluations.json\"\n",
    "\n",
    "if execute_cell:\n",
    "    if not bypass_existing_file and os.path.exists(output_file):\n",
    "        # Load existing evaluation results from file\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_vendor_evals = json.load(f)\n",
    "        log_phase(f\"‚úÖ Loaded existing vendor evaluations from: {output_file}\")\n",
    "    else:\n",
    "        # Load proposals from folder\n",
    "        proposals, rfp_path = load_default_scenario(\"scenario1_basic\")\n",
    "        all_vendor_evals, summary_text = run_multi_proposal_evaluation(proposals, rfp_file=rfp_path)\n",
    "        \n",
    "        # Write the evaluation results to a file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_vendor_evals, f, indent=4, ensure_ascii=False)\n",
    "        log_phase(f\"‚úÖ All vendor evaluations saved to: {output_file}\")\n",
    "        log_phase(f\"‚úÖ Summary of evaluations: {summary_text}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Execution skipped. Set `execute_cell = True` to run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tool stats: {tool_stats}\") # Logger tool_stats\n",
    "print(f\"Thought score stats: {thought_score_stats}\") # Logger thought_score_stats\n",
    "print(f\"OpenAI call counter: {openai_call_counter}\") # Logger openai_call_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a log summary report\n",
    "finalize_evaluation_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = list_available_scenarios()\n",
    "print(\"Available rfp scenarios:\", scenarios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
