{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: Setup and Imports** <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Shared Functions From Project Folder** <a id=\"1.1\"></a>\n",
    "We will import the shared functions from the project folder that we built for IT Consultant Report Reviewer with ReAct. This is done to keep the notebook clean and organized. The functions are used for data loading, preprocessing, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Adjust the path if your notebook is in /notebooks and src/ is a sibling\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(os.path.join(project_root))\n",
    "\n",
    "# Core OpenAI API\n",
    "from src.models.openai_interface import call_openai_with_tracking\n",
    "\n",
    "# ReAct reasoning agent and tools\n",
    "from src.server.react_agent import (\n",
    "    ReActConsultantAgent,\n",
    "    run_react_loop_check_withTool,\n",
    "    dispatch_tool_action, \n",
    "    select_best_tool_with_llm,\n",
    "    run_react_loop_for_rfp_eval\n",
    ")\n",
    "\n",
    "# Proposal orchestration (ToT + ReAct hybrid)\n",
    "#from src.server.report_review_runner import (\n",
    "#    summarize_and_score_section\n",
    "#)\n",
    "\n",
    "# Prompt builders and tool descriptions\n",
    "from src.server.prompt_builders import (\n",
    "    build_tool_hints,\n",
    "    format_tool_catalog_for_prompt\n",
    ")\n",
    "\n",
    "# Proposal scoring and analysis\n",
    "from src.models.scoring import summarize_and_score_section\n",
    "\n",
    "# LLM-based section tools (for evaluation scoring)\n",
    "from src.models.section_tools_llm import (\n",
    "    should_cite,\n",
    "    auto_fill_gaps_with_research,\n",
    "    upgrade_section_with_research,\n",
    "    make_text_coherent,\n",
    "    generate_final_summary,\n",
    "    format_upgraded_sections\n",
    ")\n",
    "\n",
    "# File loading utility (later use for multi-proposal)\n",
    "from src.utils.file_loader import load_report_text_from_file\n",
    "\n",
    "# Text processing (optional for parsing requirements/proposals)\n",
    "#from src.utils.text_processing import (\n",
    "#    split_report_into_sections,\n",
    "#    map_section_to_canonical,\n",
    "#    guess_canonical_section_with_llm\n",
    "#)\n",
    "\n",
    "# Visualization (optional for tool analysis)\n",
    "from src.utils.visualization import (\n",
    "    print_tool_usage,\n",
    "    plot_tool_usage\n",
    ")\n",
    "\n",
    "# Exporting to markdown + PDF\n",
    "#from src.utils.export_utils import (\n",
    "#    export_report_to_markdown,\n",
    "#    export_report_to_markdown_and_pdf,\n",
    "#    show_agent_memory\n",
    "#)\n",
    "\n",
    "# Basic tool logic\n",
    "from src.utils.tools.tools_basic import (\n",
    "    check_guideline_dynamic,\n",
    "    keyword_match_in_section,\n",
    "    check_timeline_feasibility,\n",
    "    search_report,\n",
    "    highlight_missing_sections,\n",
    "    check_alignment_with_goals,\n",
    "    compare_with_other_section,\n",
    "    generate_client_questions\n",
    ")\n",
    "\n",
    "# Web tools\n",
    "from src.utils.tools.tools_web import (\n",
    "    search_web,\n",
    "    search_serpapi,\n",
    "    search_wikipedia,\n",
    "    search_arxiv,\n",
    "    should_search_arxiv\n",
    ")\n",
    "\n",
    "# NLP tools\n",
    "from src.utils.tools.tools_nlp import (\n",
    "    check_for_jargon,\n",
    "    check_readability,\n",
    "    analyze_tone_textblob,\n",
    "    extract_named_entities\n",
    ")\n",
    "\n",
    "# Reasoning tools\n",
    "from src.utils.tools.tools_reasoning import (\n",
    "    pick_tool_by_intent,\n",
    "    pick_tool_by_intent_fuzzy,\n",
    "    categorize_tools_by_priority,\n",
    "    analyze_math_question,\n",
    ")\n",
    "\n",
    "# LLM-based tools (LLM - thought generator)\n",
    "from src.models.tot_agent import generate_thoughts_openai\n",
    "\n",
    "from src.utils.tools.tool_embeddings import (\n",
    "    build_tool_embeddings, \n",
    "    suggest_tools_by_embedding\n",
    ")\n",
    "\n",
    "from src.models.openai_embeddings import get_openai_embedding\n",
    "\n",
    "from src.utils.tools.tool_catalog import tool_catalog\n",
    "\n",
    "from src.utils.export_utils import export_proposal_report_to_markdown, convert_markdown_to_pdf\n",
    "\n",
    "from src.server.proposal_eval import evaluate_proposal\n",
    "\n",
    "from src.server.multi_agent_rfpevalrunner import run_multi_proposal_evaluation\n",
    "\n",
    "from src.server.final_eval_summary import generate_final_comparison_summary\n",
    "\n",
    "from src.utils.export_utils import save_markdown_and_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Embeddings of Tool Catalog** <a id=\"1.1\"></a>\n",
    "First, we need to load the tool catalog and create text embeddings for each tool. This will allow us to compare the user's query with the tools available in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time setup\n",
    "tool_embeddings = build_tool_embeddings(tool_catalog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Functions** <a id=\"1\"></a>\n",
    "\n",
    "Only new ones built for RFP Evaluation (ToT+ReACT Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_evaluation_report(results):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Formats a full evaluation report from a list of Tree of Thought (ToT) results into a markdown string.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): A list of evaluation results, where each result is a dictionary containing:\n",
    "        - criterion (str): The evaluation criterion.\n",
    "        - score (int): The score assigned to the criterion (1–10).\n",
    "        - reasoning_path (list of str): The reasoning path (thoughts) generated during the evaluation.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a markdown report string with a title.\n",
    "    2. Iterates through the `results` list:\n",
    "        - Extracts the criterion, score, and reasoning path for each result.\n",
    "        - Appends the criterion, score, and reasoning path to the report in markdown format.\n",
    "        - Accumulates the total score for calculating the average.\n",
    "    3. Computes the average score across all criteria.\n",
    "    4. Appends the overall average score to the report.\n",
    "    5. Returns the formatted markdown report string.\n",
    "\n",
    "    Returns:\n",
    "    - str: A markdown-formatted string representing the evaluation report.\n",
    "    \"\"\"\n",
    "    report = \"# 📄 Proposal Evaluation Report\\n\\n\"\n",
    "    total_score = 0\n",
    "\n",
    "    for result in results:\n",
    "        criterion = result[\"criterion\"]\n",
    "        score = result[\"score\"]\n",
    "        thoughts = result[\"reasoning_path\"]\n",
    "        total_score += score\n",
    "\n",
    "        report += f\"## {criterion}\\n\"\n",
    "        report += f\"**Score**: {score}/10\\n\\n\"\n",
    "        report += \"**Reasoning Path:**\\n\\n\"\n",
    "        for i, t in enumerate(thoughts, 1):\n",
    "            report += f\"{i}. {t}\\n\\n\"\n",
    "        report += \"\\n\"\n",
    "\n",
    "    avg_score = round(total_score / len(results), 2)\n",
    "    report += f\"---\\n\\n**🧾 Overall Average Score:** {avg_score}/10\\n\"\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_report_to_markdown_tot(report_text, filename=\"proposal_evaluation_report.md\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Exports a Tree of Thought (ToT) evaluation report to a markdown (.md) file.\n",
    "\n",
    "    Parameters:\n",
    "    - report_text (str): The content of the evaluation report in markdown format.\n",
    "    - filename (str): The name of the markdown file to save the report to. Default is \"proposal_evaluation_report.md\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Opens the specified file in write mode with UTF-8 encoding.\n",
    "    2. Writes the provided `report_text` to the file.\n",
    "    3. Prints a confirmation message with the file path.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value. It saves the report to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "    print(f\"✅ Markdown report saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def export_tot_report_to_markdown_and_pdf(\n",
    "    report_md_text,\n",
    "    markdown_file=\"proposal_evaluation_report.md\",\n",
    "    pdf_file=\"proposal_evaluation_report.pdf\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Exports a Tree of Thought (ToT) evaluation report (provided as a markdown string) to both `.md` and `.pdf` formats.\n",
    "\n",
    "    Parameters:\n",
    "    - report_md_text (str): The content of the evaluation report in markdown format.\n",
    "    - markdown_file (str): The name of the markdown file to save the report to. Default is \"proposal_evaluation_report.md\".\n",
    "    - pdf_file (str): The name of the PDF file to save the report to. Default is \"proposal_evaluation_report.pdf\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Ensures the output directory exists (`../outputs/` relative to the current working directory).\n",
    "    2. Saves the markdown content to a `.md` file in the output directory.\n",
    "    3. Converts the markdown content to HTML using the `markdown` library.\n",
    "    4. Wraps the HTML content in a basic HTML template with styling.\n",
    "    5. Saves the HTML content to a temporary `.html` file in the output directory.\n",
    "    6. Uses Playwright to render the HTML file and export it as a PDF.\n",
    "    7. Handles any exceptions during the PDF export process and logs an error message if it fails.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value. It saves the report to `.md` and `.pdf` files in the output directory.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_proposal_content_with_llm(proposal, criterion, top_thoughts=None, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Scores how well the proposal meets a specific RFP criterion, optionally guided by Tree of Thought (ToT) thoughts.\n",
    "\n",
    "    Parameters:\n",
    "    - proposal (str): The text of the vendor proposal being evaluated.\n",
    "    - criterion (str): The evaluation criterion against which the proposal is being assessed.\n",
    "    - top_thoughts (list of str, optional): A list of key thoughts or considerations generated during the evaluation process. Default is None.\n",
    "    - model (str): The name of the OpenAI model to use for scoring. Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that includes the proposal text, evaluation criterion, and optionally, key thoughts.\n",
    "    2. Sends the prompt to the OpenAI API using the `call_openai_with_tracking` function.\n",
    "    3. Parses the response to extract the score and explanation.\n",
    "    4. If parsing fails, defaults to a fallback score of 5 and a generic explanation.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - score (int): The score assigned to the proposal, ranging from 1 to 10.\n",
    "        - explanation (str): The reasoning behind the assigned score.\n",
    "    \"\"\"\n",
    "    thoughts_text = \"\"\n",
    "    if top_thoughts:\n",
    "        thoughts_text = \"\\nHere are some important considerations:\\n\" + \"\\n\".join(f\"- {t}\" for t in top_thoughts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are evaluating a vendor proposal on the criterion: **{criterion}**.\n",
    "\n",
    "Proposal:\n",
    "\\\"\\\"\\\"\n",
    "{proposal}\n",
    "\\\"\\\"\\\"\n",
    "{thoughts_text}\n",
    "\n",
    "Based on the proposal and the evaluation criteria above, assign a score from 1 to 10.\n",
    "\n",
    "Respond in this format:\n",
    "Score: X\n",
    "Explanation: (why this score)\n",
    "\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = call_openai_with_tracking(messages, model=model, temperature=0)\n",
    "\n",
    "    try:\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        score_line = next((l for l in lines if \"Score:\" in l), \"Score: 5\")\n",
    "        explanation = next((l for l in lines if \"Explanation:\" in l), \"Explanation: No explanation found.\")\n",
    "        score = int(score_line.split(\":\")[1].strip())\n",
    "        explanation = explanation.split(\":\", 1)[1].strip()\n",
    "        return score, explanation\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to parse score: {str(e)}\")\n",
    "        return 5, \"Failed to parse explanation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_proposal_evaluation(results, overall_score, swot_summary):\n",
    "    \"\"\"\n",
    "    Nicely prints the full evaluation for a proposal, including:\n",
    "    - Criterion-level scores and reasoning\n",
    "    - ToT thoughts\n",
    "    - Tools used and results\n",
    "    - Final overall score\n",
    "    - SWOT summary\n",
    "    \"\"\"\n",
    "    print(\"\\n====================\")\n",
    "    print(\"📊 PROPOSAL EVALUATION\")\n",
    "    print(\"====================\\n\")\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"🔹 Criterion: {result['criterion']}\")\n",
    "        print(f\"📈 Score: {result['proposal_score']}/10\")\n",
    "        print(f\"🧠 Thoughts:\")\n",
    "        for t in result[\"reasoning_path\"]:\n",
    "            print(f\"   • {t}\")\n",
    "        print(\"🛠️ Tools Used:\")\n",
    "        for tool in result[\"triggered_tools\"]:\n",
    "            print(f\"   • {tool['tool']}: {tool['result']}\")\n",
    "        print(f\"🗣️ Explanation: {result['proposal_explanation']}\")\n",
    "        print(\"\\n--------------------\\n\")\n",
    "\n",
    "    print(f\"✅ OVERALL SCORE: {overall_score}/10\\n\")\n",
    "    print(\"📋 SWOT ASSESSMENT:\\n\")\n",
    "    print(swot_summary)\n",
    "    print(\"\\n====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Load RFP Data** <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use Case ---\n",
    "use_case = \"\"\"\n",
    "A public sector organization is seeking a new cloud-based Electronic Health Record (EHR) system. \n",
    "They've issued an RFP to multiple technology vendors. Each proposal must address:\n",
    "- Functional fit to healthcare workflows\n",
    "- Technical architecture\n",
    "- Cost structure\n",
    "- Implementation timeline\n",
    "- Vendor experience\n",
    "- Risk management\n",
    "\n",
    "As a first step, we want our AI agent to evaluate a single proposal using Tree of Thought reasoning.\n",
    "\"\"\"\n",
    "\n",
    "# --- Evaluation Criteria ---\n",
    "rfp_criteria = [\n",
    "    \"Solution Fit\",\n",
    "    \"Technical Architecture\",\n",
    "    \"Cost\",\n",
    "    \"Implementation Timeline\",\n",
    "    \"Vendor Experience\",\n",
    "    \"Risk Management\",\n",
    "]\n",
    "\n",
    "# --- Sample Proposal (Vendor A) ---\n",
    "proposal_a = \"\"\"\n",
    "Vendor A proposes a modular, cloud-native EHR platform with configurable workflows for clinics and hospitals. \n",
    "The platform supports FHIR interoperability, role-based access, and integration with provincial health registries.\n",
    "The implementation will follow a phased approach over 18 months. Initial go-live includes primary care, with \n",
    "specialty modules added later.\n",
    "\n",
    "The vendor has delivered similar solutions in two provinces and offers a dedicated implementation team. \n",
    "They note some assumptions: client to provide system integration support, legacy data migration, and end-user training. \n",
    "Pricing is tiered by user volume and includes hosting, support, and upgrades.\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Use Case and Sample Proposal loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_proposals = {\n",
    "    \"Vendor A\": \"\"\"\n",
    "Vendor A proposes a modular, cloud-native EHR platform designed for scalability and ease of integration. The platform supports configurable workflows, role-based access control, FHIR interoperability, and integration with provincial health registries. The system includes built-in audit trails and customizable dashboards for clinical decision support.\n",
    "\n",
    "Implementation is structured as a phased 18-month rollout. Phase 1 will onboard primary care clinics, followed by specialty care modules in Phase 2. Vendor A includes hosting, support, and system upgrades in the pricing model but assumes the client will handle data migration and end-user training. The solution is deployed in AWS Canada with regional failover capability and conforms to ISO 27001 and SOC 2 standards.\n",
    "\n",
    "Vendor A has implemented similar platforms in Ontario and Alberta with reported satisfaction scores over 85%. However, case studies and detailed metrics are not included in the proposal. The pricing model is based on tiered user volume, starting at $20/user/month for under 500 users. No information is provided on cost escalation or exit clauses.\n",
    "\"\"\",\n",
    "    \"Vendor B\": \"\"\"\n",
    "Vendor B offers an AI-enabled EHR solution hosted in a secure, government-certified private cloud. The platform includes predictive analytics for patient risk profiling, NLP-enabled clinical note summarization, and native integrations with existing hospital systems. Vendor B emphasizes data privacy, noting compliance with HIPAA, GDPR, and local PHIPA requirements. Role-based permissions, audit logs, and zero-trust access are built into the platform.\n",
    "\n",
    "Implementation is projected at 12 months, with milestones defined at onboarding, integration, testing, and go-live. Vendor B provides full support for data migration, end-user training, and post-go-live optimization. Their implementation team includes certified project managers, clinicians, and technical specialists.\n",
    "\n",
    "Vendor B has successfully deployed the platform in seven Canadian hospital networks and three US healthcare systems. Case studies and outcomes (e.g., 25% reduction in documentation time) are summarized in an appendix. Pricing includes a one-time onboarding fee and an annual subscription based on modules selected. No client-side resources are required except project governance participation.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"✅ Sample proposals loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 4: Single Agent - Evaluate One Proposal** <a id=\"4\"></a>\n",
    "**Run Hybrid ToT + ReAct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run evaluation for single proposal\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    results_a, overall_score_a, swot_summary_a = evaluate_proposal(proposal_a, rfp_criteria)\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set `execute_cell = True` to run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Print single proposal report after getting results:\n",
    "    md_path = export_proposal_report_to_markdown(results_a, overall_score_a, swot_summary_a, proposal_name=\"VendorA\")\n",
    "    pdf_path = convert_markdown_to_pdf(md_path)\n",
    "\n",
    "    print(f\"✅ Saved: {md_path}\")\n",
    "    print(f\"✅ Saved: {pdf_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set `execute_cell = True` to run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 5: Multi-Agent - Evaluate Multiple Proposals** <a id=\"5\"></a>\n",
    "**Display Evaluation Metrics & Export to MD, HTML and PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple proposals with multi-agent evaluation\n",
    "execute_cell = True\n",
    "\n",
    "if execute_cell:\n",
    "    all_vendor_evals = run_multi_proposal_evaluation(\n",
    "        proposals=dummy_proposals,\n",
    "        rfp_criteria=rfp_criteria,\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set `execute_cell = True` to run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_cell = True\n",
    "\n",
    "if execute_cell:\n",
    "    final_summary_text, score_table_md = generate_final_comparison_summary(all_vendor_evals)\n",
    "    print(final_summary_text[:1000])  # just a preview\n",
    "    print(score_table_md)  # just a preview\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set `execute_cell = True` to run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_cell = True\n",
    "\n",
    "if execute_cell:\n",
    "    save_markdown_and_pdf(markdown_text=final_summary_text, additional_md=score_table_md, filename=\"Final_Proposal_Comparison_Summary\")\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set `execute_cell = True` to run this cell.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
