{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: Setup and Imports** <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed (uncomment below in Jupyter)\n",
    "# !pip install openai python-dotenv\n",
    "# %pip install duckduckgo-search\n",
    "\n",
    "import asyncio  # for async functions\n",
    "import difflib  # for comparing strings\n",
    "import os  # for environment variables\n",
    "import re  # regular expressions\n",
    "import webbrowser  # for opening web pages\n",
    "from difflib import SequenceMatcher  # for comparing strings\n",
    "\n",
    "import matplotlib.patches as mpatches  # for patches\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import textstat  # for text statistics\n",
    "from dotenv import load_dotenv  # for loading environment variables\n",
    "from duckduckgo_search import (\n",
    "    DDGS,\n",
    ")  # for searching DuckDuckGo (web information retrieval)\n",
    "from langchain_community.tools import WikipediaQueryRun  # for querying Wikipedia\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # for querying Wikipedia\n",
    "from markdown2 import markdown  # for converting markdown to HTML\n",
    "from openai import OpenAI  # for OpenAI API\n",
    "from playwright.async_api import async_playwright  # for scraping web pages\n",
    "import asyncio\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "import spacy  # for NLP\n",
    "from langchain_community.utilities import SerpAPIWrapper  # for querying SERP API\n",
    "from textblob import TextBlob  # for sentiment analysis\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from langchain.chains import LLMMathChain  # for chaining models - math\n",
    "from langchain_community.tools import ArxivQueryRun  # for querying ArXiv\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper  # for querying ArXiv\n",
    "\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(load_max_docs=3))\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "import random\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path  # for file paths\n",
    "\n",
    "import docx  # for Word documents\n",
    "import fitz  # for PDFs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create a client instance\n",
    "my_openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=my_openai_api_key)\n",
    "\n",
    "# Check to confirm it's loaded (optional)\n",
    "print(\"✅ API key loaded:\", my_openai_api_key is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# Check to confirm it's loaded (optional)\n",
    "print(\"✅ API key loaded:\", serpapi_key is not None)\n",
    "\n",
    "serp_tool = SerpAPIWrapper(serpapi_api_key=serpapi_key)\n",
    "\n",
    "serp_tool.run(\"examples of NIST framework in healthcare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI  # for chaining models - chat\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    ")  # temp = 0 ensures deterministic results (math precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Functions** <a id=\"2\"></a>\n",
    "## **2.1 OpenAI Functions** <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call OpenAI's ChatCompletion API with structured messages\n",
    "def call_openai(messages, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with structured messages.\n",
    "\n",
    "    Parameters:\n",
    "    messages (list): A list of message dictionaries, where each dictionary contains 'role' and 'content' keys.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function takes the input parameters and calls the OpenAI ChatCompletion API.\n",
    "    2. The API returns a response containing multiple choices.\n",
    "    3. The function extracts the content of the first choice from the response.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the first choice from the API response.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=temperature\n",
    "        )\n",
    "        return response.choices[\n",
    "            0\n",
    "        ].message.content.strip()  # Extract the content of the first choice\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Tool execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to construct system + user messages for the reasoning agent\n",
    "\n",
    "\n",
    "def build_review_prompt(report_text, history=[]):\n",
    "    \"\"\"\n",
    "    Constructs system and user messages for the reasoning agent to review a consulting report.\n",
    "\n",
    "    Parameters:\n",
    "    report_text (str): The text of the consulting report to be reviewed.\n",
    "    history (list): A list of previous messages (optional). Default is an empty list.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function creates a system message that sets the context for the reasoning agent.\n",
    "    2. It then creates a user message containing the consulting report text.\n",
    "    3. The function combines the system message, the history of previous messages, and the user message into a single list.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of message dictionaries, including the system message, any historical messages, and the user message.\n",
    "    \"\"\"\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an experienced IT strategy consultant. \"\n",
    "            \"You are reviewing a consulting report for completeness, clarity, risks, and alignment with best practices. \"\n",
    "            \"Think step-by-step and identify gaps, ask clarifying questions, or suggest improvements. \"\n",
    "            \"Your goal is to provide helpful, critical feedback using your expert knowledge.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the consulting report to review:\\n\\n{report_text}\",\n",
    "    }\n",
    "\n",
    "    return [system_msg] + history + [user_msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track total tokens used and estimated cost\n",
    "total_tokens_used = 0\n",
    "estimated_cost_usd = 0.0\n",
    "\n",
    "# Cost per 1K tokens for GPT-3.5-turbo (adjust if using GPT-4)\n",
    "COST_PER_1K_TOKENS = 0.0015\n",
    "\n",
    "\n",
    "def call_openai_with_tracking(\n",
    "    messages, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with structured messages and tracks token usage and estimated cost.\n",
    "\n",
    "    Parameters:\n",
    "    messages (list): A list of message dictionaries, where each dictionary contains 'role' and 'content' keys.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    max_tokens (int): The maximum number of tokens to generate in the completion. Default is 500.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function takes the input parameters and calls the OpenAI ChatCompletion API.\n",
    "    2. The API returns a response containing multiple choices and token usage information.\n",
    "    3. The function extracts the content of the first choice from the response.\n",
    "    4. It updates the total tokens used and the estimated cost in USD.\n",
    "    5. It logs the prompt tokens, completion tokens, total tokens used so far, and the estimated cost.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the first choice from the API response.\n",
    "    \"\"\"\n",
    "    global total_tokens_used, estimated_cost_usd\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Tool execution error: {str(e)}\"\n",
    "\n",
    "    # Extract token usage and calculate estimated cost\n",
    "    usage = response.usage\n",
    "    prompt_tokens = usage.prompt_tokens or 0\n",
    "    completion_tokens = usage.completion_tokens or 0\n",
    "    total = usage.total_tokens or (prompt_tokens + completion_tokens)\n",
    "\n",
    "    # Update tracking\n",
    "    total_tokens_used += total\n",
    "    estimated_cost_usd += (total / 1000) * COST_PER_1K_TOKENS\n",
    "\n",
    "    # Logging\n",
    "    print(\n",
    "        f\"🔢 Prompt: {prompt_tokens} tokens | Completion: {completion_tokens} tokens | Total so far: {total_tokens_used} tokens\"\n",
    "    )\n",
    "    print(f\"💰 Estimated cost so far: ${estimated_cost_usd:.4f} USD\")\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Data Preprocessing Functions** <a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_report_text_from_file(filepath):\n",
    "    \"\"\"\n",
    "    Loads text from a supported file format (txt, md, docx, pdf).\n",
    "    \"\"\"\n",
    "    ext = Path(filepath).suffix.lower()\n",
    "\n",
    "    if ext in [\".txt\", \".md\"]:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    elif ext == \".docx\":\n",
    "        doc = docx.Document(filepath)\n",
    "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    elif ext == \".pdf\":\n",
    "        import fitz  # PyMuPDF\n",
    "\n",
    "        doc = fitz.open(filepath)\n",
    "        return \"\\n\".join([page.get_text() for page in doc])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .txt, .md, .docx, or .pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_report_into_sections(report_text):\n",
    "    \"\"\"\n",
    "    Splits a report into sections based on headers.\n",
    "\n",
    "    Purpose:\n",
    "    This function processes a report text and splits it into sections using headers as delimiters.\n",
    "    It identifies headers based on lines ending with a colon (\":\") and assumes that headers contain\n",
    "    up to four words. Content before the first header is treated as the \"Introduction\" section.\n",
    "\n",
    "    Parameters:\n",
    "    report_text (str): The full text of the report to be split into sections.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty dictionary `sections` to store the output as {section_name: text}.\n",
    "    2. Splits the report into lines and iterates through each line.\n",
    "    3. Skips blank lines.\n",
    "    4. Detects section headers based on lines ending with a colon and containing up to four words.\n",
    "       - If a header is detected:\n",
    "         a. Saves the content of the previous section (if any) into the `sections` dictionary.\n",
    "         b. Updates the `current_section` to the new header.\n",
    "         c. Clears the buffer for the new section's content.\n",
    "       - If no header is detected, appends the line to the buffer for the current section.\n",
    "    5. After processing all lines, saves the content of the last section or treats it as \"Introduction\"\n",
    "       if no headers were found.\n",
    "    6. Returns the `sections` dictionary containing section names as keys and their corresponding content as values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are section names and values are the corresponding section content.\n",
    "    \"\"\"\n",
    "    sections = {}  # To store final output as {section_name: text}\n",
    "    lines = report_text.strip().split(\"\\n\")  # Break the report into lines\n",
    "    current_section = None  # Track the active section label\n",
    "    buffer = []  # Temporarily hold lines belonging to the current section\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip blank lines\n",
    "\n",
    "        # Detect section header\n",
    "        if line.endswith(\":\") and len(line.split()) <= 4:\n",
    "            # Store previous buffer if it exists\n",
    "            if (\n",
    "                current_section is None and buffer\n",
    "            ):  # Handle content before the first header\n",
    "                sections[\"Introduction\"] = \"\\n\".join(buffer).strip()\n",
    "            elif current_section:  # Save the previous section's content\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "\n",
    "            current_section = line.replace(\n",
    "                \":\", \"\"\n",
    "            ).strip()  # Update the current_section label\n",
    "            buffer = []  # Clear the buffer for new section content\n",
    "        else:\n",
    "            buffer.append(line)  # Add line to buffer for the current section\n",
    "\n",
    "    # Save last section or intro\n",
    "    if current_section:  # Save the last section's content\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    elif buffer:  # Handle content with no headers\n",
    "        sections[\"Introduction\"] = \"\\n\".join(buffer).strip()\n",
    "\n",
    "    # After section splitting\n",
    "    normalized_sections = {}\n",
    "    for label, text in sections.items():\n",
    "        canonical = map_section_to_canonical(label)\n",
    "        normalized_sections[canonical or label] = text  # use original if no match\n",
    "\n",
    "    return normalized_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the canonical section map for mapping section names to canonical names (similar to the tool catalog)\n",
    "canonical_section_map = {\n",
    "    \"Introduction\": [\"header\", \"intro\", \"project context\", \"introduction\", \"overview\"],\n",
    "    \"Summary\": [\"summary\", \"executive summary\"],\n",
    "    \"Goals & Objectives\": [\"goals\", \"objectives\", \"strategic priorities\"],\n",
    "    \"Current State Assessment\": [\"current state\", \"as-is\", \"status quo\"],\n",
    "    \"Future State\": [\"future state\", \"to-be\", \"vision\", \"target state\"],\n",
    "    \"Key Recommendations\": [\"recommendations\", \"our recommendations\", \"next steps\"],\n",
    "    \"Implementation Plan\": [\n",
    "        \"implementation plan\",\n",
    "        \"roadmap\",\n",
    "        \"deployment\",\n",
    "        \"schedule\",\n",
    "        \"timeline\",\n",
    "        \"phasing\",\n",
    "    ],\n",
    "    \"Benefits\": [\"benefits\", \"value\", \"expected outcomes\"],\n",
    "    \"Costs\": [\"costs\", \"financials\", \"budget\", \"investment\"],\n",
    "    \"Resources\": [\"resources\", \"team structure\", \"staffing\", \"governance\"],\n",
    "    \"Risks & Mitigations\": [\"risks\", \"mitigations\", \"risk mitigation\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_section_to_canonical(label, threshold=0.6, use_llm_fallback=True):\n",
    "    \"\"\"\n",
    "    Maps a section label to its canonical name.\n",
    "\n",
    "    Purpose:\n",
    "    This function attempts to map a given section label to a canonical section name using a combination of direct matching, fuzzy matching, and optionally a language model fallback. It is useful for normalizing section names in a report to a standard set of canonical names.\n",
    "\n",
    "    Parameters:\n",
    "    label (str): The section label to map to a canonical name.\n",
    "    threshold (float): The similarity threshold for fuzzy matching. Default is 0.6.\n",
    "    use_llm_fallback (bool): Whether to use a language model fallback if no match is found. Default is True.\n",
    "\n",
    "    Workflow:\n",
    "    1. Cleans the input label by stripping whitespace and converting it to lowercase.\n",
    "    2. Performs direct matching:\n",
    "       - Checks if the label matches any canonical name or its variants directly or as a substring.\n",
    "    3. Performs fuzzy matching:\n",
    "       - Uses fuzzy string matching to find the closest match among all canonical names and their variants.\n",
    "       - If a match is found with a similarity ratio above the threshold, returns the corresponding canonical name.\n",
    "    4. Uses a language model fallback (optional):\n",
    "       - If no match is found through direct or fuzzy matching, calls the `guess_canonical_section_with_llm` function to infer the canonical name using a language model.\n",
    "    5. Returns `None` if no match is found through any method.\n",
    "\n",
    "    Returns:\n",
    "    str or None: The canonical section name if a match is found, or `None` if no match is found.\n",
    "    \"\"\"\n",
    "    label_clean = label.strip().lower()\n",
    "\n",
    "    # 1. Direct and substring match\n",
    "    for canonical, variants in canonical_section_map.items():\n",
    "        for variant in variants:\n",
    "            if variant in label_clean:\n",
    "                return canonical\n",
    "        if canonical.lower() in label_clean:\n",
    "            return canonical\n",
    "\n",
    "    # 2. Fuzzy match\n",
    "    all_variants = {\n",
    "        v.lower(): canon\n",
    "        for canon, variants in canonical_section_map.items()\n",
    "        for v in variants\n",
    "    }\n",
    "    match = difflib.get_close_matches(\n",
    "        label_clean, all_variants.keys(), n=1, cutoff=threshold\n",
    "    )\n",
    "    if match:\n",
    "        return all_variants[match[0]]\n",
    "\n",
    "    # 3. LLM fallback\n",
    "    if use_llm_fallback:\n",
    "        return guess_canonical_section_with_llm(label)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_canonical_section_with_llm(label, model=\"gpt-3.5-turbo\", temperature=0.2):\n",
    "    \"\"\"\n",
    "    Uses LLM to guess the canonical section name from a list of known options.\n",
    "\n",
    "    Returns:\n",
    "    str or None\n",
    "    \"\"\"\n",
    "    canonical_names = list(canonical_section_map.keys())\n",
    "    prompt = (\n",
    "        \"You're an expert in business consulting. \"\n",
    "        \"Given this section title: '{label}', what is the most likely standard section label \"\n",
    "        \"from the following options:\\n\\n\"\n",
    "        + \", \".join(canonical_names)\n",
    "        + \"\\n\\nReturn only the best matching label. If unsure, return 'Unknown'.\"\n",
    "    ).replace(\"{label}\", label.strip())\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    result = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "\n",
    "    cleaned = result.strip()\n",
    "    if cleaned in canonical_names:\n",
    "        return cleaned\n",
    "    elif \"unknown\" in cleaned.lower():\n",
    "        return None\n",
    "    else:\n",
    "        # Try fuzzy match if response is slightly off (e.g., \"Implementation Timeline\")\n",
    "        return (\n",
    "            difflib.get_close_matches(cleaned, canonical_names, n=1, cutoff=0.6)[0]\n",
    "            if difflib.get_close_matches(cleaned, canonical_names)\n",
    "            else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Static Reasoning Agent Functions** <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITReportReviewer:\n",
    "    \"\"\"\n",
    "    A class to review sections of an IT consulting report using OpenAI's ChatCompletion API.\n",
    "\n",
    "    Process:\n",
    "    1. Initializes the reviewer with the report sections\n",
    "    2. Lets it review one section at a time using real reasoning\n",
    "    3. Stores and prints each review with feedback\n",
    "\n",
    "    Attributes:\n",
    "    sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    review_history (list): A list to store the history of reviews for each section.\n",
    "\n",
    "    Methods:\n",
    "    review_section(section_name):\n",
    "        Reviews a specific section of the report using OpenAI's ChatCompletion API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, report_sections, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "        \"\"\"\n",
    "        Initializes the ITReportReviewer with the given report sections, model, and temperature.\n",
    "\n",
    "        Parameters:\n",
    "        report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "        model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "        temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "        \"\"\"\n",
    "        self.sections = report_sections\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.review_history = []\n",
    "\n",
    "    def review_section(self, section_name):\n",
    "        \"\"\"\n",
    "        Reviews a specific section of the report using OpenAI's ChatCompletion API.\n",
    "\n",
    "        Parameters:\n",
    "        section_name (str): The name of the section to review.\n",
    "\n",
    "        Workflow:\n",
    "        1. Retrieves the text of the specified section from the sections dictionary.\n",
    "        2. If the section is empty or missing, prints a warning message.\n",
    "        3. Builds a prompt for reviewing the section using the build_review_prompt function.\n",
    "        4. Calls the OpenAI ChatCompletion API with tracking using the call_openai_with_tracking function.\n",
    "        5. Saves the review in the review_history attribute.\n",
    "        6. Prints the review for the specified section.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        section_text = self.sections.get(section_name, \"\")\n",
    "        if not section_text:\n",
    "            print(f\"⚠️ Section '{section_name}' is empty or missing.\")\n",
    "            return\n",
    "\n",
    "        # Build prompt for reviewing the section\n",
    "        messages = build_review_prompt(\n",
    "            report_text=f\"Section: {section_name}\\n\\n{section_text}\", history=[]\n",
    "        )\n",
    "\n",
    "        # Get AI-generated reasoning using tracked OpenAI call\n",
    "        review = call_openai_with_tracking(\n",
    "            messages, model=self.model, temperature=self.temperature\n",
    "        )\n",
    "\n",
    "        # Save the review\n",
    "        self.review_history.append({\"section\": section_name, \"review\": review})\n",
    "\n",
    "        print(f\"\\n✅ Review for section '{section_name}':\\n{review}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_full_review(agent):\n",
    "    \"\"\"\n",
    "    Summarizes the full review of an IT consulting report.\n",
    "\n",
    "    Purpose:\n",
    "    This function consolidates the feedback from all reviewed sections of an IT consulting report and generates a final summary. The summary highlights the overall quality of the report, identifies gaps, strengths, and suggests next steps for improvement.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ITReportReviewer): An instance of the ITReportReviewer class, which contains the review history for each section.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty string `combined_review_text` to store the consolidated feedback.\n",
    "    2. Iterates through the `review_history` attribute of the `agent` object.\n",
    "       - For each review, appends the section name and feedback to `combined_review_text`.\n",
    "    3. Constructs a prompt for the OpenAI API to summarize the overall quality of the report.\n",
    "       - The prompt includes the consolidated feedback and instructions to highlight gaps, strengths, and next steps.\n",
    "    4. Calls the OpenAI API using the `call_openai_with_tracking` function to generate the summary.\n",
    "    5. Returns the generated summary.\n",
    "\n",
    "    Returns:\n",
    "    str: A summary of the overall quality of the report, including gaps, strengths, and suggested next steps.\n",
    "    \"\"\"\n",
    "    # Combine all reviews into a single prompt\n",
    "    combined_review_text = \"\"\n",
    "    for step in agent.review_history:\n",
    "        combined_review_text += (\n",
    "            f\"Section: {step['section']}\\nFeedback: {step['review']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # Build new prompt asking for a final report assessment\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert IT strategy consultant reviewing an internal report assessment. \"\n",
    "                \"Summarize the overall quality of the report based on the following section reviews. \"\n",
    "                \"Highlight gaps, strengths, and suggest next steps to improve the report.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": combined_review_text},\n",
    "    ]\n",
    "\n",
    "    summary = call_openai_with_tracking(messages)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 ReAct Reasoning Agent Functions** <a id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActConsultantAgent:\n",
    "    \"\"\"\n",
    "    A class to review sections of an IT consulting report using the ReAct (Reason + Act) framework with OpenAI's ChatCompletion API.\n",
    "\n",
    "    Process:\n",
    "    1. Initializes the agent with the section name and text.\n",
    "    2. Builds a prompt for the ReAct framework.\n",
    "    3. Tracks the history of thoughts, actions, and observations.\n",
    "\n",
    "    Attributes:\n",
    "    section_name (str): The name of the section to review.\n",
    "    section_text (str): The text of the section to review.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    history (list): A list to store the history of thoughts, actions, and observations.\n",
    "\n",
    "    Methods:\n",
    "    build_react_prompt():\n",
    "        Builds a prompt for the ReAct framework based on the section text and history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, section_name, section_text, model=\"gpt-3.5-turbo\", temperature=0.7\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ReActConsultantAgent with the given section name, section text, model, and temperature.\n",
    "\n",
    "        Parameters:\n",
    "        section_name (str): The name of the section to review.\n",
    "        section_text (str): The text of the section to review.\n",
    "        model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "        temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "        \"\"\"\n",
    "        self.section_name = section_name\n",
    "        self.section_text = section_text\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.history = []\n",
    "        self.tool_usage = {}  # {action_name: count}\n",
    "        self.memory = {\n",
    "            \"section_notes\": {},  # {section_name: [insight1, insight2, ...]}\n",
    "            \"cross_section_flags\": [],  # [(sectionA, sectionB, observation)]\n",
    "            \"tool_history\": [],  # [(step_number, action, section)]\n",
    "        }\n",
    "\n",
    "    def build_react_prompt(self):\n",
    "        \"\"\"\n",
    "        Builds a prompt for the ReAct framework based on the section text and history.\n",
    "\n",
    "        Workflow:\n",
    "        1. Constructs a base prompt with the section name and text.\n",
    "        2. Iterates through the history of thoughts, actions, and observations, appending them to the base prompt.\n",
    "        3. Adds a final line asking for the next thought and action.\n",
    "\n",
    "        Returns:\n",
    "        list: A list containing a single dictionary with the role 'user' and the constructed prompt as content.\n",
    "        \"\"\"\n",
    "        base_prompt = (\n",
    "            f\"You are an expert IT strategy consultant reviewing a report section titled '{self.section_name}'.\\n\"\n",
    "            \"You are using ReAct (Reason + Act) to think through the review.\\n\\n\"\n",
    "            \"Format each response like this:\\n\"\n",
    "            \"Thought: <your reasoning>\\n\"\n",
    "            \"Action: <one of: ask_question, flag_risk, recommend_fix, summarize>\\n\\n\"\n",
    "            f\"Here is the section content:\\n{self.section_text}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        for step in self.history:\n",
    "            base_prompt += f\"Thought: {step['thought']}\\n\"\n",
    "            base_prompt += f\"Action: {step['action']}\\n\"\n",
    "            base_prompt += f\"Observation: {step['observation']}\\n\\n\"\n",
    "\n",
    "        base_prompt += \"What is your next Thought and Action?\"\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "\n",
    "    def build_react_prompt_withTools(self):\n",
    "        \"\"\"\n",
    "        Builds a prompt for the ReAct framework based on the section text and history.\n",
    "\n",
    "        Workflow:\n",
    "        1. Constructs a base prompt with the section name and text.\n",
    "        2. Iterates through the history of thoughts, actions, and observations, appending them to the base prompt.\n",
    "        3. Adds a final line asking for the next thought and action.\n",
    "        For the check_guideline action, the prompt includes a placeholder for the topic.\n",
    "        LLM infers topic from the section_text\n",
    "\n",
    "        Returns:\n",
    "        list: A list containing a single dictionary with the role 'user' and the constructed prompt as content.\n",
    "        \"\"\"\n",
    "        tool_hint_text, tools_to_focus = build_tool_hints(self)\n",
    "\n",
    "        base_prompt = (\n",
    "            f\"You are an expert IT strategy consultant reviewing a report section titled '{self.section_name}'.\\n\"\n",
    "            \"You are using ReAct (Reason + Act) to think through the review.\\n\\n\"\n",
    "            \"Format each response like this:\\n\"\n",
    "            \"Thought: <your reasoning>\\n\"\n",
    "            \"Action: <choose ONE tool from the prioritized list below, unless you strongly believe another tool is better>\\n\\n\"\n",
    "            f\"Prioritized tools for this section:\\n{tool_hint_text}\\n\\n\"\n",
    "            \"Available tools and their descriptions:\\n\"\n",
    "        )\n",
    "\n",
    "        base_prompt += format_tool_catalog_for_prompt(tool_catalog)\n",
    "        base_prompt += f\"Here is the section content:\\n{self.section_text}\\n\\n\"\n",
    "\n",
    "        for step in self.history:\n",
    "            base_prompt += f\"Thought: {step['thought']}\\n\"\n",
    "            base_prompt += f\"Action: {step['action']}\\n\"\n",
    "            base_prompt += f\"Observation: {step['observation']}\\n\\n\"\n",
    "\n",
    "        base_prompt += \"What is your next Thought and Action?\"\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": base_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_loop_static(agent, max_steps=5):\n",
    "    \"\"\"\n",
    "    Runs the ReAct (Reason + Act) loop for a specified number of steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through a reasoning and action loop using the ReAct framework to review a section of an IT consulting report. It generates thoughts, actions, and observations at each step, and stores the history of these steps.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    max_steps (int): The maximum number of steps to run the loop. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through the loop for a maximum of `max_steps` times.\n",
    "    2. In each iteration:\n",
    "    - Calls `agent.build_react_prompt()` to construct the prompt for the ReAct framework.\n",
    "    - Calls `call_openai_with_tracking()` to get the response from the OpenAI API.\n",
    "    - Parses the response to extract the thought and action.\n",
    "    - Generates an observation based on the action.\n",
    "    - Stores the thought, action, and observation in the agent's history.\n",
    "    - Prints the result of the current step.\n",
    "    - Breaks the loop if the action is \"summarize\".\n",
    "    3. Returns the full reasoning history.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, where each dictionary contains the thought, action, and observation for each step.\n",
    "    \"\"\"\n",
    "    for step_num in range(max_steps):\n",
    "        messages = agent.build_react_prompt()\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=agent.model, temperature=agent.temperature\n",
    "        )\n",
    "\n",
    "        # Parse response\n",
    "        try:\n",
    "            lines = response.strip().split(\"\\n\")\n",
    "            thought = next(\n",
    "                line.split(\":\", 1)[1].strip()\n",
    "                for line in lines\n",
    "                if line.lower().startswith(\"thought\")\n",
    "            )\n",
    "            action = next(\n",
    "                line.split(\":\", 1)[1].strip()\n",
    "                for line in lines\n",
    "                if line.lower().startswith(\"action\")\n",
    "            )\n",
    "        except:\n",
    "            print(\"⚠️ Failed to parse model response.\")\n",
    "            break\n",
    "\n",
    "        # Generate observation based on action\n",
    "        if action == \"ask_question\":\n",
    "            observation = \"Good question to ask the client for clarification.\"\n",
    "        elif action == \"flag_risk\":\n",
    "            observation = \"This is a legitimate risk that should be addressed.\"\n",
    "        elif action == \"recommend_fix\":\n",
    "            observation = (\n",
    "                \"The recommendation improves the section's clarity and compliance.\"\n",
    "            )\n",
    "        elif action == \"summarize\":\n",
    "            observation = \"Review complete.\"\n",
    "        else:\n",
    "            observation = \"Unrecognized action.\"\n",
    "\n",
    "        # Store step\n",
    "        agent.history.append(\n",
    "            {\"thought\": thought, \"action\": action, \"observation\": observation}\n",
    "        )\n",
    "\n",
    "        # Print result of this step\n",
    "        print(f\"\\n🔁 Step {step_num + 1}\")\n",
    "        print(f\"🧠 Thought: {thought}\")\n",
    "        print(f\"⚙️ Action: {action}\")\n",
    "        print(f\"👀 Observation: {observation}\")\n",
    "\n",
    "        if action == \"summarize\":\n",
    "            break\n",
    "\n",
    "    # Return full reasoning history\n",
    "    return agent.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_loop_check_withTool(agent, max_steps=5):\n",
    "    \"\"\"\n",
    "    Runs the ReAct (Reason + Act) loop for a specified number of steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through a reasoning and action loop using the ReAct framework to review a section of an IT consulting report. It generates thoughts, actions, and observations at each step, and stores the history of these steps.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    max_steps (int): The maximum number of steps to run the loop. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through the loop for a maximum of `max_steps` times.\n",
    "    2. In each iteration:\n",
    "       - Calls `agent.build_react_prompt()` to construct the prompt for the ReAct framework.\n",
    "       - Calls `call_openai_with_tracking()` to get the response from the OpenAI API.\n",
    "       - Parses the response to extract the thought and action.\n",
    "       - Generates an observation based on the action.\n",
    "       - Stores the thought, action, and observation in the agent's history.\n",
    "       - Prints the result of the current step.\n",
    "       - Breaks the loop if the action is \"summarize\".\n",
    "    3. Returns the full reasoning history.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, where each dictionary contains the thought, action, and observation for each step.\n",
    "    \"\"\"\n",
    "    for step_num in range(max_steps):\n",
    "        messages = agent.build_react_prompt_withTools()\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=agent.model, temperature=agent.temperature\n",
    "        )\n",
    "\n",
    "        # Parse response\n",
    "        try:\n",
    "            lines = response.strip().split(\"\\n\")\n",
    "            thought = next(\n",
    "                line.split(\":\", 1)[1].strip()\n",
    "                for line in lines\n",
    "                if line.lower().startswith(\"thought\")\n",
    "            )\n",
    "            action = next(\n",
    "                line.split(\":\", 1)[1].strip()\n",
    "                for line in lines\n",
    "                if line.lower().startswith(\"action\")\n",
    "            )\n",
    "            # Log tool usage\n",
    "            if hasattr(agent, \"tool_usage\"):\n",
    "                agent.tool_usage[action] = agent.tool_usage.get(action, 0) + 1\n",
    "        except:\n",
    "            print(\"⚠️ Failed to parse model response.\")\n",
    "            break\n",
    "\n",
    "        # Generate observation based on action\n",
    "        observation = dispatch_tool_action(agent, action)\n",
    "\n",
    "        # Track tool history\n",
    "        agent.memory[\"tool_history\"].append((step_num, action, agent.section_name))\n",
    "\n",
    "        # Special case for section comparisons\n",
    "        if action.startswith(\"compare_with_other_section\"):\n",
    "            match = re.match(\n",
    "                r'compare_with_other_section\\[\"(.+?)\",\\s*\"(.+?)\"\\]', action\n",
    "            )\n",
    "            if match:\n",
    "                sectionA, sectionB = match.groups()\n",
    "                agent.memory[\"cross_section_flags\"].append(\n",
    "                    (sectionA, sectionB, observation)\n",
    "                )\n",
    "\n",
    "        # Store step\n",
    "        agent.history.append(\n",
    "            {\"thought\": thought, \"action\": action, \"observation\": observation}\n",
    "        )\n",
    "\n",
    "        # Print result of this step\n",
    "        print(f\"\\n🔁 Step {step_num + 1}\")\n",
    "        print(f\"🧠 Thought: {thought}\")\n",
    "        print(f\"⚙️ Action: {action}\")\n",
    "        print(f\"👀 Observation: {observation}\")\n",
    "\n",
    "        if action == \"summarize\":\n",
    "            break\n",
    "\n",
    "    # Summarize and store section notes, scores, fixes, confidence level, raw ouputs\n",
    "    summarize_and_score_section(agent)\n",
    "\n",
    "    # Return full reasoning history\n",
    "    return agent.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_tool_action(agent, action):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Dispatches a tool action based on the provided action string and executes the corresponding function.\n",
    "\n",
    "    Parameters:\n",
    "        agent (object): The agent object containing context such as section text, section name, and memory.\n",
    "        action (str): The action string specifying the tool action to be executed.\n",
    "\n",
    "    Workflow:\n",
    "        1. Logs the tool action being executed.\n",
    "        2. Matches the action string against predefined patterns using regular expressions.\n",
    "        3. Executes the corresponding function based on the matched action.\n",
    "        4. Handles specific cases such as academic support, citation checks, and section upgrades.\n",
    "        5. Returns the result of the executed function or an appropriate message if the action is unrecognized.\n",
    "\n",
    "    Returns:\n",
    "        str: The result of the executed tool action or an error message if an exception occurs.\n",
    "    \"\"\"\n",
    "    print(f\"🛠️ Tool action: {action}\")\n",
    "    try:\n",
    "        if action.startswith(\"check_guideline\"):\n",
    "            match = re.match(r'check_guideline\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_guideline(match.group(1))\n",
    "        elif action.startswith(\"keyword_match_in_section\"):\n",
    "            match = re.match(r'keyword_match_in_section\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return keyword_match_in_section(match.group(1), agent.section_text)\n",
    "        elif action.startswith(\"check_timeline_feasibility\"):\n",
    "            match = re.match(r'check_timeline_feasibility\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_timeline_feasibility(match.group(1))\n",
    "        elif action.startswith(\"search_report\"):\n",
    "            match = re.match(r'search_report\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return search_report(match.group(1), report_sections)\n",
    "        elif action.startswith(\"search_web\"):\n",
    "            match = re.match(r'search_web\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return search_web(match.group(1))\n",
    "        elif action == \"check_for_jargon\":\n",
    "            return check_for_jargon(agent.section_text)\n",
    "        elif action == \"generate_client_questions\":\n",
    "            return generate_client_questions(agent.section_text)\n",
    "        elif action == \"highlight_missing_sections\":\n",
    "            return highlight_missing_sections(report_sections)\n",
    "        elif action.startswith(\"check_alignment_with_goals\"):\n",
    "            match = re.match(r'check_alignment_with_goals\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_alignment_with_goals(match.group(1), report_sections)\n",
    "        elif action.startswith(\"compare_with_other_section\"):\n",
    "            match = re.match(\n",
    "                r'compare_with_other_section\\[\"(.+?)\",\\s*\"(.+?)\"\\]', action\n",
    "            )\n",
    "            if match:\n",
    "                return compare_with_other_section(\n",
    "                    match.group(1), match.group(2), report_sections\n",
    "                )\n",
    "        elif action.startswith(\"check_summary_support\"):\n",
    "            match = re.match(r'check_summary_support\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_summary_support(match.group(1), report_sections)\n",
    "        elif action == \"evaluate_smart_goals\":\n",
    "            return evaluate_smart_goals(agent.section_text)\n",
    "        elif action.startswith(\"check_recommendation_alignment\"):\n",
    "            match = re.match(r'check_recommendation_alignment\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                goals = report_sections.get(\"Goals & Objectives\", \"\")\n",
    "                return check_recommendation_alignment(match.group(1), goals)\n",
    "        elif action == \"ask_question\":\n",
    "            return \"Good question to ask the client for clarification.\"\n",
    "        elif action == \"flag_risk\":\n",
    "            return \"This is a legitimate risk that should be addressed.\"\n",
    "        elif action == \"recommend_fix\":\n",
    "            return \"The recommendation improves the section's clarity and compliance.\"\n",
    "        elif action == \"summarize\":\n",
    "            return \"Review complete.\"\n",
    "        elif action == \"tool_help\":\n",
    "            return format_tool_catalog_for_prompt(tool_catalog)\n",
    "        elif action.startswith(\"suggest_tool_for\"):\n",
    "            match = re.match(r'suggest_tool_for\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                matches = pick_tool_by_intent_fuzzy(match.group(1), tool_catalog)\n",
    "                if matches:\n",
    "                    return \"Best match based on your goal:\\n\" + \"\\n\".join(\n",
    "                        [f\"{tool} (match: {score})\" for tool, score in matches]\n",
    "                    )\n",
    "                else:\n",
    "                    return (\n",
    "                        \"⚠️ No matching tool found. Showing available tools:\\n\"\n",
    "                        + format_tool_catalog_for_prompt(tool_catalog)\n",
    "                    )\n",
    "        elif action == \"final_summary\":\n",
    "            return generate_final_summary(agent)\n",
    "        elif action == \"check_readability\":\n",
    "            return check_readability(agent.section_text)\n",
    "        elif action.startswith(\"search_wikipedia\"):\n",
    "            match = re.match(r'search_wikipedia\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "                return search_wikipedia(query)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse search_wikipedia action.\"\n",
    "        elif action == \"analyze_tone_textblob\":\n",
    "            return analyze_tone_textblob(agent.section_text)\n",
    "        elif action.startswith(\"search_serpapi\"):\n",
    "            match = re.match(r'search_serpapi\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "                return search_serpapi(query)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse search_serpapi action.\"\n",
    "        elif action == \"extract_named_entities\":\n",
    "            return extract_named_entities(agent.section_text)\n",
    "        elif action.startswith(\"analyze_math_question\"):\n",
    "            match = re.match(r'analyze_math_question\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                expr = match.group(1)\n",
    "                return analyze_math_question(expr)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse analyze_math_question action.\"\n",
    "        elif action.startswith(\"search_arxiv\"):\n",
    "            match = re.match(r'search_arxiv\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "                return search_arxiv(query)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse search_arxiv action.\"\n",
    "        elif action == \"auto_check_for_academic_support\":\n",
    "            needs_citation, reason = should_search_arxiv(agent.section_text)\n",
    "            if needs_citation:\n",
    "                # Automatically create and run an arxiv search\n",
    "                followup_action = f'search_arxiv[\"{agent.section_name}\"]'\n",
    "                followup_obs = dispatch_tool_action(agent, followup_action)\n",
    "                # Log follow-up to memory\n",
    "                agent.memory.setdefault(\"academic_support\", {})[agent.section_name] = {\n",
    "                    \"reason\": reason,\n",
    "                    \"action\": followup_action,\n",
    "                    \"observation\": followup_obs,\n",
    "                }\n",
    "                return f\"✅ Academic support was added.\\nReason: {reason}\\n\\n{followup_obs}\"\n",
    "            else:\n",
    "                return f\"🟢 No academic support needed.\\nReason: {reason}\"\n",
    "        elif action.startswith(\"should_cite\"):\n",
    "            match = re.match(r'should_cite\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                statement = match.group(1)\n",
    "                needs, reason = should_cite(statement)\n",
    "                if needs:\n",
    "                    # Run research-based rewrite\n",
    "                    improved = auto_fill_gaps_with_research(statement)\n",
    "                    observation = (\n",
    "                        f\"✅ Citation recommended. Reason: {reason}\\n\\n\"\n",
    "                        f\"📚 Improved statement with research:\\n{improved}\"\n",
    "                    )\n",
    "                    agent.memory.setdefault(\"enhanced_statements\", {})[\n",
    "                        statement\n",
    "                    ] = improved\n",
    "                    return observation\n",
    "                else:\n",
    "                    observation = f\"🟢 No citation needed. Reason: {reason}\"\n",
    "                    return observation\n",
    "            else:\n",
    "                return \"⚠️ Could not parse should_cite action.\"\n",
    "        elif action == \"auto_fill_gaps_with_research\":\n",
    "            return auto_fill_gaps_with_research(agent.section_text)\n",
    "        elif action == \"upgrade_section_with_research\":\n",
    "            improved, log, footnotes = upgrade_section_with_research(agent.section_text)\n",
    "            observation = \"🧠 Section upgraded with research. Rewrites:\\n\"\n",
    "            for change in log:\n",
    "                observation += f\"- 🔹 '{change['original']}'\\n  🧠 → {change['improved']}\\n  📚 Reason: {change['reason']}\\n\\n\"\n",
    "\n",
    "            # Store in memory for final report\n",
    "            agent.memory.setdefault(\"section_upgrades\", {})[agent.section_name] = {\n",
    "                \"original\": agent.section_text,\n",
    "                \"improved\": improved,\n",
    "                \"log\": log,\n",
    "                \"footnotes\": footnotes,\n",
    "            }\n",
    "\n",
    "            return observation\n",
    "        else:\n",
    "            return \"Unrecognized action.\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Tool execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_score_section(agent, report_sections=None):\n",
    "    section_name = agent.section_name\n",
    "    section_text = agent.section_text\n",
    "    \n",
    "    if report_sections:\n",
    "        goals_text = report_sections.get(\"Goals & Objectives\", None)\n",
    "    else:   \n",
    "        goals_text = \"\"\n",
    "        \n",
    "    # Summarize\n",
    "    agent.memory[\"section_notes\"][section_name] = [summarize_section_insights(agent)]\n",
    "\n",
    "    # Score\n",
    "    agent.memory.setdefault(\"section_scores\", {})[section_name] = score_section(section_name, section_text, goals_text)\n",
    "\n",
    "    # Confidence\n",
    "    agent.memory.setdefault(\"confidence_levels\", {})[section_name] = get_confidence_level(agent)\n",
    "\n",
    "    # Fix suggestions\n",
    "    agent.memory.setdefault(\"section_fixes\", {})[section_name] = recommend_fixes(agent)\n",
    "\n",
    "    # Debug notes\n",
    "    agent.memory.setdefault(\"debug_notes\", {})[section_name] = agent.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dicionary of tools available for ReActConsultantAgent to call\n",
    "\n",
    "tool_catalog = {\n",
    "    \"check_guideline\": {\n",
    "        \"description\": \"Look up a best practice for a given topic\",\n",
    "        \"usage\": 'check_guideline[\"cloud security\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'check_guideline[\"data governance\"]',\n",
    "            'check_guideline[\"migration strategy\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"keyword_match_in_section\": {\n",
    "        \"description\": \"Check if a keyword appears in the current section\",\n",
    "        \"usage\": 'keyword_match_in_section[\"encryption\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['keyword_match_in_section[\"stakeholders\"]'],\n",
    "    },\n",
    "    \"check_timeline_feasibility\": {\n",
    "        \"description\": \"Check if a project timeline is realistic for migration\",\n",
    "        \"usage\": 'check_timeline_feasibility[\"12 weeks\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['check_timeline_feasibility[\"3 months\"]'],\n",
    "    },\n",
    "    \"search_report\": {\n",
    "        \"description\": \"Search the entire report for a concept or term\",\n",
    "        \"usage\": 'search_report[\"data governance\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['search_report[\"Zero Trust\"]'],\n",
    "    },\n",
    "    \"ask_question\": {\n",
    "        \"description\": \"Ask a clarifying question about the report\",\n",
    "        \"usage\": \"ask_question\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"ask_question\"],\n",
    "    },\n",
    "    \"flag_risk\": {\n",
    "        \"description\": \"Flag a gap, issue, or concern in the section\",\n",
    "        \"usage\": \"flag_risk\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"flag_risk\"],\n",
    "    },\n",
    "    \"recommend_fix\": {\n",
    "        \"description\": \"Suggest a specific improvement\",\n",
    "        \"usage\": \"recommend_fix\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"recommend_fix\"],\n",
    "    },\n",
    "    \"summarize\": {\n",
    "        \"description\": \"Summarize your review and end the loop\",\n",
    "        \"usage\": \"summarize\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"summarize\"],\n",
    "    },\n",
    "    \"tool_help\": {\n",
    "        \"description\": \"View descriptions, usage, and examples for available tools\",\n",
    "        \"usage\": \"tool_help\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"tool_help\"],\n",
    "    },\n",
    "    \"suggest_tool_for\": {\n",
    "        \"description\": \"Ask which tool best supports a particular goal or intent\",\n",
    "        \"usage\": 'suggest_tool_for[\"goal or intent\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'suggest_tool_for[\"check if encryption is included\"]',\n",
    "            'suggest_tool_for[\"evaluate feasibility of 12-week timeline\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"search_web\": {\n",
    "        \"description\": \"Look up a concept, framework, or term on the web (DuckDuckGo)\",\n",
    "        \"usage\": 'search_web[\"Zero Trust model\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['search_web[\"data mesh\"]'],\n",
    "    },\n",
    "    \"check_for_jargon\": {\n",
    "        \"description\": \"Identify jargon or overly technical terms that may need simplification\",\n",
    "        \"usage\": \"check_for_jargon\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"check_for_jargon\"],\n",
    "    },\n",
    "    \"generate_client_questions\": {\n",
    "        \"description\": \"Generate clarifying or skeptical questions a client might ask about the section\",\n",
    "        \"usage\": \"generate_client_questions\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"generate_client_questions\"],\n",
    "    },\n",
    "    \"highlight_missing_sections\": {\n",
    "        \"description\": \"Identify which expected sections are missing from the report\",\n",
    "        \"usage\": \"highlight_missing_sections\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"highlight_missing_sections\"],\n",
    "    },\n",
    "    \"check_alignment_with_goals\": {\n",
    "        \"description\": \"Evaluate how well a report section aligns with the stated goals\",\n",
    "        \"usage\": 'check_alignment_with_goals[\"section_name\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['check_alignment_with_goals[\"Key Recommendations\"]'],\n",
    "    },\n",
    "    \"compare_with_other_section\": {\n",
    "        \"description\": \"Compare two sections to identify overlaps, contradictions, or gaps\",\n",
    "        \"usage\": 'compare_with_other_section[\"sectionA\", \"sectionB\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'compare_with_other_section[\"Key Recommendations\", \"Roadmap & Timeline\"]',\n",
    "            'compare_with_other_section[\"Future State Vision\", \"Technology Architecture\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"final_summary\": {\n",
    "        \"description\": \"Generate a final client-facing summary based on insights gathered across all sections\",\n",
    "        \"usage\": \"final_summary\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"final_summary\"],\n",
    "    },\n",
    "    \"check_summary_support\": {\n",
    "        \"description\": \"Check if a summary is supported by details in the rest of the report\",\n",
    "        \"usage\": 'check_summary_support[\"summary text\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'check_summary_support[\"The report outlines benefits of cloud migration...\"]'\n",
    "        ],\n",
    "    },\n",
    "    \"evaluate_smart_goals\": {\n",
    "        \"description\": \"Evaluate whether stated goals follow the SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound)\",\n",
    "        \"usage\": \"evaluate_smart_goals\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"evaluate_smart_goals\"],\n",
    "    },\n",
    "    \"check_recommendation_alignment\": {\n",
    "        \"description\": \"Check whether the recommendations align with the goals of the report\",\n",
    "        \"usage\": 'check_recommendation_alignment[\"recommendation text\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'check_recommendation_alignment[\"Modernize CRM and EHR with cloud-based tools\"]'\n",
    "        ],\n",
    "    },\n",
    "    \"check_readability\": {\n",
    "        \"description\": \"Evaluate the section’s readability and complexity using text statistics\",\n",
    "        \"usage\": \"check_readability\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"check_readability\"],\n",
    "    },\n",
    "    \"search_wikipedia\": {\n",
    "        \"description\": \"Look up a concept on Wikipedia to gain factual context or definition\",\n",
    "        \"usage\": 'search_wikipedia[\"cloud computing\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'search_wikipedia[\"Zero Trust\"]',\n",
    "            'search_wikipedia[\"data governance\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"analyze_tone_textblob\": {\n",
    "        \"description\": \"Analyze tone and subjectivity of the section using TextBlob\",\n",
    "        \"usage\": \"analyze_tone_textblob\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"analyze_tone_textblob\"],\n",
    "    },\n",
    "    \"search_serpapi\": {\n",
    "        \"description\": \"Search the web in real time using Google (via SerpAPI)\",\n",
    "        \"usage\": 'search_serpapi[\"query here\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'search_serpapi[\"cloud security trends 2024\"]',\n",
    "            'search_serpapi[\"real-world Zero Trust case studies\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"extract_named_entities\": {\n",
    "        \"description\": \"Extract names of people, organizations, dates, and locations from the section\",\n",
    "        \"usage\": \"extract_named_entities\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"extract_named_entities\"],\n",
    "    },\n",
    "    \"analyze_math_question\": {\n",
    "        \"description\": \"Ask a question involving math (ROI, budgets, timeframes) and get an explained answer\",\n",
    "        \"usage\": 'analyze_math_question[\"What is 20% of $120,000 over 3 years?\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'analyze_math_question[\"If we save $500/month for 18 months, what’s the total savings?\"]',\n",
    "            'analyze_math_question[\"What’s 10% annual growth over 3 years?\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"search_arxiv\": {\n",
    "        \"description\": \"Search academic papers on arXiv for technical or scientific topics\",\n",
    "        \"usage\": 'search_arxiv[\"FHIR interoperability\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'search_arxiv[\"Zero Trust network security\"]',\n",
    "            'search_arxiv[\"AI for EHR classification\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"auto_check_for_academic_support\": {\n",
    "        \"description\": \"Auto-evaluate if this section needs scientific citation. If yes, run arXiv search.\",\n",
    "        \"usage\": \"auto_check_for_academic_support\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"auto_check_for_academic_support\"],\n",
    "    },\n",
    "    \"should_cite\": {\n",
    "        \"description\": \"Evaluate whether a statement should be supported by a citation\",\n",
    "        \"usage\": 'should_cite[\"statement text\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'should_cite[\"Zero Trust improves security\"]',\n",
    "            'should_cite[\"Migrating in 12 weeks is realistic\"]',\n",
    "        ],\n",
    "    },\n",
    "    \"auto_fill_gaps_with_research\": {\n",
    "        \"description\": \"Expands vague or incomplete sections using search and reasoning\",\n",
    "        \"usage\": \"auto_fill_gaps_with_research\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"auto_fill_gaps_with_research\"],\n",
    "    },\n",
    "    \"upgrade_section_with_research\": {\n",
    "        \"description\": \"Scans the section for weak or vague claims, determines if they need a citation, and improves them using external research.\",\n",
    "        \"usage\": \"upgrade_section_with_research\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"upgrade_section_with_research\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global tools available for ReActConsultantAgent to call regardless of the section\n",
    "global_tools = [\n",
    "    \"keyword_match_in_section\",\n",
    "    \"ask_question\",\n",
    "    \"recommend_fix\",\n",
    "    \"flag_risk\",\n",
    "    \"tool_help\",\n",
    "    \"search_report\",\n",
    "    \"search_web\",\n",
    "    \"check_for_jargon\",\n",
    "    \"generate_client_questions\",\n",
    "    \"check_readability\",\n",
    "    \"search_wikipedia\",\n",
    "    \"analyze_tone_textblob\",\n",
    "    \"search_serpapi\",\n",
    "    \"extract_named_entities\",\n",
    "    \"search_arxiv\",\n",
    "    \"should_cite\",\n",
    "    \"upgrade_section_with_research\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tool priority map for each section\n",
    "tool_priority_map = {\n",
    "    \"Introduction\": {\"primary\": [], \"optional\": []},\n",
    "    \"Summary\": {\n",
    "        \"primary\": [\"check_summary_support\", \"check_readability\"],\n",
    "        \"optional\": [\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"analyze_tone_textblob\",\n",
    "            \"extract_named_entities\",\n",
    "        ],\n",
    "    },\n",
    "    \"Goals & Objectives\": {\n",
    "        \"primary\": [\"evaluate_smart_goals\"],\n",
    "        \"optional\": [\"check_guideline\"],\n",
    "    },\n",
    "    \"Current State Assessment\": {\n",
    "        \"primary\": [\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"check_for_jargon\",\n",
    "            \"flag_risk\",\n",
    "            \"search_wikipedia\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\"check_guideline\", \"search_serpapi\", \"extract_named_entities\"],\n",
    "    },\n",
    "    \"Future State\": {\n",
    "        \"primary\": [\n",
    "            \"search_arxiv\",\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"check_guideline\",\n",
    "            \"flag_risk\",\n",
    "            \"recommend_fix\",\n",
    "            \"search_wikipedia\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "            \"upgrade_section_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\n",
    "            \"compare_with_other_section\",\n",
    "            \"check_guideline\",\n",
    "            \"search_serpapi\",\n",
    "            \"analyze_tone_textblob\",\n",
    "            \"extract_named_entities\",\n",
    "        ],\n",
    "    },\n",
    "    \"Key Recommendations\": {\n",
    "        \"primary\": [\n",
    "            \"search_arxiv\",\n",
    "            \"check_recommendation_alignment\",\n",
    "            \"flag_risk\",\n",
    "            \"recommend_fix\",\n",
    "            \"check_readability\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "            \"upgrade_section_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\n",
    "            \"check_guideline\",\n",
    "            \"generate_client_questions\",\n",
    "            \"search_serpapi\",\n",
    "            \"extract_named_entities\",\n",
    "        ],\n",
    "    },\n",
    "    \"Implementation Plan\": {\n",
    "        \"primary\": [\n",
    "            \"check_timeline_feasibility\",\n",
    "            \"compare_with_other_section\",\n",
    "            \"flag_risk\",\n",
    "            \"recommend_fix\",\n",
    "            \"search_wikipedia\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "            \"upgrade_section_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"search_serpapi\",\n",
    "            \"extract_named_entities\",\n",
    "        ],\n",
    "    },\n",
    "    \"Timeline\": {  # Legacy alias\n",
    "        \"primary\": [\n",
    "            \"check_timeline_feasibility\",\n",
    "            \"flag_risk\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\"search_serpapi\"],\n",
    "    },\n",
    "    \"Benefits\": {\n",
    "        \"primary\": [\n",
    "            \"search_arxiv\",\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\n",
    "            \"recommend_fix\",\n",
    "            \"analyze_tone_textblob\",\n",
    "            \"search_serpapi\",\n",
    "            \"extract_named_entities\",\n",
    "        ],\n",
    "    },\n",
    "    \"Costs\": {\n",
    "        \"primary\": [\n",
    "            \"search_arxiv\",\n",
    "            \"check_alignment_with_goals\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\"recommend_fix\", \"search_serpapi\"],\n",
    "    },\n",
    "    \"Resources\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\", \"auto_fill_gaps_with_research\"],\n",
    "        \"optional\": [\"compare_with_other_section\"],\n",
    "    },\n",
    "    \"Risks & Mitigations\": {\n",
    "        \"primary\": [\n",
    "            \"flag_risk\",\n",
    "            \"search_arxiv\",\n",
    "            \"should_cite\",\n",
    "            \"auto_fill_gaps_with_research\",\n",
    "        ],\n",
    "        \"optional\": [\"check_guideline\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tool_hints(agent):\n",
    "    \"\"\"\n",
    "    Builds a hint message and a combined list of tools for the ReActConsultantAgent based on the section being reviewed.\n",
    "\n",
    "    Purpose:\n",
    "    This function generates a hint message prioritizing tools for the agent to use based on the section being reviewed. It combines primary, optional, and global tools into a sorted list.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the section name being reviewed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Maps the section name to its canonical form using the map_section_to_canonical function.\n",
    "    2. Retrieves the primary and optional tools for the canonical section from the tool_priority_map.\n",
    "    3. Combines the primary, optional, and global tools into a sorted list.\n",
    "    4. Constructs a hint message prioritizing the primary, optional, and global tools.\n",
    "    5. Returns the hint message and the combined list of tools.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the hint message (str) and the combined list of tools (list).\n",
    "    \"\"\"\n",
    "    canonical = map_section_to_canonical(agent.section_name)\n",
    "    priorities = tool_priority_map.get(canonical, {})\n",
    "\n",
    "    primary = priorities.get(\"primary\", [])\n",
    "    optional = priorities.get(\"optional\", [])\n",
    "\n",
    "    combined_tools = sorted(set(primary + optional + global_tools))\n",
    "\n",
    "    hint = \"You may use any tool, but prioritize:\\n\"\n",
    "    for tool in primary:\n",
    "        hint += f\"- {tool} ✅ (primary)\\n\"\n",
    "    for tool in optional:\n",
    "        hint += f\"- {tool} ◽️ (optional)\\n\"\n",
    "    for tool in global_tools:\n",
    "        if tool not in primary and tool not in optional:\n",
    "            hint += f\"- {tool} 🌐 (global)\\n\"\n",
    "\n",
    "    return hint, combined_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the tool catalog for display in the prompt for consumption by LLM\n",
    "\n",
    "\n",
    "def format_tool_catalog_for_prompt(tool_catalog):\n",
    "    \"\"\"\n",
    "    Formats the tool catalog for display in the prompt for consumption by LLM.\n",
    "\n",
    "    Purpose:\n",
    "    This function formats the tool catalog into a human-readable string that lists each tool along with its description and usage. This formatted string can be used in prompts for language models to understand the available tools.\n",
    "\n",
    "    Parameters:\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description' and 'usage'.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a list with the header \"Available tools:\".\n",
    "    2. Iterates through each tool in the tool_catalog dictionary.\n",
    "    3. For each tool, appends its name, description, and usage to the list.\n",
    "    4. Joins the list into a single string with newline characters.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string listing all tools with their descriptions and usage.\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        \"\\n(You may use these tools if you believe they apply — but prioritize the tools listed above.)\\n\\n\"\n",
    "    ]\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        lines.append(f\"- {tool} (v{meta['version']}): {meta['description']}\")\n",
    "        lines.append(f\"  Usage: {meta['usage']}\")\n",
    "        if meta.get(\"examples\"):\n",
    "            for ex in meta[\"examples\"]:\n",
    "                lines.append(f\"  Example: {ex}\")\n",
    "        lines.append(\"\")  # spacing between tools\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tools_list(tool_catalog):\n",
    "    \"\"\"\n",
    "    Formats the list of tools from the tool_catalog dictionary as a comma-separated string.\n",
    "\n",
    "    Parameters:\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata.\n",
    "\n",
    "    Returns:\n",
    "    str: A comma-separated string of tool names.\n",
    "    \"\"\"\n",
    "    tools_list = \", \".join(tool_catalog.keys())\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "# Example usage\n",
    "formatted_tools = format_tools_list(tool_catalog)\n",
    "print(formatted_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #1: Check for best practices in a given section\n",
    "\n",
    "# Simulated best practices reference data\n",
    "best_practices = {\n",
    "    \"cloud security\": \"Follow NIST Cybersecurity Framework. Include access control, encryption at rest/in-transit, and regular audits.\",\n",
    "    \"data governance\": \"Establish data stewards, quality standards, lifecycle rules, and metadata documentation.\",\n",
    "    \"migration\": \"Use phased migration, sandbox testing, rollback planning, and stakeholder communication.\",\n",
    "}\n",
    "\n",
    "\n",
    "def check_guideline(topic):\n",
    "    \"\"\"\n",
    "    Checks for best practices related to a given topic.\n",
    "\n",
    "    Purpose:\n",
    "    This function looks up best practices for a specified topic from a predefined dictionary of best practices.\n",
    "\n",
    "    Parameters:\n",
    "    topic (str): The topic for which best practices are to be checked.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function converts the topic to lowercase to ensure case-insensitive matching.\n",
    "    2. It looks up the topic in the `best_practices` dictionary.\n",
    "    3. If a matching guideline is found, it returns the guideline.\n",
    "    4. If no matching guideline is found, it returns a message indicating that no matching guideline was found.\n",
    "\n",
    "    Returns:\n",
    "    str: The best practice guideline for the specified topic, or a message indicating no matching guideline was found.\n",
    "    \"\"\"\n",
    "    return best_practices.get(topic.lower(), \"No matching guideline found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #2: Keyword matching in a section\n",
    "# This tool helps the agent check if a keyword or concept is explicitly mentioned in the section.\n",
    "# This is great for validating whether the report includes key elements (e.g., \"encryption\", \"stakeholders\", \"Zero Trust\").\n",
    "\n",
    "\n",
    "def keyword_match_in_section(term, section_text):\n",
    "    \"\"\"\n",
    "    Checks if a keyword or concept is explicitly mentioned in a section of the report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps validate whether the report includes key elements by checking if a specified keyword or concept is mentioned in the section text.\n",
    "\n",
    "    Parameters:\n",
    "    term (str): The keyword or concept to search for in the section.\n",
    "    section_text (str): The text of the section to search within.\n",
    "\n",
    "    Workflow:\n",
    "    1. Converts the keyword and section text to lowercase to ensure case-insensitive matching.\n",
    "    2. Checks if the keyword is present in the section text.\n",
    "    3. Returns a message indicating whether the keyword was found or not.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating whether the keyword was found in the section.\n",
    "    \"\"\"\n",
    "    term_lower = term.lower()\n",
    "    if term_lower in section_text.lower():\n",
    "        return f\"The keyword '{term}' was found in the section.\"\n",
    "    else:\n",
    "        return f\"The keyword '{term}' was NOT found in the section.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #3: Check feasibility of timeline for IT migration\n",
    "# This tool helps the agent assess the feasibility of a timeline for an IT migration project.\n",
    "# It checks if the timeline is too short, potentially feasible, or reasonable for a full migration.\n",
    "\n",
    "\n",
    "def check_timeline_feasibility(duration_str):\n",
    "    \"\"\"\n",
    "    Checks the feasibility of a timeline for an IT migration project.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps assess whether a given timeline for an IT migration project is too short, potentially feasible, or reasonable.\n",
    "\n",
    "    Parameters:\n",
    "    duration_str (str): The timeline duration as a string (e.g., \"6-12 months\", \"8 to 10 weeks\", \"a few months\").\n",
    "\n",
    "    Workflow:\n",
    "    1. Converts the duration string to lowercase and strips any leading/trailing whitespace.\n",
    "    2. Initializes a dictionary of fuzzy terms (e.g., \"a few\", \"several\") with their estimated numeric values.\n",
    "    3. Checks if the duration string contains any fuzzy terms and estimates the duration in months.\n",
    "    4. If no fuzzy terms are found, checks for ranges (e.g., \"6-12 months\") or single values (e.g., \"6 months\") and calculates the average duration in months.\n",
    "    5. If the duration cannot be parsed, returns a warning message.\n",
    "    6. Evaluates the feasibility of the timeline based on the calculated duration in months:\n",
    "       - If less than 3 months, returns that the timeline is likely too short.\n",
    "       - If between 3 and 12 months, returns that the timeline is potentially feasible.\n",
    "       - If more than 12 months, returns that the timeline seems reasonable.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the feasibility of the timeline.\n",
    "    \"\"\"\n",
    "    duration_str = duration_str.lower().strip()\n",
    "\n",
    "    fuzzy_terms = {\n",
    "        \"a few\": 3,\n",
    "        \"a couple\": 2,\n",
    "        \"several\": 6,\n",
    "        \"some\": 4,\n",
    "        \"many\": 9,\n",
    "        \"a handful\": 5,\n",
    "    }\n",
    "\n",
    "    months = None\n",
    "\n",
    "    # Check for fuzzy terms like \"a few months\"\n",
    "    for fuzzy_word, estimated_num in fuzzy_terms.items():\n",
    "        if fuzzy_word in duration_str:\n",
    "            if \"week\" in duration_str:\n",
    "                months = estimated_num / 4\n",
    "            elif \"month\" in duration_str:\n",
    "                months = estimated_num\n",
    "            break\n",
    "\n",
    "    # Check for ranges like \"6-12 months\" or \"8 to 10 weeks\"\n",
    "    if months is None:\n",
    "        range_match = re.match(r\"(\\d+)\\s*[-to]+\\s*(\\d+)\\s*(weeks|months)\", duration_str)\n",
    "        single_match = re.match(r\"(\\d+)\\s*(weeks|months)\", duration_str)\n",
    "\n",
    "        try:\n",
    "            if range_match:\n",
    "                start = int(range_match.group(1))\n",
    "                end = int(range_match.group(2))\n",
    "                unit = range_match.group(3)\n",
    "                avg = (start + end) / 2\n",
    "                months = avg / 4 if \"week\" in unit else avg\n",
    "\n",
    "            elif single_match:\n",
    "                num = int(single_match.group(1))\n",
    "                unit = single_match.group(2)\n",
    "                months = num / 4 if \"week\" in unit else num\n",
    "        except:\n",
    "            return \"⚠️ Could not parse timeline value.\"\n",
    "\n",
    "    if months is None:\n",
    "        return \"⚠️ Could not understand the timeline. Use phrases like '6 months', '8-12 weeks', or 'a few months'.\"\n",
    "\n",
    "    # Evaluate the feasibility\n",
    "    if months < 3:\n",
    "        return (\n",
    "            f\"The timeline ({duration_str}) is likely too short for a full migration.\"\n",
    "        )\n",
    "    elif 3 <= months <= 12:\n",
    "        return f\"The timeline ({duration_str}) is potentially feasible depending on complexity.\"\n",
    "    else:\n",
    "        return (\n",
    "            f\"The timeline ({duration_str}) seems reasonable for a full IT migration.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #4: Search for a term in the entire report\n",
    "# This tool helps the agent search for a specific term in the entire consulting report.\n",
    "# It returns the sections where the term was found, if any.\n",
    "\n",
    "\n",
    "def search_report(term, report_sections):\n",
    "    \"\"\"\n",
    "    Searches for a specific term in the entire consulting report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps the agent search for a specific term in the entire consulting report and returns the sections where the term was found, if any.\n",
    "\n",
    "    Parameters:\n",
    "    term (str): The term to search for in the report.\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `found_in` to store the sections where the term is found.\n",
    "    2. Iterates through each section in the `report_sections` dictionary.\n",
    "    3. For each section, checks if the term (case-insensitive) is present in the section content.\n",
    "    4. If the term is found, appends the section header to the `found_in` list.\n",
    "    5. After checking all sections, returns a message indicating the sections where the term was found or a message indicating that the term was not found.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the sections where the term was found or a message indicating that the term was not found.\n",
    "    \"\"\"\n",
    "    found_in = []\n",
    "    for section, content in report_sections.items():\n",
    "        if term.lower() in content.lower():\n",
    "            found_in.append(section)\n",
    "    if found_in:\n",
    "        return f\"The term '{term}' was found in: {', '.join(found_in)}.\"\n",
    "    else:\n",
    "        return f\"The term '{term}' was NOT found anywhere in the report.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool picker: Pick tools based on intent description\n",
    "# This function picks tools from the tool catalog based on the intent description.\n",
    "# It searches through the tool catalog and returns a list of tools whose descriptions match the given intent description.\n",
    "\n",
    "\n",
    "def pick_tool_by_intent(intent_description, tool_catalog):\n",
    "    \"\"\"\n",
    "    Picks tools from the tool catalog based on the intent description.\n",
    "\n",
    "    Purpose:\n",
    "    This function searches through the tool catalog and returns a list of tools whose descriptions match the given intent description.\n",
    "\n",
    "    Parameters:\n",
    "    intent_description (str): A description of the intent to match against tool descriptions.\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description'.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `matches` to store the names of matching tools.\n",
    "    2. Iterates through each tool in the `tool_catalog` dictionary.\n",
    "    3. For each tool, checks if the `intent_description` is present in the tool's description (case-insensitive).\n",
    "    4. If a match is found, appends the tool name to the `matches` list.\n",
    "    5. Returns the list of matching tools.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tool names whose descriptions match the given intent description.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        if intent_description.lower() in meta[\"description\"].lower():\n",
    "            matches.append(tool)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool picker: Pick tools based on fuzzy intent description matching\n",
    "# This function picks tools from the tool catalog based on a fuzzy intent description matching.\n",
    "# It uses fuzzy string matching to find tools whose descriptions closely match the given intent description.\n",
    "\n",
    "\n",
    "def pick_tool_by_intent_fuzzy(intent_description, tool_catalog, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Picks tools from the tool catalog based on a fuzzy intent description matching.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses fuzzy string matching to find tools whose descriptions closely match the given intent description.\n",
    "\n",
    "    Parameters:\n",
    "    intent_description (str): A description of the intent to match against tool descriptions.\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description'.\n",
    "    threshold (float): The minimum similarity ratio (between 0 and 1) to consider a match. Default is 0.3.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `matches` to store the names and similarity ratios of matching tools.\n",
    "    2. Iterates through each tool in the `tool_catalog` dictionary.\n",
    "    3. For each tool, calculates the similarity ratio between the `intent_description` and the tool's description using `SequenceMatcher`.\n",
    "    4. If the similarity ratio exceeds the `threshold`, appends the tool name and ratio to the `matches` list.\n",
    "    5. Sorts the `matches` list by similarity ratio in descending order.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples, where each tuple contains a tool name and its similarity ratio, sorted by best match.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        ratio = SequenceMatcher(\n",
    "            None, intent_description.lower(), meta[\"description\"].lower()\n",
    "        ).ratio()\n",
    "        if ratio > threshold:\n",
    "            matches.append((tool, round(ratio, 2)))\n",
    "    return sorted(matches, key=lambda x: -x[1])  # sort by best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tools_by_priority(tool_usage, section_tool_map, global_tools):\n",
    "    \"\"\"\n",
    "    Categorizes each tool usage as Global, Primary, or Optional.\n",
    "\n",
    "    Purpose:\n",
    "    This function categorizes the usage of tools into four categories: Global, Primary, Optional, and Uncategorized. It helps in understanding the distribution of tool usage based on their priority and scope.\n",
    "\n",
    "    Parameters:\n",
    "    tool_usage (dict): A dictionary where keys are tool names and values are their respective usage counts.\n",
    "    section_tool_map (dict): A dictionary where keys are section names and values are dictionaries containing 'primary' and 'optional' tools for each section.\n",
    "    global_tools (list): A list of tools that are considered global and can be used across any section.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a dictionary `categorized` with keys 'Global', 'Primary', 'Optional', and 'Uncategorized', each containing an empty dictionary.\n",
    "    2. Iterates through each tool and its count in the `tool_usage` dictionary.\n",
    "    3. Checks if the tool is in the `global_tools` list. If yes, adds it to the 'Global' category and marks it as matched.\n",
    "    4. Iterates through the `section_tool_map` to check if the tool is listed as 'primary' or 'optional' for any section. If found, adds it to the respective category and marks it as matched.\n",
    "    5. If the tool is not matched in any category, adds it to the 'Uncategorized' category.\n",
    "    6. Returns the `categorized` dictionary containing the categorized tool usage.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are categories ('Global', 'Primary', 'Optional', 'Uncategorized') and values are dictionaries of tools and their usage counts.\n",
    "    \"\"\"\n",
    "    categorized = {\"Global\": {}, \"Primary\": {}, \"Optional\": {}, \"Uncategorized\": {}}\n",
    "\n",
    "    for tool, count in tool_usage.items():\n",
    "        matched = False\n",
    "        # Check if tool is in global\n",
    "        if tool in global_tools:\n",
    "            categorized[\"Global\"][tool] = count\n",
    "            matched = True\n",
    "        # Check if tool is in primary/optional across any section\n",
    "        for priorities in section_tool_map.values():\n",
    "            if tool in priorities.get(\"primary\", []):\n",
    "                categorized[\"Primary\"][tool] = count\n",
    "                matched = True\n",
    "            elif tool in priorities.get(\"optional\", []):\n",
    "                categorized[\"Optional\"][tool] = count\n",
    "                matched = True\n",
    "        if not matched:\n",
    "            categorized[\"Uncategorized\"][tool] = count\n",
    "\n",
    "    return categorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tool_usage(agent):\n",
    "    \"\"\"\n",
    "    Prints the usage of tools categorized by their priority.\n",
    "\n",
    "    Purpose:\n",
    "    This function categorizes the usage of tools into four categories: Primary, Optional, Global, and Uncategorized. It then prints the usage count of each tool within these categories.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the tool usage data to be categorized and printed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the categorize_tools_by_priority function to categorize the tools based on their priority.\n",
    "    2. Iterates through each category (Primary, Optional, Global, Uncategorized).\n",
    "    3. For each category, retrieves the tools and their usage counts.\n",
    "    4. Prints the category name and the usage count of each tool within the category, sorted by usage count in descending order.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Tool Usage by Category:\")\n",
    "\n",
    "    categorized = categorize_tools_by_priority(\n",
    "        agent.tool_usage, tool_priority_map, global_tools\n",
    "    )\n",
    "\n",
    "    for category in [\"Primary\", \"Optional\", \"Global\", \"Uncategorized\"]:\n",
    "        tools = categorized[category]\n",
    "        if tools:\n",
    "            print(f\"\\n🔹 {category} Tools:\")\n",
    "            for tool, count in sorted(tools.items(), key=lambda x: -x[1]):\n",
    "                print(f\"  - {tool}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tool_usage(tool_usage_dict, title=\"Tool Usage Summary\"):\n",
    "    \"\"\"\n",
    "    Plots the usage of tools categorized by their priority.\n",
    "\n",
    "    Purpose:\n",
    "    This function visualizes the usage of tools in a horizontal bar chart, categorized by their priority (Primary, Optional, Global, Uncategorized). It helps in understanding the distribution of tool usage based on their priority and scope.\n",
    "\n",
    "    Parameters:\n",
    "    tool_usage_dict (dict): A dictionary where keys are tool names and values are their respective usage counts.\n",
    "    title (str): The title of the plot. Default is \"Tool Usage Summary\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the categorize_tools_by_priority function to categorize the tools based on their priority.\n",
    "    2. Initializes lists to store tool names, usage counts, and colors for plotting.\n",
    "    3. Iterates through each category (Primary, Optional, Global, Uncategorized) and appends the tool names, counts, and corresponding colors to the lists.\n",
    "    4. Creates a horizontal bar plot using matplotlib with the tool names, counts, and colors.\n",
    "    5. Adds usage count labels to each bar.\n",
    "    6. Inverts the y-axis to display the highest usage count at the top.\n",
    "    7. Adds a legend to the plot indicating the tool categories.\n",
    "    8. Displays the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    categorized = categorize_tools_by_priority(\n",
    "        tool_usage_dict, tool_priority_map, global_tools\n",
    "    )\n",
    "\n",
    "    # Combine for plotting\n",
    "    tools, counts, colors = [], [], []\n",
    "    color_map = {\n",
    "        \"Primary\": \"green\",\n",
    "        \"Optional\": \"orange\",\n",
    "        \"Global\": \"blue\",\n",
    "        \"Uncategorized\": \"gray\",\n",
    "    }\n",
    "\n",
    "    for category in [\"Primary\", \"Optional\", \"Global\", \"Uncategorized\"]:\n",
    "        for tool, count in categorized[category].items():\n",
    "            tools.append(tool)\n",
    "            counts.append(count)\n",
    "            colors.append(color_map[category])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(tools, counts, color=colors)\n",
    "    plt.xlabel(\"Usage Count\")\n",
    "    plt.title(title)\n",
    "\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width + 0.2,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            str(int(width)),\n",
    "            va=\"center\",\n",
    "        )\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add legend\n",
    "    legend_patches = [mpatches.Patch(color=color_map[c], label=c) for c in color_map]\n",
    "    plt.legend(handles=legend_patches, title=\"Tool Category\", loc=\"lower right\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to search the web for relevant information\n",
    "\n",
    "\n",
    "def search_web(query, max_results=1):\n",
    "    \"\"\"\n",
    "    Searches the web for relevant information using DuckDuckGo.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the DuckDuckGo search engine to find relevant information based on a given query. It returns the snippet of the first search result.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query to find relevant information.\n",
    "    max_results (int): The maximum number of search results to retrieve. Default is 1.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a DuckDuckGo search session using DDGS.\n",
    "    2. Performs a text search with the given query and retrieves up to `max_results` results.\n",
    "    3. Iterates through the search results and returns the snippet of the first result.\n",
    "    4. If no results are found, returns a message indicating no relevant results were found.\n",
    "    5. If an exception occurs during the search, returns a message indicating the web search failed along with the exception message.\n",
    "\n",
    "    Returns:\n",
    "    str: The snippet of the first search result, or a message indicating no relevant results were found or the web search failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.text(query, max_results=max_results)\n",
    "            for r in results:\n",
    "                return r[\"body\"]  # return the first result's snippet\n",
    "        return \"No relevant results found.\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Web search failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check for jargon or technical terms in a section\n",
    "\n",
    "JARGON_LIST = {\n",
    "    \"synergy\",\n",
    "    \"leverage\",\n",
    "    \"optimize\",\n",
    "    \"stakeholder alignment\",\n",
    "    \"enablement\",\n",
    "    \"digital transformation\",\n",
    "    \"bandwidth\",\n",
    "    \"scalability\",\n",
    "    \"paradigm\",\n",
    "    \"blockchain\",\n",
    "    \"AI\",\n",
    "    \"ML\",\n",
    "    \"IoT\",\n",
    "    \"Zero Trust\",\n",
    "    \"DevOps\",\n",
    "    \"infrastructure-as-code\",\n",
    "    \"EHR\",\n",
    "    \"CRM\",\n",
    "    \"VPN\",\n",
    "    \"cloud-native\",\n",
    "    \"containerization\",\n",
    "    \"agile methodology\",\n",
    "}\n",
    "\n",
    "\n",
    "def check_for_jargon(section_text):\n",
    "    \"\"\"\n",
    "    Checks for jargon or technical terms in a section of the report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps identify the presence of jargon or technical terms in a section of the report by searching for predefined terms.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to search within.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `found_terms` to store the jargon or technical terms found in the section.\n",
    "    2. Iterates through each term in the `JARGON_LIST` set.\n",
    "    3. For each term, constructs a regex pattern to match the term as a whole word (case-insensitive).\n",
    "    4. Searches for the term in the section text using the regex pattern.\n",
    "    5. If the term is found, appends it to the `found_terms` list.\n",
    "    6. After checking all terms, returns a message indicating the jargon or technical terms found or a message indicating that no notable jargon or technical terms were found.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the jargon or technical terms found in the section, or a message indicating that no notable jargon or technical terms were found.\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    for term in JARGON_LIST:\n",
    "        pattern = r\"\\b\" + re.escape(term) + r\"\\b\"\n",
    "        if re.search(pattern, section_text, flags=re.IGNORECASE):\n",
    "            found_terms.append(term)\n",
    "    if found_terms:\n",
    "        return (\n",
    "            f\"The section includes jargon or technical terms: {', '.join(found_terms)}.\"\n",
    "        )\n",
    "    else:\n",
    "        return \"No notable jargon or technical terms found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to generate client questions based on a section of a report\n",
    "def generate_client_questions(section_text, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Generates client questions based on a section of an IT strategy report.\n",
    "\n",
    "    Purpose:\n",
    "    This function acts as a skeptical client reviewing a section of an IT strategy report and generates 3-5 clarifying or challenging questions based on potential assumptions, unclear terms, or missing context.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to generate questions for.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that instructs the model to act as a skeptical client and generate questions based on the section text.\n",
    "    2. Creates a list of messages with the constructed prompt.\n",
    "    3. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    4. If the API call is successful, returns the generated questions.\n",
    "    5. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated questions or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are acting as a skeptical client reviewing the following section of an IT strategy report.\\n\"\n",
    "        \"Generate 3-5 clarifying or challenging questions the client might ask based on potential assumptions, unclear terms, or missing context.\\n\\n\"\n",
    "        f\"Section:\\n{section_text}\\n\\n\"\n",
    "        \"Questions:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate questions: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check report has expected sections\n",
    "def highlight_missing_sections(report_sections):\n",
    "    \"\"\"\n",
    "    Compares expected canonical (standard) sections against the actual report_sections keys.\n",
    "    Returns a list of missing expected sections.\n",
    "    \"\"\"\n",
    "    expected_sections = set(canonical_section_map.keys())\n",
    "    present_sections = set(report_sections.keys())\n",
    "\n",
    "    missing = expected_sections - present_sections\n",
    "    if missing:\n",
    "        return \"🚨 Missing sections:\\n\" + \"\\n\".join(f\"- {s}\" for s in sorted(missing))\n",
    "    else:\n",
    "        return \"✅ All expected sections are present.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check alignment between section and report goals\n",
    "def check_alignment_with_goals(\n",
    "    section_name, report_sections_dict, model=\"gpt-3.5-turbo\", temperature=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Checks the alignment between the goals of the report and a specific section.\n",
    "\n",
    "    Purpose:\n",
    "    This function evaluates whether a specific section of the report aligns with the stated goals and objectives. It uses the OpenAI API to generate an evaluation of the alignment.\n",
    "\n",
    "    Parameters:\n",
    "    section_name (str): The name of the section to evaluate for alignment.\n",
    "    report_sections_dict (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Tries to find the \"Goals & Objectives\" section in the report.\n",
    "    2. If the \"Goals & Objectives\" section is not found, searches for goals in other sections based on keywords.\n",
    "    3. Retrieves the text of the specified section to evaluate.\n",
    "    4. If either the goals or the section text is not found, returns a warning message.\n",
    "    5. Constructs a prompt for the OpenAI API to evaluate the alignment between the goals and the specified section.\n",
    "    6. Calls the OpenAI API with tracking to get the evaluation.\n",
    "    7. Returns the evaluation or an error message if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "    str: The evaluation of the alignment between the goals and the specified section, or an error message if the API call fails.\n",
    "    \"\"\"\n",
    "    # Step 1: Try exact match first\n",
    "    try:\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to check alignment: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to compare two sections of a report for duplication, contradictions, or inconsistencies\n",
    "# This tool compares two sections of an IT consulting report for duplication, contradictions, or inconsistencies.\n",
    "# It also notes if one section covers content that the other should include.\n",
    "def compare_with_other_section(\n",
    "    section_a, section_b, report_sections_dict, model=\"gpt-3.5-turbo\", temperature=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares two sections of an IT consulting report for duplication, contradictions, or inconsistencies.\n",
    "\n",
    "    Purpose:\n",
    "    This function compares two specified sections of an IT consulting report to identify any duplication, contradictions, or inconsistencies between them. It also notes if one section covers content that the other should include.\n",
    "\n",
    "    Parameters:\n",
    "    section_a (str): The name of the first section to compare.\n",
    "    section_b (str): The name of the second section to compare.\n",
    "    report_sections_dict (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the text of the specified sections from the report_sections_dict.\n",
    "    2. If either section is not found, returns a warning message.\n",
    "    3. Constructs a prompt for the OpenAI API to compare the two sections.\n",
    "    4. Calls the OpenAI API with tracking to get the comparison.\n",
    "    5. Returns the comparison or an error message if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "    str: A summary of the comparison between the two sections, or an error message if the API call fails.\n",
    "    \"\"\"\n",
    "    text_a = report_sections_dict.get(section_a)\n",
    "    text_b = report_sections_dict.get(section_b)\n",
    "\n",
    "    if not text_a or not text_b:\n",
    "        return f\"⚠️ One or both sections not found: '{section_a}' or '{section_b}'\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are comparing two sections of an IT consulting report.\\n\"\n",
    "        f\"Identify any duplication, contradictions, or inconsistencies between them.\\n\"\n",
    "        f\"Also note if one section covers content the other should include.\\n\\n\"\n",
    "        f\"Section A: {section_a}\\n{text_a}\\n\\n\"\n",
    "        f\"Section B: {section_b}\\n{text_b}\\n\\n\"\n",
    "        \"Provide a 3-5 sentence summary of your comparison:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to compare sections: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show agent memory\n",
    "def show_agent_memory(agent):\n",
    "    \"\"\"\n",
    "    Displays a snapshot of the agent's memory, including section notes, cross-section observations, and tool usage history.\n",
    "\n",
    "    Purpose:\n",
    "    This function provides a detailed view of the agent's internal memory, which includes notes for each section, observations comparing different sections, and the history of tool usage. This is useful for understanding the agent's reasoning process and the actions it has taken.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be displayed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Prints a header \"Agent Memory Snapshot\".\n",
    "    2. Prints the section notes stored in the agent's memory.\n",
    "    3. Prints the cross-section observations stored in the agent's memory.\n",
    "    4. Prints the tool usage history stored in the agent's memory.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\n🧠 Agent Memory Snapshot\\n\")\n",
    "\n",
    "    print(\"🔹 Section Notes:\")\n",
    "    for section, notes in agent.memory[\"section_notes\"].items():\n",
    "        print(f\"- {section}:\")\n",
    "        for note in notes:\n",
    "            print(f\"  • {note}\")\n",
    "\n",
    "    print(\"\\n🔹 Cross-Section Observations:\")\n",
    "    for a, b, obs in agent.memory[\"cross_section_flags\"]:\n",
    "        print(f\"- {a} vs. {b}: {obs}\")\n",
    "\n",
    "    print(\"\\n🔹 Tool History:\")\n",
    "    for step, action, section in agent.memory[\"tool_history\"]:\n",
    "        print(f\"Step {step} | {action} | Section: {section}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generates a final summary for the client based on the agent's memory of section insights and cross-section observations.\n",
    "\n",
    "    Purpose:\n",
    "    This function constructs a prompt using the agent's memory of section insights and cross-section observations to generate a final summary for the client. It uses the OpenAI API to create a concise summary covering strengths, issues, and overall alignment with goals.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be used for generating the summary.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves section notes and cross-section observations from the agent's memory.\n",
    "    2. Constructs a prompt that includes the section insights and cross-section observations.\n",
    "    3. Adds instructions to write a short, clear 4-6 sentence final summary covering strengths, issues, and overall alignment with goals.\n",
    "    4. Creates a list of messages with the constructed prompt.\n",
    "    5. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    6. If the API call is successful, returns the generated summary.\n",
    "    7. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated final summary or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    # Build a summary prompt using memory\n",
    "    notes_by_section = agent.memory.get(\"section_notes\", {})\n",
    "    cross_section = agent.memory.get(\"cross_section_flags\", [])\n",
    "\n",
    "    prompt = \"You are a senior consultant wrapping up your review of an IT strategy report.\\n\"\n",
    "    prompt += \"Use the following section insights and cross-section observations to write a final summary for the client.\\n\\n\"\n",
    "\n",
    "    for section, notes in notes_by_section.items():\n",
    "        prompt += f\"Section: {section}\\n\"\n",
    "        for note in notes:\n",
    "            prompt += f\"- {note}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    if cross_section:\n",
    "        prompt += \"Cross-Section Findings:\\n\"\n",
    "        for a, b, obs in cross_section:\n",
    "            prompt += f\"- {a} vs. {b}: {obs}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    # Include overall score summary\n",
    "    if \"section_scores\" in agent.memory:\n",
    "        prompt += \"\\nOverall Ratings:\\n\"\n",
    "        for section, score_text in agent.memory[\"section_scores\"].items():\n",
    "            prompt += f\"{section}:\\n{score_text}\\n\\n\"\n",
    "\n",
    "    prompt += \"Write a short, clear 4-6 sentence final summary covering strengths, issues, and overall alignment with goals.\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    try:\n",
    "        return call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        ).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate final summary: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_report_to_markdown(agent, filename=\"consultant_ai_report.md\"):\n",
    "    \"\"\"\n",
    "    Exports a polished consulting report review in markdown format.\n",
    "    Structure:\n",
    "    1. Final Summary\n",
    "    2. Top Issues\n",
    "    3. Cross-Section Findings\n",
    "    4. Missing Sections\n",
    "    5. Section-by-Section Review (notes, ratings, fixes, confidence, rewrite)\n",
    "    6. Citations\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"# 🧾 AI-Powered Consulting Report\\n\")\n",
    "\n",
    "        # 1. Final Summary\n",
    "        f.write(\"\\n## 🧠 Final Summary\\n\")\n",
    "        f.write(agent.memory.get(\"final_summary\", \"No summary generated.\") + \"\\n\")\n",
    "\n",
    "        # 2. Top Issues\n",
    "        if \"top_issues\" in agent.memory:\n",
    "            f.write(\"\\n## 🚨 Top 3 Issues\\n\")\n",
    "            f.write(agent.memory[\"top_issues\"] + \"\\n\")\n",
    "\n",
    "        # 3. Cross-Section Findings\n",
    "        cross_flags = agent.memory.get(\"cross_section_flags\", [])\n",
    "        if cross_flags:\n",
    "            f.write(\"\\n## 🔀 Cross-Section Findings\\n\")\n",
    "            for a, b, obs in cross_flags:\n",
    "                f.write(f\"- **{a} vs. {b}**: {obs}\\n\")\n",
    "\n",
    "        # 4. Missing Sections\n",
    "        if \"highlight_missing\" in agent.memory:\n",
    "            f.write(\"\\n## ❗ Missing Sections\\n\")\n",
    "            f.write(agent.memory[\"highlight_missing\"] + \"\\n\")\n",
    "        if \"missing_analysis\" in agent.memory:\n",
    "            f.write(\"\\n## 📌 Missing Sections Analysis\\n\")\n",
    "            f.write(agent.memory[\"missing_analysis\"] + \"\\n\")\n",
    "\n",
    "        # 5. Section Reviews\n",
    "        write_section_insights(f, agent)\n",
    "\n",
    "        # 6. Citations\n",
    "        f.write(\"\\n\\n## 🔖 Citations & References\\n\")\n",
    "        f.write(format_citations_block(agent) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Markdown report saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_section_insights(f, agent):\n",
    "    \"\"\"\n",
    "    Writes section insights, ratings, fixes, confidence levels, and improved sections to a markdown file.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through the sections stored in the agent's memory and writes detailed insights for each section to a markdown file. It includes notes, ratings, fix recommendations, confidence levels, and optionally improved sections if available.\n",
    "\n",
    "    Parameters:\n",
    "    f (file object): The file object to write the markdown content to.\n",
    "\n",
    "    Workflow:\n",
    "    1. Writes a header for the section insights and ratings.\n",
    "    2. Iterates through each section in the agent's memory.\n",
    "    3. For each section:\n",
    "        - Writes the section name as a subheader.\n",
    "        - Writes the notes for the section, if available.\n",
    "        - Writes the ratings for the section, if available, formatted as a score block.\n",
    "        - Writes fix recommendations for the section, if available.\n",
    "        - Writes the confidence level for the section, if available.\n",
    "        - Writes the improved section content, if available, under a separate subheader.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    f.write(\"\\n\\n## 📚 Section Insights & Ratings\\n\")\n",
    "    for section in agent.memory.get(\"section_notes\", {}).keys():\n",
    "        f.write(f\"\\n### 🔸 {section}\\n\")\n",
    "\n",
    "        # Notes\n",
    "        notes = agent.memory[\"section_notes\"].get(section, [])\n",
    "        if notes:\n",
    "            f.write(\"**Notes:**\\n\")\n",
    "            for note in notes:\n",
    "                f.write(f\"- {note}\\n\")\n",
    "\n",
    "        # Ratings\n",
    "        score_text = agent.memory.get(\"section_scores\", {}).get(section, \"\")\n",
    "        if score_text:\n",
    "            f.write(\"\\n**Ratings:**\\n\")\n",
    "            f.write(f\"{format_score_block(score_text)}\\n\")\n",
    "\n",
    "        # Fixes\n",
    "        fixes = agent.memory.get(\"section_fixes\", {}).get(section, \"\")\n",
    "        if fixes:\n",
    "            f.write(\"\\n**Fix Recommendations:**\\n\")\n",
    "            f.write(f\"{fixes}\\n\")\n",
    "\n",
    "        # Confidence\n",
    "        confidence = agent.memory.get(\"confidence_levels\", {}).get(section, \"\")\n",
    "        if confidence:\n",
    "            f.write(f\"\\n**Confidence Level:** {confidence}/10\\n\")\n",
    "\n",
    "        # Rewritten Section (optional)\n",
    "        if section in agent.memory.get(\"section_upgrades\", {}):\n",
    "            upgrade = agent.memory[\"section_upgrades\"][section]\n",
    "            f.write(\"\\n**💡 Improved Section:**\\n\")\n",
    "            f.write(f\"{upgrade['improved']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def export_report_to_markdown_and_pdf(\n",
    "    agent, markdown_file=\"consultant_ai_report.md\", pdf_file=\"consultant_ai_report.pdf\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports the consulting report review to both markdown and PDF formats.\n",
    "\n",
    "    Purpose:\n",
    "    This function generates a markdown file summarizing the consulting report review and then converts it to a PDF file. It ensures the output directory exists, exports the report to markdown, converts the markdown to HTML, and finally renders the HTML to a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be exported.\n",
    "    markdown_file (str): The name of the markdown file to save. Default is \"consultant_ai_report.md\".\n",
    "    pdf_file (str): The name of the PDF file to save. Default is \"consultant_ai_report.pdf\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Ensures the output directory exists.\n",
    "    2. Exports the report to a markdown file using the `export_report_to_markdown` function.\n",
    "    3. Reads the markdown file and converts its content to HTML.\n",
    "    4. Wraps the HTML content in a basic HTML page structure.\n",
    "    5. Saves the HTML content to a temporary file.\n",
    "    6. Uses Playwright to render the HTML file to a PDF.\n",
    "    7. Handles any exceptions that occur during the PDF rendering process and prints appropriate messages.\n",
    "\n",
    "    Asynchronous Workflow:\n",
    "    1. Opens an async Playwright session.\n",
    "    2. Launches a Chromium browser.\n",
    "    3. Opens a new page in the browser.\n",
    "    4. Navigates to the local HTML file.\n",
    "    5. Generates a PDF from the HTML content.\n",
    "    6. Closes the browser.\n",
    "    7. Prints a success message if the PDF is saved successfully.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_dir = \"../outputs/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    markdown_file = os.path.join(output_dir, markdown_file)\n",
    "    pdf_file = os.path.join(output_dir, pdf_file)\n",
    "\n",
    "    # Step 1: Export to Markdown\n",
    "    export_report_to_markdown(agent, filename=markdown_file)\n",
    "\n",
    "    # Step 2: Convert to HTML\n",
    "    with open(markdown_file, \"r\") as f:\n",
    "        md_text = f.read()\n",
    "    html_content = markdown(md_text)\n",
    "\n",
    "    # Optional: wrap in basic HTML page\n",
    "    full_html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Consulting Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "            h1, h2, h3 {{ color: #003366; }}\n",
    "            ul {{ margin-top: 0; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    {html_content}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Save HTML and render to PDF\n",
    "    temp_html_path = os.path.join(output_dir, \"temp_report.html\")\n",
    "    with open(temp_html_path, \"w\") as f:\n",
    "        f.write(full_html)\n",
    "\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(\"file://\" + os.path.abspath(temp_html_path))\n",
    "            await page.pdf(path=pdf_file, format=\"A4\")\n",
    "            await browser.close()\n",
    "            print(f\"✅ PDF report saved as: {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ PDF export failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_section(\n",
    "    section_name, section_text, goals_text=None, model=\"gpt-3.5-turbo\", temperature=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Scores a section of a consulting report based on clarity, alignment, and completeness.\n",
    "\n",
    "    Purpose:\n",
    "    This function evaluates a specified section of a consulting report using three criteria: clarity, alignment with goals, and completeness. It generates a score out of 10 for each criterion along with a one-line explanation for each score.\n",
    "\n",
    "    Parameters:\n",
    "    section_name (str): The name of the section to evaluate.\n",
    "    section_text (str): The text of the section to evaluate.\n",
    "    goals_text (str, optional): The text of the report's goals to evaluate alignment. Default is None.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that instructs the model to evaluate the section based on clarity, alignment, and completeness.\n",
    "    2. If goals_text is provided, includes it in the prompt for evaluating alignment.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the generated scores and explanations.\n",
    "    6. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated scores and explanations for clarity, alignment, and completeness, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Evaluate the section '{section_name}' of a consulting report using the following 3 criteria:\\n\"\n",
    "        \"1. Clarity (Is the writing clear, well-structured, and understandable?)\\n\"\n",
    "        \"2. Alignment (Does it align with the report’s goals?)\\n\"\n",
    "        \"3. Completeness (Does it cover the necessary topics?)\\n\\n\"\n",
    "    )\n",
    "\n",
    "    if goals_text:\n",
    "        prompt += f\"Report Goals:\\n{goals_text}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Section:\\n{section_text}\\n\\n\"\n",
    "    prompt += (\n",
    "        \"Provide a score out of 10 for each criterion with a one-line explanation per score. Format:\\n\"\n",
    "        \"Clarity: [score]/10 – [reason]\\n\"\n",
    "        \"Alignment: [score]/10 – [reason]\\n\"\n",
    "        \"Completeness: [score]/10 – [reason]\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to score section: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_section_insights(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Summarize key insights from this section's reasoning steps into a short, client-friendly paragraph.\n",
    "    \"\"\"\n",
    "    steps = agent.history[-5:]  # Last 5 steps, or you could use all\n",
    "    thoughts = \"\\n\".join(\n",
    "        [\n",
    "            f\"Thought: {s['thought']}\\nAction: {s['action']}\\nObservation: {s['observation']}\"\n",
    "            for s in steps\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are summarizing an internal AI review of the consulting report section '{agent.section_name}'.\\n\"\n",
    "        \"Write a concise, client-facing summary of the most important observations based on the review.\\n\"\n",
    "        \"Focus on useful insights, potential risks, and gaps. Avoid tool names or internal system messages.\\n\\n\"\n",
    "        f\"Review Log:\\n{thoughts}\\n\\n\"\n",
    "        \"Summary:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        ).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to summarize section insights: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_issues(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Extracts the top 3 most important issues or gaps from the section summaries in the agent's memory.\n",
    "\n",
    "    Purpose:\n",
    "    This function reviews feedback across multiple sections of a report and identifies the top 3 most important issues or gaps that should be addressed. It prioritizes issues that impact clarity, alignment, or completeness across the report.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be reviewed.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the section summaries from the agent's memory.\n",
    "    2. Concatenates the section summaries into a single string.\n",
    "    3. Constructs a prompt instructing the model to identify the top 3 most important issues or gaps based on the section summaries.\n",
    "    4. Creates a list of messages with the constructed prompt.\n",
    "    5. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    6. If the API call is successful, returns the extracted top issues.\n",
    "    7. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted top 3 issues or gaps, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    summaries = agent.memory.get(\"section_notes\", {})\n",
    "    all_text = \"\\n\".join([f\"{sec}: {obs[0]}\" for sec, obs in summaries.items()])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an AI consultant reviewing feedback across multiple sections of a report.\\n\"\n",
    "        \"Based on the following section summaries, identify the top 3 most important issues or gaps that should be addressed.\\n\"\n",
    "        \"Be concise, specific, and prioritize issues that impact clarity, alignment, or completeness across the report.\\n\\n\"\n",
    "        f\"Section Summaries:\\n{all_text}\\n\\n\"\n",
    "        \"Top 3 Issues:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        ).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to extract top issues: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_score_block(score_text):\n",
    "    \"\"\"\n",
    "    Formats a block of score text by adding icons based on the score.\n",
    "\n",
    "    Purpose:\n",
    "    This function processes a block of text containing scores out of 10 and adds icons to each line based on the score. The icons indicate the quality of the score: green for high scores, yellow for medium scores, and red for low scores.\n",
    "\n",
    "    Parameters:\n",
    "    score_text (str): A block of text containing scores out of 10.\n",
    "\n",
    "    Workflow:\n",
    "    1. Splits the input score_text into individual lines.\n",
    "    2. For each line, searches for a score in the format \"X/10\".\n",
    "    3. If a score is found:\n",
    "       - Adds a green icon (🟢) for scores 8 and above.\n",
    "       - Adds a yellow icon (🟡) for scores between 6 and 7.\n",
    "       - Adds a red icon (🔴) for scores below 6.\n",
    "    4. If no score is found, the line remains unchanged.\n",
    "    5. Joins the processed lines back into a single block of text.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted block of text with icons added based on the scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_icons(line):\n",
    "        match = re.search(r\"(\\d+)/10\", line)\n",
    "        if not match:\n",
    "            return line\n",
    "        score = int(match.group(1))\n",
    "        if score >= 8:\n",
    "            icon = \"🟢\"\n",
    "        elif score >= 6:\n",
    "            icon = \"🟡\"\n",
    "        else:\n",
    "            icon = \"🔴\"\n",
    "        return f\"{icon} {line}\"\n",
    "\n",
    "    return \"\".join([add_icons(line) + \"  \\n\" for line in score_text.splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_level(agent, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Evaluates the confidence level of the AI consultant's reasoning steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function assesses the confidence level of the AI consultant's reasoning steps based on clarity, consistency, and support of the analysis. It uses the OpenAI API to rate the confidence level from 1 to 10.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the reasoning history to be evaluated.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Extracts the last 5 reasoning steps from the agent's history.\n",
    "    2. Constructs a prompt that includes the reasoning steps and asks for a confidence level rating from 1 to 10.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the confidence level rating.\n",
    "    6. If an exception occurs, returns a warning message.\n",
    "\n",
    "    Returns:\n",
    "    str: The confidence level rating from 1 to 10, or a warning message if the API call fails.\n",
    "    \"\"\"\n",
    "    reasoning_log = \"\\n\".join(\n",
    "        [\n",
    "            f\"{s['thought']} → {s['action']} → {s['observation']}\"\n",
    "            for s in agent.history[-5:]\n",
    "        ]\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Given the following reasoning steps from an AI consultant reviewing a report section, rate the confidence \"\n",
    "        \"level from 1 to 10 based on how clear, consistent, and well-supported the analysis appears. Just return a number (1–10).\\n\\n\"\n",
    "        f\"{reasoning_log}\\n\\nConfidence Level:\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        ).strip()\n",
    "    except Exception as e:\n",
    "        return \"⚠️\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fixes(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generates specific fixes or improvements for a consulting report section.\n",
    "\n",
    "    Purpose:\n",
    "    This function reviews the notes for a specific section of a consulting report and generates 2-3 specific fixes or improvements to make the section stronger.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be reviewed.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the notes for the current section from the agent's memory.\n",
    "    2. Constructs a prompt that includes the section notes and asks for 2-3 specific fixes or improvements.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the generated fixes or improvements.\n",
    "    6. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated fixes or improvements, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    notes = agent.memory[\"section_notes\"].get(agent.section_name, [\"\"])[0]\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are reviewing this consulting section:\\n\\n{notes}\\n\\n\"\n",
    "        \"List 2–3 specific fixes or improvements to make this section stronger.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(\n",
    "            messages, model=model, temperature=temperature\n",
    "        ).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate fixes: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_summary_support(summary_text, other_sections):\n",
    "    \"\"\"\n",
    "    Check if the summary reflects content from the rest of the report.\n",
    "    \"\"\"\n",
    "    combined = \"\\n\".join(other_sections.values())\n",
    "    prompt = (\n",
    "        \"Does the following summary accurately reflect the details and points made in the full report?\\n\\n\"\n",
    "        f\"Summary:\\n{summary_text}\\n\\n\"\n",
    "        f\"Report Body:\\n{combined}\\n\\n\"\n",
    "        \"Answer in 3–5 sentences highlighting any gaps, overstatements, or missing support.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_smart_goals(section_text):\n",
    "    \"\"\"\n",
    "    Evaluate whether the section's goals follow the SMART criteria.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Evaluate the following goals using the SMART criteria:\\n\"\n",
    "        \"Specific, Measurable, Achievable, Relevant, Time-bound.\\n\\n\"\n",
    "        f\"Goals:\\n{section_text}\\n\\n\"\n",
    "        \"Rate each of the SMART attributes and explain briefly.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recommendation_alignment(recommendation_text, goals_text):\n",
    "    \"\"\"\n",
    "    Check if recommendations align with the stated goals.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Here are the goals of the report:\\n{goals_text}\\n\\n\"\n",
    "        f\"And here are the key recommendations:\\n{recommendation_text}\\n\\n\"\n",
    "        \"Do the recommendations clearly align with the goals? Identify any mismatches or missing connections.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_sections(report_sections):\n",
    "    \"\"\"\n",
    "    Analyzes missing sections in an IT consulting report and evaluates their impact.\n",
    "\n",
    "    Purpose:\n",
    "    This function identifies any missing sections in an IT consulting report based on a predefined set of expected sections. It then evaluates how the absence of these sections might affect the report's effectiveness, highlighting any critical omissions.\n",
    "\n",
    "    Parameters:\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "\n",
    "    Workflow:\n",
    "    1. Identifies the missing sections by comparing the keys in `report_sections` with the expected sections in `canonical_section_map`.\n",
    "    2. If no sections are missing, returns a message indicating that no critical sections are missing.\n",
    "    3. If there are missing sections, constructs a prompt listing the missing sections and asking how their absence might affect the report's effectiveness.\n",
    "    4. Calls the OpenAI API with tracking using the `call_openai_with_tracking` function to get the evaluation.\n",
    "    5. Returns the evaluation from the OpenAI API.\n",
    "\n",
    "    Returns:\n",
    "    str: The evaluation of the impact of the missing sections on the report's effectiveness, or a message indicating that no critical sections are missing.\n",
    "    \"\"\"\n",
    "    missing = list(set(canonical_section_map.keys()) - set(report_sections.keys()))\n",
    "    if not missing:\n",
    "        return \"✅ No critical sections appear to be missing from the report.\"\n",
    "\n",
    "    missing_str = \"\\n\".join(f\"- {s}\" for s in sorted(missing))\n",
    "\n",
    "    prompt = (\n",
    "        \"You're reviewing an IT consulting report.\\n\"\n",
    "        \"The following expected sections are missing:\\n\"\n",
    "        f\"{missing_str}\\n\\n\"\n",
    "        \"Based on standard consulting practices, how might these missing sections affect the report's effectiveness?\\n\"\n",
    "        \"Highlight if any of these are critical omissions, and explain why.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_report_review(agent, report_sections, max_steps=5):\n",
    "    \"\"\"\n",
    "    Conducts a full review of an IT consulting report using the ReAct framework.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through all sections of an IT consulting report, performing a reasoning and action loop for each section using the ReAct framework. It then performs post-processing steps to highlight missing sections, generate a final summary, and extract the top issues.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    max_steps (int): The maximum number of steps to run the loop for each section. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through each section in the report_sections dictionary.\n",
    "    2. For each section:\n",
    "       - Sets the agent's section_name and section_text attributes.\n",
    "       - Calls the run_react_loop_check_withTool function to perform the reasoning and action loop.\n",
    "    3. After processing all sections, performs post-processing steps:\n",
    "       - Calls highlight_missing_sections to identify any missing sections in the report.\n",
    "       - Calls generate_final_summary to generate a final summary of the report.\n",
    "       - Calls extract_top_issues to identify the top issues or gaps in the report.\n",
    "    4. Stores the results of the post-processing steps in the agent's memory.\n",
    "\n",
    "    Returns:\n",
    "    ReActConsultantAgent: The agent instance with updated memory containing the results of the full report review.\n",
    "    \"\"\"\n",
    "    # Loop through all sections\n",
    "    for section_name, section_text in report_sections.items():\n",
    "        agent.section_name = section_name\n",
    "        agent.section_text = section_text\n",
    "        run_react_loop_check_withTool(agent, max_steps=max_steps)\n",
    "\n",
    "    # Post-processing steps\n",
    "    agent.memory[\"highlight_missing\"] = highlight_missing_sections(report_sections)\n",
    "    agent.memory[\"missing_analysis\"] = analyze_missing_sections(report_sections)\n",
    "    agent.memory[\"final_summary\"] = generate_final_summary(agent)\n",
    "    agent.memory[\"top_issues\"] = extract_top_issues(agent)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to assess readability of a section with textstat scores\n",
    "# Flesch Reading Ease: higher score indicates easier readability\n",
    "# Reading Level Estimate: grade level of text\n",
    "# Difficult Words Count: number of difficult words in the text\n",
    "def check_readability(section_text):\n",
    "    score = textstat.flesch_reading_ease(section_text)\n",
    "    level = textstat.text_standard(section_text)\n",
    "    difficult = textstat.difficult_words(section_text)\n",
    "\n",
    "    summary = (\n",
    "        f\"📖 **Flesch Reading Ease**: {score:.1f} (higher = easier)\\n\"\n",
    "        f\"🏫 **Reading Level Estimate**: {level}\\n\"\n",
    "        f\"🧠 **Difficult Words Count**: {difficult}\"\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_react_step(agent, thought, action, step_num=0):\n",
    "    \"\"\"\n",
    "    Simulates a single ReAct step using a given thought and action.\n",
    "    Delegates action execution to dispatch_tool_action().\n",
    "\n",
    "    Returns:\n",
    "    - observation: result of executing the tool\n",
    "    \"\"\"\n",
    "    # Log tool usage\n",
    "    if hasattr(agent, \"tool_usage\"):\n",
    "        agent.tool_usage[action] = agent.tool_usage.get(action, 0) + 1\n",
    "\n",
    "    # Execute the action using shared dispatch function\n",
    "    observation = dispatch_tool_action(agent, action)\n",
    "\n",
    "    # Store step in history\n",
    "    agent.history.append(\n",
    "        {\"thought\": thought, \"action\": action, \"observation\": observation}\n",
    "    )\n",
    "\n",
    "    # Track tool usage in memory\n",
    "    agent.memory[\"tool_history\"].append((step_num, action, agent.section_name))\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tool_action(action_string, text=\"Default text\", name=\"Test Section\"):\n",
    "    \"\"\"\n",
    "    Tests a specific tool action by simulating a single ReAct step.\n",
    "\n",
    "    Purpose:\n",
    "    This function creates an instance of the ReActConsultantAgent with a given section name and text, then simulates a single ReAct step using the specified action string. It prints the action being tested and the resulting observation.\n",
    "\n",
    "    Parameters:\n",
    "    action_string (str): The action to be tested.\n",
    "    text (str): The text of the section to be used for testing. Default is \"Default text\".\n",
    "    name (str): The name of the section to be used for testing. Default is \"Test Section\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a ReActConsultantAgent instance with the given section name and text.\n",
    "    2. Calls the run_single_react_step function with the agent, a test thought, and the specified action string.\n",
    "    3. Prints the action being tested and the resulting observation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    agent = ReActConsultantAgent(section_name=name, section_text=text)\n",
    "    obs = run_single_react_step(agent, \"Test Thought\", action_string)\n",
    "    print(\"🔍 Tool:\", action_string)\n",
    "    print(\"🧠 Observation:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tone_textblob(section_text):\n",
    "    \"\"\"\n",
    "    Analyzes the tone and clarity of a given text section using TextBlob.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the TextBlob library to analyze the sentiment of a given text section. It determines the tone (positive, neutral, or negative) based on the polarity score and the clarity (objective or subjective) based on the subjectivity score.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to be analyzed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Creates a TextBlob object from the section text.\n",
    "    2. Extracts the polarity and subjectivity scores from the TextBlob object.\n",
    "    3. Determines the tone based on the polarity score:\n",
    "       - If polarity > 0.2, the tone is positive.\n",
    "       - If polarity < -0.2, the tone is negative.\n",
    "       - Otherwise, the tone is neutral.\n",
    "    4. Determines the clarity based on the subjectivity score:\n",
    "       - If subjectivity < 0.4, the clarity is objective.\n",
    "       - Otherwise, the clarity is subjective.\n",
    "    5. Returns a formatted string with the tone and clarity information.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string indicating the tone and clarity of the text, including the polarity and subjectivity scores.\n",
    "    \"\"\"\n",
    "    blob = TextBlob(section_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    tone = \"neutral\"\n",
    "    if polarity > 0.2:\n",
    "        tone = \"positive\"\n",
    "    elif polarity < -0.2:\n",
    "        tone = \"negative\"\n",
    "\n",
    "    clarity = \"objective\" if subjectivity < 0.4 else \"subjective\"\n",
    "\n",
    "    return (\n",
    "        f\"💬 **Tone**: {tone} (polarity: {polarity:.2f})\\n\"\n",
    "        f\"🧠 **Clarity**: {clarity} (subjectivity: {subjectivity:.2f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(section_text):\n",
    "    \"\"\"\n",
    "    Extracts named entities from a given text section using a preloaded NLP model.\n",
    "\n",
    "    Purpose:\n",
    "    This function analyzes a text section to identify named entities (e.g., persons, organizations, dates) and categorizes them by their entity type. It provides a summary of the detected entities, grouped by their labels.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to be analyzed for named entities.\n",
    "\n",
    "    Workflow:\n",
    "    1. Processes the input text using the preloaded NLP model (`nlp`).\n",
    "    2. Checks if any named entities are detected in the text.\n",
    "       - If no entities are found, returns a message indicating no named entities were detected.\n",
    "    3. Iterates through the detected entities and groups them by their labels (e.g., PERSON, ORG, DATE).\n",
    "    4. Removes duplicate entities within each label and limits the output to the first 5 unique entities per label.\n",
    "    5. Constructs a formatted summary of the detected entities, grouped by their labels.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string summarizing the detected named entities, grouped by their labels. If no entities are found, returns a message indicating this.\n",
    "    \"\"\"\n",
    "    doc = nlp(section_text)\n",
    "    if not doc.ents:\n",
    "        return \"No named entities found.\"\n",
    "\n",
    "    entity_summary = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        entity_summary.setdefault(label, []).append(ent.text)\n",
    "\n",
    "    result = \"🧾 **Named Entities Detected:**\\n\"\n",
    "    for label, values in entity_summary.items():\n",
    "        unique_vals = list(set(values))\n",
    "        result += f\"- **{label}**: {', '.join(unique_vals[:5])}\\n\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_math_question(expression):\n",
    "    \"\"\"\n",
    "    Analyzes and evaluates a mathematical expression using the LLMMathChain.\n",
    "\n",
    "    Purpose:\n",
    "    This function takes a mathematical expression as input, evaluates it using the LLMMathChain,\n",
    "    and returns the result. It handles any exceptions that may occur during the evaluation process.\n",
    "\n",
    "    Parameters:\n",
    "    expression (str): The mathematical expression to be evaluated.\n",
    "\n",
    "    Workflow:\n",
    "    1. Receives a mathematical expression as input.\n",
    "    2. Calls the `run` method of the `llm_math` object to evaluate the expression.\n",
    "    3. If the evaluation is successful, returns the result.\n",
    "    4. If an exception occurs during the evaluation, catches the exception and returns an error message.\n",
    "\n",
    "    Returns:\n",
    "    str: The result of the evaluated mathematical expression, or an error message if the evaluation fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = llm_math.invoke(expression)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Math error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query):\n",
    "    \"\"\"\n",
    "    Searches academic papers on arXiv for technical or scientific topics.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the `arxiv_tool` to search for academic papers on arXiv based on a given query.\n",
    "    It stores the search results in the agent's memory for later use and returns the results.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query to find relevant academic papers on arXiv.\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the `run` method of the `arxiv_tool` with the provided query to perform the search.\n",
    "    2. If the search is successful:\n",
    "       - Stores the search results in the agent's memory under the \"citations\" key, categorized by the section name.\n",
    "       - Returns the search results.\n",
    "    3. If an exception occurs during the search, catches the exception and returns an error message.\n",
    "\n",
    "    Returns:\n",
    "    str: The search results from arXiv if successful, or an error message if the search fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = arxiv_tool.run(query)\n",
    "\n",
    "        # Store citation for later use\n",
    "        agent.memory.setdefault(\"citations\", {}).setdefault(\n",
    "            agent.section_name, []\n",
    "        ).append({\"source\": \"arxiv\", \"query\": query, \"result\": results})\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Arxiv search failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(query):\n",
    "    \"\"\"\n",
    "    Searches Wikipedia for a given query and returns a summary.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the `wikipedia` tool to search for a given query on Wikipedia and retrieve a summary of the relevant information. It handles exceptions to ensure graceful failure in case of errors.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query to find relevant information on Wikipedia.\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the `wikipedia.run` method with the provided query to perform the search.\n",
    "    2. If the search is successful, retrieves and returns the summary as a string.\n",
    "    3. If an exception occurs during the search, catches the exception and returns an error message.\n",
    "\n",
    "    Returns:\n",
    "    str: The summary of the search result from Wikipedia if successful, or an error message if the search fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = wikipedia.run(query)  # already returns a string summary\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Wikipedia error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_serpapi(query):\n",
    "    \"\"\"\n",
    "    Searches the web for relevant information using SerpAPI.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the SerpAPI to perform a web search based on a given query. It retrieves the top result, extracts relevant metadata (title, URL, snippet), and stores the result in the agent's memory for future reference.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query to find relevant information.\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the `run` method of the `serpapi` object to perform the search.\n",
    "    2. Retrieves the list of organic results from the search response.\n",
    "    3. If results are found:\n",
    "       - Extracts the title, URL, and snippet of the top result.\n",
    "       - Stores the metadata in the agent's memory under the \"citations\" key, categorized by the section name.\n",
    "       - Returns a formatted string containing the title, URL, and snippet of the top result.\n",
    "    4. If no results are found, returns a message indicating no web results were found.\n",
    "    5. If an exception occurs during the search, catches the exception and returns an error message.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string containing the title, URL, and snippet of the top result, or a message indicating no results were found or an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = serp_tool.run(query)  # already returns a string summary\n",
    "        parsed_results = serp_tool.results.get(\n",
    "            \"organic_results\", []\n",
    "        )  # safely get result list\n",
    "\n",
    "        if parsed_results:\n",
    "            top_result = parsed_results[0]  # Grab best result\n",
    "            title = top_result.get(\"title\", \"Unknown title\")\n",
    "            url = top_result.get(\"link\", \"No link found\")\n",
    "            snippet = top_result.get(\"snippet\", \"No snippet provided\")\n",
    "\n",
    "            # Store citation with metadata\n",
    "            agent.memory.setdefault(\"citations\", {}).setdefault(\n",
    "                agent.section_name, []\n",
    "            ).append(\n",
    "                {\n",
    "                    \"source\": \"serpapi\",\n",
    "                    \"query\": query,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"snippet\": snippet,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return f\"📄 {title}\\n🔗 {url}\\n📌 {snippet}\"\n",
    "        else:\n",
    "            return \"⚠️ No web results found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ SerpAPI error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_search_arxiv(section_text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Determines whether academic research or scientific citations could enhance the credibility of a report section.\n",
    "\n",
    "    Purpose:\n",
    "    This function evaluates a given section of an IT consulting report to decide if referencing academic research or scientific citations would improve its credibility. It uses an AI model to analyze the section and provide a decision along with a reason.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to be evaluated.\n",
    "    model (str): The name of the AI model to use for evaluation. Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt instructing the AI model to analyze the section and determine if academic research would enhance its credibility.\n",
    "    2. Sends the prompt to the AI model using the `call_openai_with_tracking` function.\n",
    "    3. Extracts the decision (YES or NO) and the reason from the AI model's response using regular expressions.\n",
    "    4. If the decision or reason cannot be extracted, defaults to \"NO\" with a generic reason.\n",
    "    5. Returns the decision as a boolean and the reason as a string.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - bool: `True` if academic research is recommended, `False` otherwise.\n",
    "        - str: The reason for the decision.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You're reviewing a section of an IT consulting report.\\n\"\n",
    "        \"Determine whether academic research or scientific citations could enhance the credibility of this section.\\n\\n\"\n",
    "        f\"Section:\\n{section_text}\\n\\n\"\n",
    "        \"Respond only with YES or NO and one short reason. Format:\\n\"\n",
    "        \"Decision: YES or NO\\nReason: ...\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = call_openai_with_tracking(messages, model=model)\n",
    "\n",
    "    decision_match = re.search(r\"Decision:\\s*(YES|NO)\", response, re.IGNORECASE)\n",
    "    reason_match = re.search(r\"Reason:\\s*(.+)\", response, re.IGNORECASE)\n",
    "\n",
    "    decision = decision_match.group(1).strip().upper() if decision_match else \"NO\"\n",
    "    reason = reason_match.group(1).strip() if reason_match else \"No reason given.\"\n",
    "\n",
    "    return decision == \"YES\", reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_citations_block(agent):\n",
    "    \"\"\"\n",
    "    Formats the citations and external references stored in the agent's memory into a markdown block.\n",
    "\n",
    "    Purpose:\n",
    "    This function retrieves the citations and external references from the agent's memory and formats them into a markdown string. Each section's citations are grouped and displayed with the query and result for easy readability.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data with citations.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a list with a header for the citations section.\n",
    "    2. Iterates through the citations stored in the agent's memory under the \"citations\" key.\n",
    "    3. For each section, adds a subsection header with the section name.\n",
    "    4. For each citation entry in the section:\n",
    "       - Adds the query used to retrieve the citation.\n",
    "       - Adds the result of the citation query.\n",
    "    5. Joins the list of lines into a single markdown-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    str: A markdown-formatted string containing the citations and external references grouped by section.\n",
    "    \"\"\"\n",
    "    lines = [\"## 📚 Citations & External References\\n\"]\n",
    "    for section, entries in agent.memory.get(\"citations\", {}).items():\n",
    "        lines.append(f\"### 🔹 {section}\")\n",
    "        for entry in entries:\n",
    "            if entry[\"source\"] == \"arxiv\":\n",
    "                lines.append(f\"- 🔍 *{entry['query']}* (from arXiv)\\n{entry['result']}\")\n",
    "            elif entry[\"source\"] == \"serpapi\":\n",
    "                lines.append(\n",
    "                    f\"- 🔍 *{entry['query']}* (from web)\\n  📄 {entry['title']}\\n  🔗 {entry['url']}\\n  📌 {entry['snippet']}\"\n",
    "                )\n",
    "            else:\n",
    "                lines.append(f\"- ❓ Unknown citation source: {entry}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_cite(statement, model=\"gpt-3.5-turbo\", temperature=0.4):\n",
    "    \"\"\"\n",
    "    Uses LLM to evaluate whether a statement or claim would benefit from a citation.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: (decision: YES/NO, reason: explanation string)\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You're reviewing a statement from an IT strategy report.\\n\"\n",
    "        \"Should this statement be supported by an external source or citation?\\n\\n\"\n",
    "        f'Statement:\\n\"{statement}\"\\n\\n'\n",
    "        \"Respond in this format:\\n\"\n",
    "        \"Decision: YES or NO\\n\"\n",
    "        \"Reason: [short explanation]\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "\n",
    "    decision_match = re.search(r\"Decision:\\s*(YES|NO)\", response, re.IGNORECASE)\n",
    "    reason_match = re.search(r\"Reason:\\s*(.+)\", response, re.IGNORECASE)\n",
    "\n",
    "    decision = decision_match.group(1).strip().upper() if decision_match else \"NO\"\n",
    "    reason = (\n",
    "        reason_match.group(1).strip() if reason_match else \"No explanation provided.\"\n",
    "    )\n",
    "\n",
    "    return decision == \"YES\", reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_fill_gaps_with_research(section_text, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Expands vague or underdeveloped parts of the section using reasoning + research context.\n",
    "    \"\"\"\n",
    "    # 1. Ask the LLM what is vague and how to improve it\n",
    "    guidance_prompt = (\n",
    "        \"You are reviewing a consulting report section that may be vague or incomplete.\\n\"\n",
    "        \"Suggest what areas could be expanded and what additional research might help.\\n\\n\"\n",
    "        f\"Section:\\n{section_text}\\n\\n\"\n",
    "        \"Respond in this format:\\n\"\n",
    "        \"Gap: [what's missing or vague]\\n\"\n",
    "        \"Suggested Search Query: [what we could search to improve it]\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        guidance = call_openai_with_tracking(\n",
    "            [{\"role\": \"user\", \"content\": guidance_prompt}],\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # 2. Extract suggested search\n",
    "        match = re.search(r\"Suggested Search Query:\\s*(.+)\", guidance)\n",
    "        query = (\n",
    "            match.group(1).strip() if match else \"best practices for IT modernization\"\n",
    "        )\n",
    "\n",
    "        # 3. Run web or arXiv search\n",
    "        research = search_arxiv(query)\n",
    "\n",
    "        # 4. Ask the LLM to rewrite the section using research\n",
    "        improve_prompt = (\n",
    "            f\"Use this research to improve the following section:\\n\\n\"\n",
    "            f\"Section:\\n{section_text}\\n\\n\"\n",
    "            f\"Research:\\n{research}\\n\\n\"\n",
    "            \"Provide the revised section with clearer, more complete content.\"\n",
    "        )\n",
    "\n",
    "        improved = call_openai_with_tracking(\n",
    "            [{\"role\": \"user\", \"content\": improve_prompt}],\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return improved.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Tool execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_section_with_research(section_text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Enhances a section of text by identifying sentences that require citations and improving them with research.\n",
    "\n",
    "    Purpose:\n",
    "    This function processes a given section of text, identifies sentences that would benefit from citations,\n",
    "    and enhances them by filling gaps with research. It appends footnotes to the improved sentences and\n",
    "    generates a log of changes and footnotes for reference.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to be enhanced.\n",
    "    model (str): The name of the AI model to use for evaluating citation needs and generating improvements.\n",
    "                 Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Splits the input text into individual sentences using `sent_tokenize`.\n",
    "    2. Iterates through each sentence:\n",
    "       - Calls `should_cite` to determine if the sentence requires a citation.\n",
    "       - If a citation is needed:\n",
    "         a. Calls `auto_fill_gaps_with_research` to generate an improved version of the sentence.\n",
    "         b. Appends the improved sentence with a footnote reference to the enhanced text.\n",
    "         c. Logs the original sentence, improved sentence, reason for citation, and footnote ID.\n",
    "       - If no citation is needed, appends the original sentence to the enhanced text.\n",
    "    3. Combines the enhanced sentences into a single improved text.\n",
    "    4. Returns the improved text, a log of changes, and a list of footnotes.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - str: The improved text with enhanced sentences and footnote references.\n",
    "        - list: A log of changes, where each entry contains:\n",
    "            - original (str): The original sentence.\n",
    "            - improved (str): The improved sentence.\n",
    "            - reason (str): The reason for requiring a citation.\n",
    "            - footnote (int): The footnote ID.\n",
    "        - list: A list of footnotes, where each entry contains:\n",
    "            - footnote_id (int): The ID of the footnote.\n",
    "            - original (str): The original sentence.\n",
    "            - improved (str): The improved sentence.\n",
    "            - reason (str): The reason for requiring a citation.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(section_text)\n",
    "    enhanced_sentences = []\n",
    "    log = []\n",
    "    footnotes = []\n",
    "\n",
    "    footnote_id = 1\n",
    "\n",
    "    for sentence in sentences:\n",
    "        needs_cite, reason = should_cite(sentence, model=model)\n",
    "\n",
    "        if needs_cite:\n",
    "            improved = auto_fill_gaps_with_research(sentence)\n",
    "            tagged = f\"{improved.strip()}[^${footnote_id}]\"\n",
    "            enhanced_sentences.append(tagged)\n",
    "            footnotes.append((footnote_id, sentence, improved.strip(), reason))\n",
    "            log.append(\n",
    "                {\n",
    "                    \"original\": sentence,\n",
    "                    \"improved\": improved,\n",
    "                    \"reason\": reason,\n",
    "                    \"footnote\": footnote_id,\n",
    "                }\n",
    "            )\n",
    "            footnote_id += 1\n",
    "        else:\n",
    "            enhanced_sentences.append(sentence)\n",
    "\n",
    "    improved_text = \" \".join(enhanced_sentences)\n",
    "    return improved_text, log, footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_coherent(enhanced_sentences, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Improves the coherence and flow of a rewritten section of a report.\n",
    "\n",
    "    Purpose:\n",
    "    This function takes a list of enhanced sentences and ensures that the resulting text flows well,\n",
    "    removes redundant ideas, and improves transitions between sentences. It uses an AI model to\n",
    "    refine the text while preserving all meaningful content.\n",
    "\n",
    "    Parameters:\n",
    "    enhanced_sentences (list): A list of sentences that have been enhanced or rewritten.\n",
    "    model (str): The name of the AI model to use for improving the text. Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Joins the list of enhanced sentences into a single block of text.\n",
    "    2. Constructs a prompt instructing the AI model to improve the coherence, remove redundancy,\n",
    "       and enhance transitions in the text.\n",
    "    3. Sends the prompt to the AI model using the `call_openai_with_tracking` function.\n",
    "    4. Receives the improved version of the text from the AI model.\n",
    "    5. Returns the improved text as a string.\n",
    "\n",
    "    Returns:\n",
    "    str: The improved version of the text with better coherence, transitions, and flow.\n",
    "    \"\"\"\n",
    "    joined = \" \".join(enhanced_sentences)\n",
    "    prompt = (\n",
    "        \"You are a consultant improving a rewritten section of a report. \"\n",
    "        \"Ensure the following text flows well, removes redundant ideas, and improves transitions. \"\n",
    "        \"Preserve all meaningful content.\\n\\n\"\n",
    "        f\"Text:\\n{joined}\\n\\n\"\n",
    "        \"Return the improved version:\"\n",
    "    )\n",
    "    return call_openai_with_tracking(\n",
    "        [{\"role\": \"user\", \"content\": prompt}], model=model\n",
    "    ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_upgraded_sections(agent):\n",
    "    \"\"\"\n",
    "    Formats the upgraded sections of a report with research enhancements into a markdown block.\n",
    "\n",
    "    Purpose:\n",
    "    This function retrieves the upgraded sections stored in the agent's memory, formats them into a markdown string,\n",
    "    and includes footnotes for any research-based improvements. It provides a clear and structured summary of\n",
    "    the enhancements made to each section.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data\n",
    "                                  with upgraded sections and their corresponding footnotes.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a list with a header for the section improvements.\n",
    "    2. Retrieves the upgraded sections from the agent's memory under the \"section_upgrades\" key.\n",
    "    3. If no upgrades are found, appends a message indicating no upgrades were made and returns the formatted string.\n",
    "    4. Iterates through each upgraded section:\n",
    "       - Adds the section name as a subheader.\n",
    "       - Appends the improved section text with footnotes.\n",
    "       - If footnotes are available, appends a list of footnotes with the original text, improved text, and reason for the improvement.\n",
    "    5. Joins the list of lines into a single markdown-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    str: A markdown-formatted string containing the upgraded sections with research enhancements and footnotes.\n",
    "    \"\"\"\n",
    "    lines = [\"## ✨ Section Improvements with Research\\n\"]\n",
    "\n",
    "    upgrades = agent.memory.get(\"section_upgrades\", {})\n",
    "    if not upgrades:\n",
    "        lines.append(\"_No upgrades were made._\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    for section, data in upgrades.items():\n",
    "        lines.append(f\"\\n### 🔹 {section}\")\n",
    "        lines.append(\"**Improved Section (with footnotes):**\\n\")\n",
    "        lines.append(data[\"improved\"].strip())\n",
    "\n",
    "        if \"footnotes\" in data and data[\"footnotes\"]:\n",
    "            lines.append(\"\\n**Footnotes:**\\n\")\n",
    "            for idx, original, improved, reason in data[\"footnotes\"]:\n",
    "                lines.append(f\"[^{idx}]: Originally: *{original.strip()}*\")\n",
    "                lines.append(f\"      → *{improved.strip()}*\")\n",
    "                lines.append(f\"      📚 Reason: {reason}\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Load Data** <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control whether to execute this cell\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Sample IT consulting report (replace with real one later)\n",
    "    # You can later replace this with a real one or load from a file.\n",
    "    # We can add file upload later (open(\"report.txt\").read())\n",
    "    sample_report = \"\"\"\n",
    "    Client: HealthConnect Systems\n",
    "    Industry: Healthcare\n",
    "    Project: IT Modernization Assessment\n",
    "\n",
    "    Summary:\n",
    "    HealthConnect currently operates on-premise infrastructure for its core clinical systems. While some departments use SaaS tools, there is no centralized cloud strategy. Security policies are documented but not consistently followed. There is no formal data governance framework. Leadership has expressed interest in migrating systems to the cloud.\n",
    "\n",
    "    Goals & Objectives:\n",
    "    1. Improve scalability and flexibility.\n",
    "    2. Enhance data security and privacy.\n",
    "    3. Streamline operations and reduce costs.\n",
    "    4. Enable remote access for healthcare providers.\n",
    "    5. Ensure compliance with industry regulations.\n",
    "\n",
    "    Key Recommendations:\n",
    "    1. Conduct a cloud readiness assessment.\n",
    "    2. Begin phased migration of CRM and EHR systems.\n",
    "    3. Establish a data governance committee.\n",
    "    4. Update security protocols to align with NIST standards.\n",
    "\n",
    "    Timeline: \n",
    "    Estimated at 6–12 months for full migration planning and execution.\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 4: Pre-process Data** <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control whether to execute the cell\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Run it on the sample report\n",
    "    report_sections = split_report_into_sections(sample_report)\n",
    "\n",
    "    # Display for verification\n",
    "    for section, content in report_sections.items():\n",
    "        print(f\"📌 {section}:\\n{content}\\n{'-'*60}\")\n",
    "else:\n",
    "    print(\"⚠️ Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 5: Model - Basic: Single-Hop Reasoning + Static Action Loop** <a id=\"5\"></a>\n",
    "\n",
    "1. Iterate through sections of report\n",
    "2. Send each section to ChatGPT for feedback\n",
    "3. Summarize feedback \n",
    "\n",
    "## **5.1 Initialize Agent** <a id=\"5.1\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control execution\n",
    "execute_cell = False  # Reversed the boolean to enable execution\n",
    "\n",
    "if execute_cell:\n",
    "    # Initialize the reasoning agent\n",
    "    agent = ITReportReviewer(report_sections)\n",
    "else:\n",
    "    print(\"⚠️ Execution skipped. Set execute_cell to True to run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Run Agent** <a id=\"5.2\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Define the order in which we want to review sections\n",
    "    sections_to_review = list(report_sections.keys())\n",
    "\n",
    "    # Loop through each section and generate a review\n",
    "    for section in sections_to_review:\n",
    "        agent.review_section(section)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Call the summarize_full_review function to get the final summary\n",
    "    final_summary = summarize_full_review(agent)\n",
    "    print(\"📋 Final Summary of the Report Review:\\n\")\n",
    "    print(final_summary)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 6: Model - ReAct** <a id=\"6\"></a>\n",
    "\n",
    "1. **Think** about each section (with ChatGPT)\n",
    "2. Decide on and take an **action**\n",
    "3. **Observe** the results and loop back to step 1 with new reasoning\n",
    "\n",
    "## **6.1 Simple Agent - Predefined Actions** <a id=\"6.1\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Choose a section to run ReAct on\n",
    "    section_name = \"Summary\"\n",
    "    section_text = report_sections.get(section_name, \"\")\n",
    "\n",
    "    # Initialize the ReAct agent\n",
    "    react_agent = ReActConsultantAgent(section_name, section_text)\n",
    "\n",
    "    # Run the ReAct loop\n",
    "    react_review_history = run_react_loop_static(react_agent)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 Agent 2 - Custom & Pre-Built Tools** <a id=\"6.2\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ReAct Loop with Tool Usage Tracking - one section at a time\n",
    "\n",
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Choose a section to run ReAct on\n",
    "    section_name = \"Key Recommendations\"\n",
    "    section_text = report_sections.get(section_name, \"\")\n",
    "\n",
    "    # Initialize the ReAct agent\n",
    "    react_agent = ReActConsultantAgent(section_name, section_text)\n",
    "\n",
    "    # Run the ReAct loop\n",
    "    react_review_history = run_react_loop_check_withTool(react_agent)\n",
    "\n",
    "    # Print tool usage summary\n",
    "    print_tool_usage(react_agent)\n",
    "    plot_tool_usage(react_agent.tool_usage)\n",
    "\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ReAct on the full report of all sections\n",
    "\n",
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    react_agent = ReActConsultantAgent(section_name=\"Full Report\", section_text=\"\")\n",
    "    react_agent = run_full_report_review(react_agent, report_sections)\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n🧾 Final Summary:\\n\")\n",
    "    print(react_agent.memory[\"final_summary\"])\n",
    "\n",
    "    # Export\n",
    "    await export_report_to_markdown_and_pdf(react_agent)\n",
    "    print(\"✅ Report exported successfully.\")\n",
    "\n",
    "    # Tool usage summary\n",
    "    print_tool_usage(react_agent)\n",
    "    plot_tool_usage(react_agent.tool_usage)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Unit test tools\n",
    "    test_tool_action(\n",
    "        'should_cite[\"We recommend to implement Salesforce CRM for better customer management.\"]',\n",
    "        name=\"Test Section\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ReAct on the full report of all sections\n",
    "# Load report from file (txt, md)\n",
    "\n",
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Load report for review from file\n",
    "    filepath = \"../data/reports/sample_report.txt\"\n",
    "    raw_text = load_report_text_from_file(filepath)\n",
    "    report_sections = split_report_into_sections(raw_text)\n",
    "\n",
    "    # Initialize and run full report review\n",
    "    react_agent = ReActConsultantAgent(section_name=\"Full Report\", section_text=\"\")\n",
    "    react_agent = run_full_report_review(react_agent, report_sections)\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n🧾 Final Summary:\\n\")\n",
    "    print(react_agent.memory[\"final_summary\"])\n",
    "\n",
    "    # Export\n",
    "    await export_report_to_markdown_and_pdf(react_agent)\n",
    "    print(\"✅ Report exported successfully.\")\n",
    "\n",
    "    # Tool usage summary\n",
    "    print_tool_usage(react_agent)\n",
    "    plot_tool_usage(react_agent.tool_usage)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    import inspect\n",
    "\n",
    "    # Get all functions defined in the current global scope\n",
    "    # Filter out functions that are not defined in this notebook\n",
    "    functions = [\n",
    "        name\n",
    "        for name, obj in globals().items()\n",
    "        if inspect.isfunction(obj) and obj.__module__ == \"__main__\"\n",
    "    ]\n",
    "\n",
    "    # Print the list of functions\n",
    "    print(\"List of functions created in this notebook:\")\n",
    "    for func in functions:\n",
    "        print(f\"- {func}\")\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
