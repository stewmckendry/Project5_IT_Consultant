{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: Setup and Imports** <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already installed (uncomment below in Jupyter)\n",
    "# !pip install openai python-dotenv\n",
    "# %pip install duckduckgo-search\n",
    "\n",
    "import openai # for GPT-3\n",
    "import os # for environment variables\n",
    "from dotenv import load_dotenv # for loading environment variables\n",
    "from difflib import SequenceMatcher # for comparing strings\n",
    "import re # regular expressions\n",
    "from duckduckgo_search import DDGS # for searching DuckDuckGo (web information retrieval)\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib.patches as mpatches # for patches\n",
    "from markdown2 import markdown # for converting markdown to HTML\n",
    "import asyncio # for async functions\n",
    "from playwright.async_api import async_playwright # for scraping web pages\n",
    "import webbrowser # for opening web pages\n",
    "import difflib # for comparing strings\n",
    "import textstat # for text statistics\n",
    "from langchain_community.tools import WikipediaQueryRun # for querying Wikipedia\n",
    "from langchain_community.utilities import WikipediaAPIWrapper # for querying Wikipedia\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "from textblob import TextBlob # for sentiment analysis\n",
    "from langchain_community.utilities import SerpAPIWrapper # for querying SERP API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check to confirm it's loaded (optional)\n",
    "print(\"✅ API key loaded:\", openai.api_key is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\' entity_type: related_questions.\\', \\'One of the frameworks recommended by the Task Group to help health care organizations manage cybersecurity and bolster their security posture is ...\\', \\'The NIST CSF can help healthcare organizations reduce cyber risk, cut costs, and potentially reduce cyber insurance premiums. Even though the ...\\', \\'Revised draft publication aims to help organizations comply with HIPAA Security Rule.\\', \\'The NIST Cybersecurity Framework consists of five core functions that guide cybersecurity risk management. These functions—Identify, Protect, ...\\', \"The NIST CSF 2.0 introduces a nuanced approach to managing cybersecurity risks in healthcare, aligning closely with the sector\\'s need for ...\", \\'For example, one of the most popular health cybersecurity framework called NIST. NIST ensures security using its core elements, implementation ...\\', \\'The NIST Privacy Framework is designed to bridge the gap between information security and individual privacy.\\', \\'The NIST Cybersecurity Framework Helps Healthcare Organizations Establish and Mature Cybersecurity Programs\\', \\'NIST 800-53 establishes a risk management framework for federal information systems and provides guidance for implementing security and privacy controls.\\', \\'For example, by implementing continuous attack surface monitoring, healthcare organizations can address any potential risks or breaches ...\\']'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# Check to confirm it's loaded (optional)\n",
    "print(\"✅ API key loaded:\", serpapi_key is not None)\n",
    "\n",
    "serp_tool = SerpAPIWrapper(serpapi_api_key=serpapi_key)\n",
    "\n",
    "serp_tool.run(\"examples of NIST framework in healthcare\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Functions** <a id=\"2\"></a>\n",
    "## **2.1 OpenAI Functions** <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call OpenAI's ChatCompletion API with structured messages\n",
    "def call_openai(messages, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with structured messages.\n",
    "\n",
    "    Parameters:\n",
    "    messages (list): A list of message dictionaries, where each dictionary contains 'role' and 'content' keys.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function takes the input parameters and calls the OpenAI ChatCompletion API.\n",
    "    2. The API returns a response containing multiple choices.\n",
    "    3. The function extracts the content of the first choice from the response.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the first choice from the API response.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to construct system + user messages for the reasoning agent\n",
    "def build_review_prompt(report_text, history=[]):\n",
    "    \"\"\"\n",
    "    Constructs system and user messages for the reasoning agent to review a consulting report.\n",
    "\n",
    "    Parameters:\n",
    "    report_text (str): The text of the consulting report to be reviewed.\n",
    "    history (list): A list of previous messages (optional). Default is an empty list.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function creates a system message that sets the context for the reasoning agent.\n",
    "    2. It then creates a user message containing the consulting report text.\n",
    "    3. The function combines the system message, the history of previous messages, and the user message into a single list.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of message dictionaries, including the system message, any historical messages, and the user message.\n",
    "    \"\"\"\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an experienced IT strategy consultant. \"\n",
    "            \"You are reviewing a consulting report for completeness, clarity, risks, and alignment with best practices. \"\n",
    "            \"Think step-by-step and identify gaps, ask clarifying questions, or suggest improvements. \"\n",
    "            \"Your goal is to provide helpful, critical feedback using your expert knowledge.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the consulting report to review:\\n\\n{report_text}\"\n",
    "    }\n",
    "\n",
    "    return [system_msg] + history + [user_msg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track total tokens used and estimated cost\n",
    "total_tokens_used = 0\n",
    "estimated_cost_usd = 0.0\n",
    "\n",
    "# Cost per 1K tokens for GPT-3.5-turbo (adjust if using GPT-4)\n",
    "COST_PER_1K_TOKENS = 0.0015\n",
    "\n",
    "def call_openai_with_tracking(messages, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with structured messages and tracks token usage and estimated cost.\n",
    "\n",
    "    Parameters:\n",
    "    messages (list): A list of message dictionaries, where each dictionary contains 'role' and 'content' keys.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    max_tokens (int): The maximum number of tokens to generate in the completion. Default is 500.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function takes the input parameters and calls the OpenAI ChatCompletion API.\n",
    "    2. The API returns a response containing multiple choices and token usage information.\n",
    "    3. The function extracts the content of the first choice from the response.\n",
    "    4. It updates the total tokens used and the estimated cost in USD.\n",
    "    5. It logs the prompt tokens, completion tokens, total tokens used so far, and the estimated cost.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the first choice from the API response.\n",
    "    \"\"\"\n",
    "    global total_tokens_used, estimated_cost_usd\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    usage = response['usage']\n",
    "    prompt_tokens = usage.get('prompt_tokens', 0)\n",
    "    completion_tokens = usage.get('completion_tokens', 0)\n",
    "    total = usage.get('total_tokens', prompt_tokens + completion_tokens)\n",
    "\n",
    "    # Update tracking\n",
    "    total_tokens_used += total\n",
    "    estimated_cost_usd += (total / 1000) * COST_PER_1K_TOKENS\n",
    "\n",
    "    # Logging\n",
    "    print(f\"🔢 Prompt: {prompt_tokens} tokens | Completion: {completion_tokens} tokens | Total so far: {total_tokens_used} tokens\")\n",
    "    print(f\"💰 Estimated cost so far: ${estimated_cost_usd:.4f} USD\")\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Data Preprocessing Functions** <a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the report into sections\n",
    "def split_report_into_sections(report_text):\n",
    "    \"\"\"\n",
    "    Splits a consulting report into sections based on headers.\n",
    "\n",
    "    Parameters:\n",
    "    report_text (str): The text of the consulting report to be split into sections.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function initializes an empty dictionary to store sections and sets the current section to \"Header\".\n",
    "    2. It splits the report text into lines and iterates through each line.\n",
    "    3. For each line:\n",
    "       - If the line is blank, it skips to the next line.\n",
    "       - If the line ends with a colon and contains 4 or fewer words, it is considered a section header.\n",
    "         The current section is updated to this header (without the colon), and a new entry is created in the dictionary.\n",
    "       - Otherwise, the line is added to the current section's content.\n",
    "    4. After processing all lines, the function joins the content of each section into a single string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    lines = report_text.strip().split(\"\\n\")\n",
    "    current_section = \"Header\"\n",
    "    sections[current_section] = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip blank lines\n",
    "        elif line.endswith(\":\") and len(line.split()) <= 4: # Check if it's a section header.  If yes, create a new section with the header as the key.\n",
    "            current_section = line.replace(\":\", \"\").strip()\n",
    "            sections[current_section] = []\n",
    "        else:\n",
    "            sections[current_section].append(line) # Add the line (contents below header) to the current section\n",
    "\n",
    "    # Join section contents with newlines\n",
    "    for key in sections:\n",
    "        sections[key] = \"\\n\".join(sections[key])\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Static Reasoning Agent Functions** <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITReportReviewer:\n",
    "    \"\"\"\n",
    "    A class to review sections of an IT consulting report using OpenAI's ChatCompletion API.\n",
    "\n",
    "    Process:\n",
    "    1. Initializes the reviewer with the report sections\n",
    "    2. Lets it review one section at a time using real reasoning\n",
    "    3. Stores and prints each review with feedback\n",
    "\n",
    "    Attributes:\n",
    "    sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    review_history (list): A list to store the history of reviews for each section.\n",
    "\n",
    "    Methods:\n",
    "    review_section(section_name):\n",
    "        Reviews a specific section of the report using OpenAI's ChatCompletion API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, report_sections, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "        \"\"\"\n",
    "        Initializes the ITReportReviewer with the given report sections, model, and temperature.\n",
    "\n",
    "        Parameters:\n",
    "        report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "        model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "        temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "        \"\"\"\n",
    "        self.sections = report_sections\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.review_history = []\n",
    "\n",
    "    def review_section(self, section_name):\n",
    "        \"\"\"\n",
    "        Reviews a specific section of the report using OpenAI's ChatCompletion API.\n",
    "\n",
    "        Parameters:\n",
    "        section_name (str): The name of the section to review.\n",
    "\n",
    "        Workflow:\n",
    "        1. Retrieves the text of the specified section from the sections dictionary.\n",
    "        2. If the section is empty or missing, prints a warning message.\n",
    "        3. Builds a prompt for reviewing the section using the build_review_prompt function.\n",
    "        4. Calls the OpenAI ChatCompletion API with tracking using the call_openai_with_tracking function.\n",
    "        5. Saves the review in the review_history attribute.\n",
    "        6. Prints the review for the specified section.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        section_text = self.sections.get(section_name, \"\")\n",
    "        if not section_text:\n",
    "            print(f\"⚠️ Section '{section_name}' is empty or missing.\")\n",
    "            return\n",
    "\n",
    "        # Build prompt for reviewing the section\n",
    "        messages = build_review_prompt(\n",
    "            report_text=f\"Section: {section_name}\\n\\n{section_text}\",\n",
    "            history=[]\n",
    "        )\n",
    "\n",
    "        # Get AI-generated reasoning using tracked OpenAI call\n",
    "        review = call_openai_with_tracking(messages, model=self.model, temperature=self.temperature)\n",
    "\n",
    "        # Save the review\n",
    "        self.review_history.append({\n",
    "            \"section\": section_name,\n",
    "            \"review\": review\n",
    "        })\n",
    "\n",
    "        print(f\"\\n✅ Review for section '{section_name}':\\n{review}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_full_review(agent):\n",
    "    # Combine all reviews into a single prompt\n",
    "    combined_review_text = \"\"\n",
    "    for step in agent.review_history:\n",
    "        combined_review_text += f\"Section: {step['section']}\\nFeedback: {step['review']}\\n\\n\"\n",
    "\n",
    "    # Build new prompt asking for a final report assessment\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert IT strategy consultant reviewing an internal report assessment. \"\n",
    "                \"Summarize the overall quality of the report based on the following section reviews. \"\n",
    "                \"Highlight gaps, strengths, and suggest next steps to improve the report.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": combined_review_text\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    summary = call_openai_with_tracking(messages)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 ReAct Reasoning Agent Functions** <a id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActConsultantAgent:\n",
    "    \"\"\"\n",
    "    A class to review sections of an IT consulting report using the ReAct (Reason + Act) framework with OpenAI's ChatCompletion API.\n",
    "\n",
    "    Process:\n",
    "    1. Initializes the agent with the section name and text.\n",
    "    2. Builds a prompt for the ReAct framework.\n",
    "    3. Tracks the history of thoughts, actions, and observations.\n",
    "\n",
    "    Attributes:\n",
    "    section_name (str): The name of the section to review.\n",
    "    section_text (str): The text of the section to review.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "    history (list): A list to store the history of thoughts, actions, and observations.\n",
    "\n",
    "    Methods:\n",
    "    build_react_prompt():\n",
    "        Builds a prompt for the ReAct framework based on the section text and history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, section_name, section_text, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "        \"\"\"\n",
    "        Initializes the ReActConsultantAgent with the given section name, section text, model, and temperature.\n",
    "\n",
    "        Parameters:\n",
    "        section_name (str): The name of the section to review.\n",
    "        section_text (str): The text of the section to review.\n",
    "        model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "        temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "        \"\"\"\n",
    "        self.section_name = section_name\n",
    "        self.section_text = section_text\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.history = []\n",
    "        self.tool_usage = {}  # {action_name: count}\n",
    "        self.memory = {\n",
    "            \"section_notes\": {},     # {section_name: [insight1, insight2, ...]}\n",
    "            \"cross_section_flags\": [],  # [(sectionA, sectionB, observation)]\n",
    "            \"tool_history\": []       # [(step_number, action, section)]\n",
    "        }\n",
    "\n",
    "    def build_react_prompt(self):\n",
    "        \"\"\"\n",
    "        Builds a prompt for the ReAct framework based on the section text and history.\n",
    "\n",
    "        Workflow:\n",
    "        1. Constructs a base prompt with the section name and text.\n",
    "        2. Iterates through the history of thoughts, actions, and observations, appending them to the base prompt.\n",
    "        3. Adds a final line asking for the next thought and action.\n",
    "\n",
    "        Returns:\n",
    "        list: A list containing a single dictionary with the role 'user' and the constructed prompt as content.\n",
    "        \"\"\"\n",
    "        base_prompt = (\n",
    "            f\"You are an expert IT strategy consultant reviewing a report section titled '{self.section_name}'.\\n\"\n",
    "            \"You are using ReAct (Reason + Act) to think through the review.\\n\\n\"\n",
    "            \"Format each response like this:\\n\"\n",
    "            \"Thought: <your reasoning>\\n\"\n",
    "            \"Action: <one of: ask_question, flag_risk, recommend_fix, summarize>\\n\\n\"\n",
    "            f\"Here is the section content:\\n{self.section_text}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        for step in self.history:\n",
    "            base_prompt += f\"Thought: {step['thought']}\\n\"\n",
    "            base_prompt += f\"Action: {step['action']}\\n\"\n",
    "            base_prompt += f\"Observation: {step['observation']}\\n\\n\"\n",
    "\n",
    "        base_prompt += \"What is your next Thought and Action?\"\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": base_prompt}]\n",
    "\n",
    "    def build_react_prompt_withTools(self):\n",
    "            \"\"\"\n",
    "            Builds a prompt for the ReAct framework based on the section text and history.\n",
    "\n",
    "            Workflow:\n",
    "            1. Constructs a base prompt with the section name and text.\n",
    "            2. Iterates through the history of thoughts, actions, and observations, appending them to the base prompt.\n",
    "            3. Adds a final line asking for the next thought and action.\n",
    "            For the check_guideline action, the prompt includes a placeholder for the topic.\n",
    "            LLM infers topic from the section_text\n",
    "\n",
    "            Returns:\n",
    "            list: A list containing a single dictionary with the role 'user' and the constructed prompt as content.\n",
    "            \"\"\"\n",
    "            tool_hint_text, tools_to_focus = build_tool_hints(self)\n",
    "\n",
    "            base_prompt = (\n",
    "                f\"You are an expert IT strategy consultant reviewing a report section titled '{self.section_name}'.\\n\"\n",
    "                \"You are using ReAct (Reason + Act) to think through the review.\\n\\n\"\n",
    "                \"Format each response like this:\\n\"\n",
    "                \"Thought: <your reasoning>\\n\"\n",
    "                \"Action: <choose ONE tool from the prioritized list below, unless you strongly believe another tool is better>\\n\\n\"\n",
    "                f\"Prioritized tools for this section:\\n{tool_hint_text}\\n\\n\"\n",
    "                \"Available tools and their descriptions:\\n\"\n",
    "            )\n",
    "\n",
    "            base_prompt += format_tool_catalog_for_prompt(tool_catalog)\n",
    "            base_prompt += f\"Here is the section content:\\n{self.section_text}\\n\\n\"\n",
    "            \n",
    "            for step in self.history:\n",
    "                base_prompt += f\"Thought: {step['thought']}\\n\"\n",
    "                base_prompt += f\"Action: {step['action']}\\n\"\n",
    "                base_prompt += f\"Observation: {step['observation']}\\n\\n\"\n",
    "\n",
    "            base_prompt += \"What is your next Thought and Action?\"\n",
    "\n",
    "            return [{\"role\": \"user\", \"content\": base_prompt}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_loop_static(agent, max_steps=5):\n",
    "    \"\"\"\n",
    "    Runs the ReAct (Reason + Act) loop for a specified number of steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through a reasoning and action loop using the ReAct framework to review a section of an IT consulting report. It generates thoughts, actions, and observations at each step, and stores the history of these steps.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    max_steps (int): The maximum number of steps to run the loop. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through the loop for a maximum of `max_steps` times.\n",
    "    2. In each iteration:\n",
    "    - Calls `agent.build_react_prompt()` to construct the prompt for the ReAct framework.\n",
    "    - Calls `call_openai_with_tracking()` to get the response from the OpenAI API.\n",
    "    - Parses the response to extract the thought and action.\n",
    "    - Generates an observation based on the action.\n",
    "    - Stores the thought, action, and observation in the agent's history.\n",
    "    - Prints the result of the current step.\n",
    "    - Breaks the loop if the action is \"summarize\".\n",
    "    3. Returns the full reasoning history.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, where each dictionary contains the thought, action, and observation for each step.\n",
    "    \"\"\"\n",
    "    for step_num in range(max_steps):\n",
    "        messages = agent.build_react_prompt()\n",
    "        response = call_openai_with_tracking(messages, model=agent.model, temperature=agent.temperature)\n",
    "\n",
    "        # Parse response\n",
    "        try:\n",
    "            lines = response.strip().split(\"\\n\")\n",
    "            thought = next(line.split(\":\", 1)[1].strip() for line in lines if line.lower().startswith(\"thought\"))\n",
    "            action = next(line.split(\":\", 1)[1].strip() for line in lines if line.lower().startswith(\"action\"))\n",
    "        except:\n",
    "            print(\"⚠️ Failed to parse model response.\")\n",
    "            break\n",
    "\n",
    "        # Generate observation based on action\n",
    "        if action == \"ask_question\":\n",
    "            observation = \"Good question to ask the client for clarification.\"\n",
    "        elif action == \"flag_risk\":\n",
    "            observation = \"This is a legitimate risk that should be addressed.\"\n",
    "        elif action == \"recommend_fix\":\n",
    "            observation = \"The recommendation improves the section's clarity and compliance.\"\n",
    "        elif action == \"summarize\":\n",
    "            observation = \"Review complete.\"\n",
    "        else:\n",
    "            observation = \"Unrecognized action.\"\n",
    "\n",
    "        # Store step\n",
    "        agent.history.append({\n",
    "            \"thought\": thought,\n",
    "            \"action\": action,\n",
    "            \"observation\": observation\n",
    "        })\n",
    "\n",
    "        # Print result of this step\n",
    "        print(f\"\\n🔁 Step {step_num + 1}\")\n",
    "        print(f\"🧠 Thought: {thought}\")\n",
    "        print(f\"⚙️ Action: {action}\")\n",
    "        print(f\"👀 Observation: {observation}\")\n",
    "\n",
    "        if action == \"summarize\":\n",
    "            break\n",
    "\n",
    "    # Return full reasoning history\n",
    "    return agent.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_loop_check_withTool(agent, max_steps=5):\n",
    "    \"\"\"\n",
    "    Runs the ReAct (Reason + Act) loop for a specified number of steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through a reasoning and action loop using the ReAct framework to review a section of an IT consulting report. It generates thoughts, actions, and observations at each step, and stores the history of these steps.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    max_steps (int): The maximum number of steps to run the loop. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through the loop for a maximum of `max_steps` times.\n",
    "    2. In each iteration:\n",
    "       - Calls `agent.build_react_prompt()` to construct the prompt for the ReAct framework.\n",
    "       - Calls `call_openai_with_tracking()` to get the response from the OpenAI API.\n",
    "       - Parses the response to extract the thought and action.\n",
    "       - Generates an observation based on the action.\n",
    "       - Stores the thought, action, and observation in the agent's history.\n",
    "       - Prints the result of the current step.\n",
    "       - Breaks the loop if the action is \"summarize\".\n",
    "    3. Returns the full reasoning history.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, where each dictionary contains the thought, action, and observation for each step.\n",
    "    \"\"\"\n",
    "    for step_num in range(max_steps):\n",
    "        messages = agent.build_react_prompt_withTools()\n",
    "        response = call_openai_with_tracking(messages, model=agent.model, temperature=agent.temperature)\n",
    "\n",
    "        # Parse response\n",
    "        try:\n",
    "            lines = response.strip().split(\"\\n\")\n",
    "            thought = next(line.split(\":\", 1)[1].strip() for line in lines if line.lower().startswith(\"thought\"))\n",
    "            action = next(line.split(\":\", 1)[1].strip() for line in lines if line.lower().startswith(\"action\"))\n",
    "            # Log tool usage\n",
    "            if hasattr(agent, \"tool_usage\"):\n",
    "                agent.tool_usage[action] = agent.tool_usage.get(action, 0) + 1\n",
    "        except:\n",
    "            print(\"⚠️ Failed to parse model response.\")\n",
    "            break\n",
    "\n",
    "        # Generate observation based on action\n",
    "        observation = dispatch_tool_action(agent, action)\n",
    "\n",
    "        # Track tool history\n",
    "        agent.memory[\"tool_history\"].append((step_num, action, agent.section_name))\n",
    "\n",
    "        # Special case for section comparisons\n",
    "        if action.startswith(\"compare_with_other_section\"):\n",
    "            match = re.match(r'compare_with_other_section\\[\"(.+?)\",\\s*\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                sectionA, sectionB = match.groups()\n",
    "                agent.memory[\"cross_section_flags\"].append((sectionA, sectionB, observation))\n",
    "\n",
    "        # Store step\n",
    "        agent.history.append({\n",
    "            \"thought\": thought,\n",
    "            \"action\": action,\n",
    "            \"observation\": observation\n",
    "        })\n",
    "        \n",
    "        # Print result of this step\n",
    "        print(f\"\\n🔁 Step {step_num + 1}\")\n",
    "        print(f\"🧠 Thought: {thought}\")\n",
    "        print(f\"⚙️ Action: {action}\")\n",
    "        print(f\"👀 Observation: {observation}\")\n",
    "\n",
    "        if action == \"summarize\":\n",
    "            break\n",
    "    \n",
    "    # Summarize and store section notes, scores, fixes, confidence level, raw ouputs\n",
    "    summarize_and_score_section(agent)\n",
    "\n",
    "    # Return full reasoning history\n",
    "    return agent.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_tool_action(agent, action):\n",
    "    print(f\"🛠️ Tool action: {action}\")\n",
    "    try:\n",
    "        if action.startswith(\"check_guideline\"):\n",
    "            match = re.match(r'check_guideline\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_guideline(match.group(1))\n",
    "        elif action.startswith(\"keyword_match_in_section\"):\n",
    "            match = re.match(r'keyword_match_in_section\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return keyword_match_in_section(match.group(1), agent.section_text)\n",
    "        elif action.startswith(\"check_timeline_feasibility\"):\n",
    "            match = re.match(r'check_timeline_feasibility\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_timeline_feasibility(match.group(1))\n",
    "        elif action.startswith(\"search_report\"):\n",
    "            match = re.match(r'search_report\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return search_report(match.group(1), report_sections)\n",
    "        elif action.startswith(\"search_web\"):\n",
    "            match = re.match(r'search_web\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return search_web(match.group(1))\n",
    "        elif action == \"check_for_jargon\":\n",
    "            return check_for_jargon(agent.section_text)\n",
    "        elif action == \"generate_client_questions\":\n",
    "            return generate_client_questions(agent.section_text)\n",
    "        elif action == \"highlight_missing_sections\":\n",
    "            return highlight_missing_sections(report_sections)\n",
    "        elif action.startswith(\"check_alignment_with_goals\"):\n",
    "            match = re.match(r'check_alignment_with_goals\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_alignment_with_goals(match.group(1), report_sections)\n",
    "        elif action.startswith(\"compare_with_other_section\"):\n",
    "            match = re.match(r'compare_with_other_section\\[\"(.+?)\",\\s*\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return compare_with_other_section(match.group(1), match.group(2), report_sections)\n",
    "        elif action.startswith(\"check_summary_support\"):\n",
    "            match = re.match(r'check_summary_support\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                return check_summary_support(match.group(1), report_sections)\n",
    "        elif action == \"evaluate_smart_goals\":\n",
    "            return evaluate_smart_goals(agent.section_text)\n",
    "        elif action.startswith(\"check_recommendation_alignment\"):\n",
    "            match = re.match(r'check_recommendation_alignment\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                goals = report_sections.get(\"Goals & Objectives\", \"\")\n",
    "                return check_recommendation_alignment(match.group(1), goals)\n",
    "        elif action == \"ask_question\":\n",
    "            return \"Good question to ask the client for clarification.\"\n",
    "        elif action == \"flag_risk\":\n",
    "            return \"This is a legitimate risk that should be addressed.\"\n",
    "        elif action == \"recommend_fix\":\n",
    "            return \"The recommendation improves the section's clarity and compliance.\"\n",
    "        elif action == \"summarize\":\n",
    "            return \"Review complete.\"\n",
    "        elif action == \"tool_help\":\n",
    "            return format_tool_catalog_for_prompt(tool_catalog)\n",
    "        elif action.startswith(\"suggest_tool_for\"):\n",
    "            match = re.match(r'suggest_tool_for\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                matches = pick_tool_by_intent_fuzzy(match.group(1), tool_catalog)\n",
    "                if matches:\n",
    "                    return \"Best match based on your goal:\\n\" + \"\\n\".join([f\"{tool} (match: {score})\" for tool, score in matches])\n",
    "                else:\n",
    "                    return \"⚠️ No matching tool found. Showing available tools:\\n\" + format_tool_catalog_for_prompt(tool_catalog)\n",
    "        elif action == \"final_summary\":\n",
    "            return generate_final_summary(agent)\n",
    "        elif action == \"check_readability\": \n",
    "            return check_readability(agent.section_text)\n",
    "        elif action.startswith(\"search_wikipedia\"):\n",
    "            match = re.match(r'search_wikipedia\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "                return wikipedia.run(query)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse search_wikipedia action.\"\n",
    "        elif action == \"analyze_tone_textblob\":\n",
    "            return analyze_tone_textblob(agent.section_text)\n",
    "        elif action.startswith(\"search_serpapi\"):\n",
    "            match = re.match(r'search_serpapi\\[\"(.+?)\"\\]', action)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "                return serp_tool.run(query)\n",
    "            else:\n",
    "                return \"⚠️ Could not parse search_serpapi action.\"\n",
    "        else:\n",
    "            return \"Unrecognized action.\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Tool execution error: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_score_section(agent):\n",
    "    section_name = agent.section_name\n",
    "    section_text = agent.section_text\n",
    "    goals_text = report_sections.get(\"Goals & Objectives\", None)\n",
    "\n",
    "    # Summarize\n",
    "    agent.memory[\"section_notes\"][section_name] = [summarize_section_insights(agent)]\n",
    "\n",
    "    # Score\n",
    "    agent.memory.setdefault(\"section_scores\", {})[section_name] = score_section(section_name, section_text, goals_text)\n",
    "\n",
    "    # Confidence\n",
    "    agent.memory.setdefault(\"confidence_levels\", {})[section_name] = get_confidence_level(agent)\n",
    "\n",
    "    # Fix suggestions\n",
    "    agent.memory.setdefault(\"section_fixes\", {})[section_name] = recommend_fixes(agent)\n",
    "\n",
    "    # Debug notes\n",
    "    agent.memory.setdefault(\"debug_notes\", {})[section_name] = agent.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dicionary of tools available for ReActConsultantAgent to call\n",
    "\n",
    "tool_catalog = {\n",
    "    \"check_guideline\": {\n",
    "        \"description\": \"Look up a best practice for a given topic\",\n",
    "        \"usage\": 'check_guideline[\"cloud security\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            \"check_guideline[\\\"data governance\\\"]\",\n",
    "            \"check_guideline[\\\"migration strategy\\\"]\"\n",
    "        ]\n",
    "    },\n",
    "    \"keyword_match_in_section\": {\n",
    "        \"description\": \"Check if a keyword appears in the current section\",\n",
    "        \"usage\": 'keyword_match_in_section[\"encryption\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            \"keyword_match_in_section[\\\"stakeholders\\\"]\"\n",
    "        ]\n",
    "    },\n",
    "    \"check_timeline_feasibility\": {\n",
    "        \"description\": \"Check if a project timeline is realistic for migration\",\n",
    "        \"usage\": 'check_timeline_feasibility[\"12 weeks\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            \"check_timeline_feasibility[\\\"3 months\\\"]\"\n",
    "        ]\n",
    "    },\n",
    "    \"search_report\": {\n",
    "        \"description\": \"Search the entire report for a concept or term\",\n",
    "        \"usage\": 'search_report[\"data governance\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            \"search_report[\\\"Zero Trust\\\"]\"\n",
    "        ]\n",
    "    },\n",
    "    \"ask_question\": {\n",
    "        \"description\": \"Ask a clarifying question about the report\",\n",
    "        \"usage\": \"ask_question\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"ask_question\"]\n",
    "    },\n",
    "    \"flag_risk\": {\n",
    "        \"description\": \"Flag a gap, issue, or concern in the section\",\n",
    "        \"usage\": \"flag_risk\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"flag_risk\"]\n",
    "    },\n",
    "    \"recommend_fix\": {\n",
    "        \"description\": \"Suggest a specific improvement\",\n",
    "        \"usage\": \"recommend_fix\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"recommend_fix\"]\n",
    "    },\n",
    "    \"summarize\": {\n",
    "        \"description\": \"Summarize your review and end the loop\",\n",
    "        \"usage\": \"summarize\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"summarize\"]\n",
    "    },\n",
    "    \"tool_help\": {\n",
    "        \"description\": \"View descriptions, usage, and examples for available tools\",\n",
    "        \"usage\": \"tool_help\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"tool_help\"]\n",
    "    },\n",
    "    \"suggest_tool_for\": {\n",
    "        \"description\": \"Ask which tool best supports a particular goal or intent\",\n",
    "        \"usage\": 'suggest_tool_for[\"goal or intent\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'suggest_tool_for[\"check if encryption is included\"]',\n",
    "            'suggest_tool_for[\"evaluate feasibility of 12-week timeline\"]'\n",
    "        ]\n",
    "    }, \n",
    "    \"search_web\": {\n",
    "        \"description\": \"Look up a concept, framework, or term on the web (DuckDuckGo)\",\n",
    "        \"usage\": 'search_web[\"Zero Trust model\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['search_web[\"data mesh\"]']\n",
    "    },\n",
    "    \"check_for_jargon\": {\n",
    "        \"description\": \"Identify jargon or overly technical terms that may need simplification\",\n",
    "        \"usage\": \"check_for_jargon\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"check_for_jargon\"]\n",
    "    },\n",
    "    \"generate_client_questions\": {\n",
    "        \"description\": \"Generate clarifying or skeptical questions a client might ask about the section\",\n",
    "        \"usage\": \"generate_client_questions\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"generate_client_questions\"]\n",
    "    },\n",
    "    \"highlight_missing_sections\": {\n",
    "        \"description\": \"Identify which expected sections are missing from the report\",\n",
    "        \"usage\": \"highlight_missing_sections\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"highlight_missing_sections\"]\n",
    "    },\n",
    "    \"check_alignment_with_goals\": {\n",
    "        \"description\": \"Evaluate how well a report section aligns with the stated goals\",\n",
    "        \"usage\": 'check_alignment_with_goals[\"section_name\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['check_alignment_with_goals[\"Key Recommendations\"]']\n",
    "    },\n",
    "    \"compare_with_other_section\": {\n",
    "        \"description\": \"Compare two sections to identify overlaps, contradictions, or gaps\",\n",
    "        \"usage\": 'compare_with_other_section[\"sectionA\", \"sectionB\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'compare_with_other_section[\"Key Recommendations\", \"Roadmap & Timeline\"]',\n",
    "            'compare_with_other_section[\"Future State Vision\", \"Technology Architecture\"]'\n",
    "        ]\n",
    "    },\n",
    "    \"final_summary\": {\n",
    "        \"description\": \"Generate a final client-facing summary based on insights gathered across all sections\",\n",
    "        \"usage\": \"final_summary\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"final_summary\"]\n",
    "    },\n",
    "    \"check_summary_support\": {\n",
    "        \"description\": \"Check if a summary is supported by details in the rest of the report\",\n",
    "        \"usage\": 'check_summary_support[\"summary text\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['check_summary_support[\"The report outlines benefits of cloud migration...\"]']\n",
    "    },\n",
    "    \"evaluate_smart_goals\": {\n",
    "        \"description\": \"Evaluate whether stated goals follow the SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound)\",\n",
    "        \"usage\": \"evaluate_smart_goals\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"evaluate_smart_goals\"]\n",
    "    },\n",
    "    \"check_recommendation_alignment\": {\n",
    "        \"description\": \"Check whether the recommendations align with the goals of the report\",\n",
    "        \"usage\": 'check_recommendation_alignment[\"recommendation text\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['check_recommendation_alignment[\"Modernize CRM and EHR with cloud-based tools\"]']\n",
    "    }, \n",
    "    \"check_readability\": {\n",
    "        \"description\": \"Evaluate the section’s readability and complexity using text statistics\",\n",
    "        \"usage\": \"check_readability\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"check_readability\"]\n",
    "    },\n",
    "    \"search_wikipedia\": {\n",
    "        \"description\": \"Look up a concept on Wikipedia to gain factual context or definition\",\n",
    "        \"usage\": 'search_wikipedia[\"cloud computing\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": ['search_wikipedia[\"Zero Trust\"]', 'search_wikipedia[\"data governance\"]']\n",
    "    },\n",
    "    \"analyze_tone_textblob\": {\n",
    "        \"description\": \"Analyze tone and subjectivity of the section using TextBlob\",\n",
    "        \"usage\": \"analyze_tone_textblob\",\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\"analyze_tone_textblob\"]\n",
    "    },\n",
    "    \"search_serpapi\": {\n",
    "        \"description\": \"Search the web in real time using Google (via SerpAPI)\",\n",
    "        \"usage\": 'search_serpapi[\"query here\"]',\n",
    "        \"version\": \"v1.0\",\n",
    "        \"examples\": [\n",
    "            'search_serpapi[\"cloud security trends 2024\"]',\n",
    "            'search_serpapi[\"real-world Zero Trust case studies\"]'\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global tools available for ReActConsultantAgent to call regardless of the section\n",
    "global_tools = [\n",
    "    \"keyword_match_in_section\",\n",
    "    \"ask_question\",\n",
    "    \"recommend_fix\",\n",
    "    \"flag_risk\",\n",
    "    \"tool_help\",\n",
    "    \"search_report\",\n",
    "    \"search_web\",\n",
    "    \"check_for_jargon\",\n",
    "    \"generate_client_questions\",\n",
    "    \"check_readability\",\n",
    "    \"search_wikipedia\",\n",
    "    \"analyze_tone_textblob\",\n",
    "    \"search_serpapi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tool priority map for each section\n",
    "tool_priority_map = {\n",
    "    \"Header\": {\n",
    "        \"primary\": [],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"Summary\": {\n",
    "        \"primary\": [\"check_summary_support\", \"check_readability\"],\n",
    "        \"optional\": [\"check_alignment_with_goals\", \"analyze_tone_textblob\"]\n",
    "    },\n",
    "    \"Goals & Objectives\": {\n",
    "        \"primary\": [\"evaluate_smart_goals\"],\n",
    "        \"optional\": [\"check_guideline\"]\n",
    "    },\n",
    "    \"Current State Assessment\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\", \"check_for_jargon\", \"flag_risk\", \"search_wikipedia\"],\n",
    "        \"optional\": [\"check_guideline\", \"search_serpapi\"]\n",
    "    },\n",
    "    \"Future State\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\", \"check_guideline\", \"flag_risk\", \"recommend_fix\", \"search_wikipedia\"],\n",
    "        \"optional\": [\"compare_with_other_section\", \"check_guideline\", \"search_serpapi\", \"analyze_tone_textblob\"]\n",
    "    },\n",
    "    \"Key Recommendations\": {\n",
    "        \"primary\": [\"check_recommendation_alignment\", \"flag_risk\", \"recommend_fix\", \"check_readability\"],\n",
    "        \"optional\": [\"check_guideline\", \"generate_client_questions\", \"search_serpapi\"]\n",
    "    },\n",
    "    \"Implementation Plan\": {\n",
    "        \"primary\": [\"check_timeline_feasibility\", \"compare_with_other_section\", \"flag_risk\", \"recommend_fix\", \"search_wikipedia\"],\n",
    "        \"optional\": [\"check_alignment_with_goals\", \"search_serpapi\"]\n",
    "    },\n",
    "    \"Timeline\": {  # Legacy alias\n",
    "        \"primary\": [\"check_timeline_feasibility\", \"flag_risk\"],\n",
    "        \"optional\": [\"search_serpapi\"]\n",
    "    },\n",
    "    \"Benefits\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\"],\n",
    "        \"optional\": [\"recommend_fix\", \"analyze_tone_textblob\",\"search_serpapi\"]\n",
    "    },\n",
    "    \"Costs\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\"],\n",
    "        \"optional\": [\"recommend_fix\", \"search_serpapi\"]\n",
    "    },\n",
    "    \"Resources\": {\n",
    "        \"primary\": [\"check_alignment_with_goals\"],\n",
    "        \"optional\": [\"compare_with_other_section\"]\n",
    "    },\n",
    "    \"Risks & Mitigations\": {\n",
    "        \"primary\": [\"flag_risk\"],\n",
    "        \"optional\": [\"check_guideline\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the canonical section map for mapping section names to canonical names (similar to the tool catalog)\n",
    "canonical_section_map = {\n",
    "    \"Header\": [\"header\", \"project context\", \"introduction\", \"overview\"],\n",
    "    \"Summary\": [\"summary\", \"executive summary\"],\n",
    "    \"Goals & Objectives\": [\"goals\", \"objectives\", \"strategic priorities\"],\n",
    "    \"Current State Assessment\": [\"current state\", \"as-is\", \"status quo\"],\n",
    "    \"Future State\": [\"future state\", \"to-be\", \"vision\", \"target state\"],\n",
    "    \"Key Recommendations\": [\"recommendations\", \"our recommendations\", \"next steps\"],\n",
    "    \"Implementation Plan\": [\"implementation plan\", \"roadmap\", \"deployment\", \"schedule\", \"timeline\", \"phasing\"],\n",
    "    \"Benefits\": [\"benefits\", \"value\", \"expected outcomes\"],\n",
    "    \"Costs\": [\"costs\", \"financials\", \"budget\", \"investment\"],\n",
    "    \"Resources\": [\"resources\", \"team structure\", \"staffing\", \"governance\"],\n",
    "    \"Risks & Mitigations\": [\"risks\", \"mitigations\", \"risk mitigation\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_section_to_canonical(label):\n",
    "    lower_label = label.lower()\n",
    "    for canonical, variants in canonical_section_map.items():\n",
    "        for variant in variants:\n",
    "            if variant in lower_label:\n",
    "                return canonical\n",
    "    # Fuzzy fallback\n",
    "    all_variants = {v: k for k_list in canonical_section_map.values() for v in k_list}\n",
    "    match = difflib.get_close_matches(lower_label, all_variants.keys(), n=1, cutoff=0.6)\n",
    "    return all_variants.get(match[0]) if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tool_hints(agent):\n",
    "    \"\"\"\n",
    "    Builds a hint message and a combined list of tools for the ReActConsultantAgent based on the section being reviewed.\n",
    "\n",
    "    Purpose:\n",
    "    This function generates a hint message prioritizing tools for the agent to use based on the section being reviewed. It combines primary, optional, and global tools into a sorted list.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the section name being reviewed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Maps the section name to its canonical form using the map_section_to_canonical function.\n",
    "    2. Retrieves the primary and optional tools for the canonical section from the tool_priority_map.\n",
    "    3. Combines the primary, optional, and global tools into a sorted list.\n",
    "    4. Constructs a hint message prioritizing the primary, optional, and global tools.\n",
    "    5. Returns the hint message and the combined list of tools.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the hint message (str) and the combined list of tools (list).\n",
    "    \"\"\"\n",
    "    canonical = map_section_to_canonical(agent.section_name)\n",
    "    priorities = tool_priority_map.get(canonical, {})\n",
    "\n",
    "    primary = priorities.get(\"primary\", [])\n",
    "    optional = priorities.get(\"optional\", [])\n",
    "\n",
    "    combined_tools = sorted(set(primary + optional + global_tools))\n",
    "\n",
    "    hint = \"You may use any tool, but prioritize:\\n\"\n",
    "    for tool in primary:\n",
    "        hint += f\"- {tool} ✅ (primary)\\n\"\n",
    "    for tool in optional:\n",
    "        hint += f\"- {tool} ◽️ (optional)\\n\"\n",
    "    for tool in global_tools:\n",
    "        if tool not in primary and tool not in optional:\n",
    "            hint += f\"- {tool} 🌐 (global)\\n\"\n",
    "\n",
    "    return hint, combined_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the tool catalog for display in the prompt for consumption by LLM\n",
    "\n",
    "def format_tool_catalog_for_prompt(tool_catalog):\n",
    "    \"\"\"\n",
    "    Formats the tool catalog for display in the prompt for consumption by LLM.\n",
    "\n",
    "    Purpose:\n",
    "    This function formats the tool catalog into a human-readable string that lists each tool along with its description and usage. This formatted string can be used in prompts for language models to understand the available tools.\n",
    "\n",
    "    Parameters:\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description' and 'usage'.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a list with the header \"Available tools:\".\n",
    "    2. Iterates through each tool in the tool_catalog dictionary.\n",
    "    3. For each tool, appends its name, description, and usage to the list.\n",
    "    4. Joins the list into a single string with newline characters.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string listing all tools with their descriptions and usage.\n",
    "    \"\"\"\n",
    "    lines = [\"\\n(You may use these tools if you believe they apply — but prioritize the tools listed above.)\\n\\n\"]\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        lines.append(f\"- {tool} (v{meta['version']}): {meta['description']}\")\n",
    "        lines.append(f\"  Usage: {meta['usage']}\")\n",
    "        if meta.get(\"examples\"):\n",
    "            for ex in meta[\"examples\"]:\n",
    "                lines.append(f\"  Example: {ex}\")\n",
    "        lines.append(\"\")  # spacing between tools\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_guideline, keyword_match_in_section, check_timeline_feasibility, search_report, ask_question, flag_risk, recommend_fix, summarize, tool_help, suggest_tool_for, search_web, check_for_jargon, generate_client_questions, highlight_missing_sections, check_alignment_with_goals, compare_with_other_section, final_summary, check_summary_support, evaluate_smart_goals, check_recommendation_alignment, check_readability, search_wikipedia, analyze_tone_textblob, search_serpapi\n"
     ]
    }
   ],
   "source": [
    "def format_tools_list(tool_catalog):\n",
    "    \"\"\"\n",
    "    Formats the list of tools from the tool_catalog dictionary as a comma-separated string.\n",
    "\n",
    "    Parameters:\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata.\n",
    "\n",
    "    Returns:\n",
    "    str: A comma-separated string of tool names.\n",
    "    \"\"\"\n",
    "    tools_list = \", \".join(tool_catalog.keys())\n",
    "    return tools_list\n",
    "\n",
    "# Example usage\n",
    "formatted_tools = format_tools_list(tool_catalog)\n",
    "print(formatted_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #1: Check for best practices in a given section\n",
    "\n",
    "# Simulated best practices reference data\n",
    "best_practices = {\n",
    "    \"cloud security\": \"Follow NIST Cybersecurity Framework. Include access control, encryption at rest/in-transit, and regular audits.\",\n",
    "    \"data governance\": \"Establish data stewards, quality standards, lifecycle rules, and metadata documentation.\",\n",
    "    \"migration\": \"Use phased migration, sandbox testing, rollback planning, and stakeholder communication.\"\n",
    "}\n",
    "\n",
    "def check_guideline(topic):\n",
    "    \"\"\"\n",
    "    Checks for best practices related to a given topic.\n",
    "\n",
    "    Purpose:\n",
    "    This function looks up best practices for a specified topic from a predefined dictionary of best practices.\n",
    "\n",
    "    Parameters:\n",
    "    topic (str): The topic for which best practices are to be checked.\n",
    "\n",
    "    Workflow:\n",
    "    1. The function converts the topic to lowercase to ensure case-insensitive matching.\n",
    "    2. It looks up the topic in the `best_practices` dictionary.\n",
    "    3. If a matching guideline is found, it returns the guideline.\n",
    "    4. If no matching guideline is found, it returns a message indicating that no matching guideline was found.\n",
    "\n",
    "    Returns:\n",
    "    str: The best practice guideline for the specified topic, or a message indicating no matching guideline was found.\n",
    "    \"\"\"\n",
    "    return best_practices.get(topic.lower(), \"No matching guideline found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #2: Keyword matching in a section\n",
    "# This tool helps the agent check if a keyword or concept is explicitly mentioned in the section.\n",
    "# This is great for validating whether the report includes key elements (e.g., \"encryption\", \"stakeholders\", \"Zero Trust\").  \n",
    "\n",
    "def keyword_match_in_section(term, section_text):\n",
    "    \"\"\"\n",
    "    Checks if a keyword or concept is explicitly mentioned in a section of the report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps validate whether the report includes key elements by checking if a specified keyword or concept is mentioned in the section text.\n",
    "\n",
    "    Parameters:\n",
    "    term (str): The keyword or concept to search for in the section.\n",
    "    section_text (str): The text of the section to search within.\n",
    "\n",
    "    Workflow:\n",
    "    1. Converts the keyword and section text to lowercase to ensure case-insensitive matching.\n",
    "    2. Checks if the keyword is present in the section text.\n",
    "    3. Returns a message indicating whether the keyword was found or not.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating whether the keyword was found in the section.\n",
    "    \"\"\"\n",
    "    term_lower = term.lower()\n",
    "    if term_lower in section_text.lower():\n",
    "        return f\"The keyword '{term}' was found in the section.\"\n",
    "    else:\n",
    "        return f\"The keyword '{term}' was NOT found in the section.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #3: Check feasibility of timeline for IT migration\n",
    "# This tool helps the agent assess the feasibility of a timeline for an IT migration project.\n",
    "# It checks if the timeline is too short, potentially feasible, or reasonable for a full migration.\n",
    "\n",
    "def check_timeline_feasibility(duration_str):\n",
    "    \"\"\"\n",
    "    Checks the feasibility of a timeline for an IT migration project.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps assess whether a given timeline for an IT migration project is too short, potentially feasible, or reasonable.\n",
    "\n",
    "    Parameters:\n",
    "    duration_str (str): The timeline duration as a string (e.g., \"6-12 months\", \"8 to 10 weeks\", \"a few months\").\n",
    "\n",
    "    Workflow:\n",
    "    1. Converts the duration string to lowercase and strips any leading/trailing whitespace.\n",
    "    2. Initializes a dictionary of fuzzy terms (e.g., \"a few\", \"several\") with their estimated numeric values.\n",
    "    3. Checks if the duration string contains any fuzzy terms and estimates the duration in months.\n",
    "    4. If no fuzzy terms are found, checks for ranges (e.g., \"6-12 months\") or single values (e.g., \"6 months\") and calculates the average duration in months.\n",
    "    5. If the duration cannot be parsed, returns a warning message.\n",
    "    6. Evaluates the feasibility of the timeline based on the calculated duration in months:\n",
    "       - If less than 3 months, returns that the timeline is likely too short.\n",
    "       - If between 3 and 12 months, returns that the timeline is potentially feasible.\n",
    "       - If more than 12 months, returns that the timeline seems reasonable.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the feasibility of the timeline.\n",
    "    \"\"\"\n",
    "    duration_str = duration_str.lower().strip()\n",
    "\n",
    "    fuzzy_terms = {\n",
    "        \"a few\": 3,\n",
    "        \"a couple\": 2,\n",
    "        \"several\": 6,\n",
    "        \"some\": 4,\n",
    "        \"many\": 9,\n",
    "        \"a handful\": 5\n",
    "    }\n",
    "\n",
    "    months = None\n",
    "\n",
    "    # Check for fuzzy terms like \"a few months\"\n",
    "    for fuzzy_word, estimated_num in fuzzy_terms.items():\n",
    "        if fuzzy_word in duration_str:\n",
    "            if \"week\" in duration_str:\n",
    "                months = estimated_num / 4\n",
    "            elif \"month\" in duration_str:\n",
    "                months = estimated_num\n",
    "            break\n",
    "\n",
    "    # Check for ranges like \"6-12 months\" or \"8 to 10 weeks\"\n",
    "    if months is None:\n",
    "        range_match = re.match(r'(\\d+)\\s*[-to]+\\s*(\\d+)\\s*(weeks|months)', duration_str)\n",
    "        single_match = re.match(r'(\\d+)\\s*(weeks|months)', duration_str)\n",
    "\n",
    "        try:\n",
    "            if range_match:\n",
    "                start = int(range_match.group(1))\n",
    "                end = int(range_match.group(2))\n",
    "                unit = range_match.group(3)\n",
    "                avg = (start + end) / 2\n",
    "                months = avg / 4 if \"week\" in unit else avg\n",
    "\n",
    "            elif single_match:\n",
    "                num = int(single_match.group(1))\n",
    "                unit = single_match.group(2)\n",
    "                months = num / 4 if \"week\" in unit else num\n",
    "        except:\n",
    "            return \"⚠️ Could not parse timeline value.\"\n",
    "\n",
    "    if months is None:\n",
    "        return \"⚠️ Could not understand the timeline. Use phrases like '6 months', '8-12 weeks', or 'a few months'.\"\n",
    "\n",
    "    # Evaluate the feasibility\n",
    "    if months < 3:\n",
    "        return f\"The timeline ({duration_str}) is likely too short for a full migration.\"\n",
    "    elif 3 <= months <= 12:\n",
    "        return f\"The timeline ({duration_str}) is potentially feasible depending on complexity.\"\n",
    "    else:\n",
    "        return f\"The timeline ({duration_str}) seems reasonable for a full IT migration.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool #4: Search for a term in the entire report\n",
    "# This tool helps the agent search for a specific term in the entire consulting report.\n",
    "# It returns the sections where the term was found, if any.\n",
    "\n",
    "def search_report(term, report_sections):\n",
    "    \"\"\"\n",
    "    Searches for a specific term in the entire consulting report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps the agent search for a specific term in the entire consulting report and returns the sections where the term was found, if any.\n",
    "\n",
    "    Parameters:\n",
    "    term (str): The term to search for in the report.\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `found_in` to store the sections where the term is found.\n",
    "    2. Iterates through each section in the `report_sections` dictionary.\n",
    "    3. For each section, checks if the term (case-insensitive) is present in the section content.\n",
    "    4. If the term is found, appends the section header to the `found_in` list.\n",
    "    5. After checking all sections, returns a message indicating the sections where the term was found or a message indicating that the term was not found.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the sections where the term was found or a message indicating that the term was not found.\n",
    "    \"\"\"\n",
    "    found_in = []\n",
    "    for section, content in report_sections.items():\n",
    "        if term.lower() in content.lower():\n",
    "            found_in.append(section)\n",
    "    if found_in:\n",
    "        return f\"The term '{term}' was found in: {', '.join(found_in)}.\"\n",
    "    else:\n",
    "        return f\"The term '{term}' was NOT found anywhere in the report.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool picker: Pick tools based on intent description\n",
    "# This function picks tools from the tool catalog based on the intent description.\n",
    "# It searches through the tool catalog and returns a list of tools whose descriptions match the given intent description.\n",
    "\n",
    "def pick_tool_by_intent(intent_description, tool_catalog):\n",
    "    \"\"\"\n",
    "    Picks tools from the tool catalog based on the intent description.\n",
    "\n",
    "    Purpose:\n",
    "    This function searches through the tool catalog and returns a list of tools whose descriptions match the given intent description.\n",
    "\n",
    "    Parameters:\n",
    "    intent_description (str): A description of the intent to match against tool descriptions.\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description'.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `matches` to store the names of matching tools.\n",
    "    2. Iterates through each tool in the `tool_catalog` dictionary.\n",
    "    3. For each tool, checks if the `intent_description` is present in the tool's description (case-insensitive).\n",
    "    4. If a match is found, appends the tool name to the `matches` list.\n",
    "    5. Returns the list of matching tools.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tool names whose descriptions match the given intent description.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        if intent_description.lower() in meta[\"description\"].lower():\n",
    "            matches.append(tool)\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool picker: Pick tools based on fuzzy intent description matching\n",
    "# This function picks tools from the tool catalog based on a fuzzy intent description matching.\n",
    "# It uses fuzzy string matching to find tools whose descriptions closely match the given intent description.\n",
    "\n",
    "def pick_tool_by_intent_fuzzy(intent_description, tool_catalog, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Picks tools from the tool catalog based on a fuzzy intent description matching.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses fuzzy string matching to find tools whose descriptions closely match the given intent description.\n",
    "\n",
    "    Parameters:\n",
    "    intent_description (str): A description of the intent to match against tool descriptions.\n",
    "    tool_catalog (dict): A dictionary where keys are tool names and values are dictionaries containing tool metadata, including 'description'.\n",
    "    threshold (float): The minimum similarity ratio (between 0 and 1) to consider a match. Default is 0.3.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `matches` to store the names and similarity ratios of matching tools.\n",
    "    2. Iterates through each tool in the `tool_catalog` dictionary.\n",
    "    3. For each tool, calculates the similarity ratio between the `intent_description` and the tool's description using `SequenceMatcher`.\n",
    "    4. If the similarity ratio exceeds the `threshold`, appends the tool name and ratio to the `matches` list.\n",
    "    5. Sorts the `matches` list by similarity ratio in descending order.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples, where each tuple contains a tool name and its similarity ratio, sorted by best match.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for tool, meta in tool_catalog.items():\n",
    "        ratio = SequenceMatcher(None, intent_description.lower(), meta[\"description\"].lower()).ratio()\n",
    "        if ratio > threshold:\n",
    "            matches.append((tool, round(ratio, 2)))\n",
    "    return sorted(matches, key=lambda x: -x[1])  # sort by best match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tools_by_priority(tool_usage, section_tool_map, global_tools):\n",
    "    \"\"\"\n",
    "    Categorizes each tool usage as Global, Primary, or Optional.\n",
    "\n",
    "    Purpose:\n",
    "    This function categorizes the usage of tools into four categories: Global, Primary, Optional, and Uncategorized. It helps in understanding the distribution of tool usage based on their priority and scope.\n",
    "\n",
    "    Parameters:\n",
    "    tool_usage (dict): A dictionary where keys are tool names and values are their respective usage counts.\n",
    "    section_tool_map (dict): A dictionary where keys are section names and values are dictionaries containing 'primary' and 'optional' tools for each section.\n",
    "    global_tools (list): A list of tools that are considered global and can be used across any section.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a dictionary `categorized` with keys 'Global', 'Primary', 'Optional', and 'Uncategorized', each containing an empty dictionary.\n",
    "    2. Iterates through each tool and its count in the `tool_usage` dictionary.\n",
    "    3. Checks if the tool is in the `global_tools` list. If yes, adds it to the 'Global' category and marks it as matched.\n",
    "    4. Iterates through the `section_tool_map` to check if the tool is listed as 'primary' or 'optional' for any section. If found, adds it to the respective category and marks it as matched.\n",
    "    5. If the tool is not matched in any category, adds it to the 'Uncategorized' category.\n",
    "    6. Returns the `categorized` dictionary containing the categorized tool usage.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are categories ('Global', 'Primary', 'Optional', 'Uncategorized') and values are dictionaries of tools and their usage counts.\n",
    "    \"\"\"\n",
    "    categorized = {\"Global\": {}, \"Primary\": {}, \"Optional\": {}, \"Uncategorized\": {}}\n",
    "\n",
    "    for tool, count in tool_usage.items():\n",
    "        matched = False\n",
    "        # Check if tool is in global\n",
    "        if tool in global_tools:\n",
    "            categorized[\"Global\"][tool] = count\n",
    "            matched = True\n",
    "        # Check if tool is in primary/optional across any section\n",
    "        for priorities in section_tool_map.values():\n",
    "            if tool in priorities.get(\"primary\", []):\n",
    "                categorized[\"Primary\"][tool] = count\n",
    "                matched = True\n",
    "            elif tool in priorities.get(\"optional\", []):\n",
    "                categorized[\"Optional\"][tool] = count\n",
    "                matched = True\n",
    "        if not matched:\n",
    "            categorized[\"Uncategorized\"][tool] = count\n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tool_usage(agent):\n",
    "    \"\"\"\n",
    "    Prints the usage of tools categorized by their priority.\n",
    "\n",
    "    Purpose:\n",
    "    This function categorizes the usage of tools into four categories: Primary, Optional, Global, and Uncategorized. It then prints the usage count of each tool within these categories.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the tool usage data to be categorized and printed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the categorize_tools_by_priority function to categorize the tools based on their priority.\n",
    "    2. Iterates through each category (Primary, Optional, Global, Uncategorized).\n",
    "    3. For each category, retrieves the tools and their usage counts.\n",
    "    4. Prints the category name and the usage count of each tool within the category, sorted by usage count in descending order.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Tool Usage by Category:\")\n",
    "\n",
    "    categorized = categorize_tools_by_priority(\n",
    "        agent.tool_usage, \n",
    "        tool_priority_map, \n",
    "        global_tools\n",
    "    )\n",
    "\n",
    "    for category in [\"Primary\", \"Optional\", \"Global\", \"Uncategorized\"]:\n",
    "        tools = categorized[category]\n",
    "        if tools:\n",
    "            print(f\"\\n🔹 {category} Tools:\")\n",
    "            for tool, count in sorted(tools.items(), key=lambda x: -x[1]):\n",
    "                print(f\"  - {tool}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tool_usage(tool_usage_dict, title=\"Tool Usage Summary\"):\n",
    "    \"\"\"\n",
    "    Plots the usage of tools categorized by their priority.\n",
    "\n",
    "    Purpose:\n",
    "    This function visualizes the usage of tools in a horizontal bar chart, categorized by their priority (Primary, Optional, Global, Uncategorized). It helps in understanding the distribution of tool usage based on their priority and scope.\n",
    "\n",
    "    Parameters:\n",
    "    tool_usage_dict (dict): A dictionary where keys are tool names and values are their respective usage counts.\n",
    "    title (str): The title of the plot. Default is \"Tool Usage Summary\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Calls the categorize_tools_by_priority function to categorize the tools based on their priority.\n",
    "    2. Initializes lists to store tool names, usage counts, and colors for plotting.\n",
    "    3. Iterates through each category (Primary, Optional, Global, Uncategorized) and appends the tool names, counts, and corresponding colors to the lists.\n",
    "    4. Creates a horizontal bar plot using matplotlib with the tool names, counts, and colors.\n",
    "    5. Adds usage count labels to each bar.\n",
    "    6. Inverts the y-axis to display the highest usage count at the top.\n",
    "    7. Adds a legend to the plot indicating the tool categories.\n",
    "    8. Displays the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    categorized = categorize_tools_by_priority(\n",
    "        tool_usage_dict, \n",
    "        tool_priority_map, \n",
    "        global_tools\n",
    "    )\n",
    "\n",
    "    # Combine for plotting\n",
    "    tools, counts, colors = [], [], []\n",
    "    color_map = {\"Primary\": \"green\", \"Optional\": \"orange\", \"Global\": \"blue\", \"Uncategorized\": \"gray\"}\n",
    "\n",
    "    for category in [\"Primary\", \"Optional\", \"Global\", \"Uncategorized\"]:\n",
    "        for tool, count in categorized[category].items():\n",
    "            tools.append(tool)\n",
    "            counts.append(count)\n",
    "            colors.append(color_map[category])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(tools, counts, color=colors)\n",
    "    plt.xlabel(\"Usage Count\")\n",
    "    plt.title(title)\n",
    "\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.2, bar.get_y() + bar.get_height()/2, str(int(width)), va='center')\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add legend\n",
    "    legend_patches = [mpatches.Patch(color=color_map[c], label=c) for c in color_map]\n",
    "    plt.legend(handles=legend_patches, title=\"Tool Category\", loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to search the web for relevant information\n",
    "\n",
    "def search_web(query, max_results=1):\n",
    "    \"\"\"\n",
    "    Searches the web for relevant information using DuckDuckGo.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the DuckDuckGo search engine to find relevant information based on a given query. It returns the snippet of the first search result.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query to find relevant information.\n",
    "    max_results (int): The maximum number of search results to retrieve. Default is 1.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a DuckDuckGo search session using DDGS.\n",
    "    2. Performs a text search with the given query and retrieves up to `max_results` results.\n",
    "    3. Iterates through the search results and returns the snippet of the first result.\n",
    "    4. If no results are found, returns a message indicating no relevant results were found.\n",
    "    5. If an exception occurs during the search, returns a message indicating the web search failed along with the exception message.\n",
    "\n",
    "    Returns:\n",
    "    str: The snippet of the first search result, or a message indicating no relevant results were found or the web search failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.text(query, max_results=max_results)\n",
    "            for r in results:\n",
    "                return r[\"body\"]  # return the first result's snippet\n",
    "        return \"No relevant results found.\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Web search failed: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check for jargon or technical terms in a section\n",
    "\n",
    "JARGON_LIST = {\n",
    "    \"synergy\", \"leverage\", \"optimize\", \"stakeholder alignment\", \"enablement\",\n",
    "    \"digital transformation\", \"bandwidth\", \"scalability\", \"paradigm\",\n",
    "    \"blockchain\", \"AI\", \"ML\", \"IoT\", \"Zero Trust\", \"DevOps\", \"infrastructure-as-code\",\n",
    "    \"EHR\", \"CRM\", \"VPN\", \"cloud-native\", \"containerization\", \"agile methodology\"\n",
    "}\n",
    "\n",
    "def check_for_jargon(section_text):\n",
    "    \"\"\"\n",
    "    Checks for jargon or technical terms in a section of the report.\n",
    "\n",
    "    Purpose:\n",
    "    This function helps identify the presence of jargon or technical terms in a section of the report by searching for predefined terms.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to search within.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an empty list `found_terms` to store the jargon or technical terms found in the section.\n",
    "    2. Iterates through each term in the `JARGON_LIST` set.\n",
    "    3. For each term, constructs a regex pattern to match the term as a whole word (case-insensitive).\n",
    "    4. Searches for the term in the section text using the regex pattern.\n",
    "    5. If the term is found, appends it to the `found_terms` list.\n",
    "    6. After checking all terms, returns a message indicating the jargon or technical terms found or a message indicating that no notable jargon or technical terms were found.\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating the jargon or technical terms found in the section, or a message indicating that no notable jargon or technical terms were found.\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    for term in JARGON_LIST:\n",
    "        pattern = r\"\\b\" + re.escape(term) + r\"\\b\"\n",
    "        if re.search(pattern, section_text, flags=re.IGNORECASE):\n",
    "            found_terms.append(term)\n",
    "    if found_terms:\n",
    "        return f\"The section includes jargon or technical terms: {', '.join(found_terms)}.\"\n",
    "    else:\n",
    "        return \"No notable jargon or technical terms found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to generate client questions based on a section of a report\n",
    "def generate_client_questions(section_text, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Generates client questions based on a section of an IT strategy report.\n",
    "\n",
    "    Purpose:\n",
    "    This function acts as a skeptical client reviewing a section of an IT strategy report and generates 3-5 clarifying or challenging questions based on potential assumptions, unclear terms, or missing context.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to generate questions for.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that instructs the model to act as a skeptical client and generate questions based on the section text.\n",
    "    2. Creates a list of messages with the constructed prompt.\n",
    "    3. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    4. If the API call is successful, returns the generated questions.\n",
    "    5. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated questions or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are acting as a skeptical client reviewing the following section of an IT strategy report.\\n\"\n",
    "        \"Generate 3-5 clarifying or challenging questions the client might ask based on potential assumptions, unclear terms, or missing context.\\n\\n\"\n",
    "        f\"Section:\\n{section_text}\\n\\n\"\n",
    "        \"Questions:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate questions: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check report has expected sections\n",
    "def highlight_missing_sections(report_sections):\n",
    "    \"\"\"\n",
    "    Compares expected canonical (standard) sections against the actual report_sections keys.\n",
    "    Returns a list of missing expected sections.\n",
    "    \"\"\"\n",
    "    expected_sections = set(canonical_section_map.keys())\n",
    "    present_sections = set(report_sections.keys())\n",
    "    \n",
    "    missing = expected_sections - present_sections\n",
    "    if missing:\n",
    "        return \"🚨 Missing sections:\\n\" + \"\\n\".join(f\"- {s}\" for s in sorted(missing))\n",
    "    else:\n",
    "        return \"✅ All expected sections are present.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to check alignment between section and report goals\n",
    "def check_alignment_with_goals(section_name, report_sections_dict, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Checks the alignment between the goals of the report and a specific section.\n",
    "\n",
    "    Purpose:\n",
    "    This function evaluates whether a specific section of the report aligns with the stated goals and objectives. It uses the OpenAI API to generate an evaluation of the alignment.\n",
    "\n",
    "    Parameters:\n",
    "    section_name (str): The name of the section to evaluate for alignment.\n",
    "    report_sections_dict (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Tries to find the \"Goals & Objectives\" section in the report.\n",
    "    2. If the \"Goals & Objectives\" section is not found, searches for goals in other sections based on keywords.\n",
    "    3. Retrieves the text of the specified section to evaluate.\n",
    "    4. If either the goals or the section text is not found, returns a warning message.\n",
    "    5. Constructs a prompt for the OpenAI API to evaluate the alignment between the goals and the specified section.\n",
    "    6. Calls the OpenAI API with tracking to get the evaluation.\n",
    "    7. Returns the evaluation or an error message if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "    str: The evaluation of the alignment between the goals and the specified section, or an error message if the API call fails.\n",
    "    \"\"\"\n",
    "    # Step 1: Try exact match first\n",
    "    try:\n",
    "        response = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to check alignment: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to compare two sections of a report for duplication, contradictions, or inconsistencies\n",
    "# This tool compares two sections of an IT consulting report for duplication, contradictions, or inconsistencies.\n",
    "# It also notes if one section covers content that the other should include.\n",
    "def compare_with_other_section(section_a, section_b, report_sections_dict, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Compares two sections of an IT consulting report for duplication, contradictions, or inconsistencies.\n",
    "\n",
    "    Purpose:\n",
    "    This function compares two specified sections of an IT consulting report to identify any duplication, contradictions, or inconsistencies between them. It also notes if one section covers content that the other should include.\n",
    "\n",
    "    Parameters:\n",
    "    section_a (str): The name of the first section to compare.\n",
    "    section_b (str): The name of the second section to compare.\n",
    "    report_sections_dict (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the text of the specified sections from the report_sections_dict.\n",
    "    2. If either section is not found, returns a warning message.\n",
    "    3. Constructs a prompt for the OpenAI API to compare the two sections.\n",
    "    4. Calls the OpenAI API with tracking to get the comparison.\n",
    "    5. Returns the comparison or an error message if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "    str: A summary of the comparison between the two sections, or an error message if the API call fails.\n",
    "    \"\"\"\n",
    "    text_a = report_sections_dict.get(section_a)\n",
    "    text_b = report_sections_dict.get(section_b)\n",
    "\n",
    "    if not text_a or not text_b:\n",
    "        return f\"⚠️ One or both sections not found: '{section_a}' or '{section_b}'\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are comparing two sections of an IT consulting report.\\n\"\n",
    "        f\"Identify any duplication, contradictions, or inconsistencies between them.\\n\"\n",
    "        f\"Also note if one section covers content the other should include.\\n\\n\"\n",
    "        f\"Section A: {section_a}\\n{text_a}\\n\\n\"\n",
    "        f\"Section B: {section_b}\\n{text_b}\\n\\n\"\n",
    "        \"Provide a 3-5 sentence summary of your comparison:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to compare sections: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show agent memory\n",
    "def show_agent_memory(agent):\n",
    "    \"\"\"\n",
    "    Displays a snapshot of the agent's memory, including section notes, cross-section observations, and tool usage history.\n",
    "\n",
    "    Purpose:\n",
    "    This function provides a detailed view of the agent's internal memory, which includes notes for each section, observations comparing different sections, and the history of tool usage. This is useful for understanding the agent's reasoning process and the actions it has taken.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be displayed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Prints a header \"Agent Memory Snapshot\".\n",
    "    2. Prints the section notes stored in the agent's memory.\n",
    "    3. Prints the cross-section observations stored in the agent's memory.\n",
    "    4. Prints the tool usage history stored in the agent's memory.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\n🧠 Agent Memory Snapshot\\n\")\n",
    "\n",
    "    print(\"🔹 Section Notes:\")\n",
    "    for section, notes in agent.memory[\"section_notes\"].items():\n",
    "        print(f\"- {section}:\")\n",
    "        for note in notes:\n",
    "            print(f\"  • {note}\")\n",
    "\n",
    "    print(\"\\n🔹 Cross-Section Observations:\")\n",
    "    for a, b, obs in agent.memory[\"cross_section_flags\"]:\n",
    "        print(f\"- {a} vs. {b}: {obs}\")\n",
    "\n",
    "    print(\"\\n🔹 Tool History:\")\n",
    "    for step, action, section in agent.memory[\"tool_history\"]:\n",
    "        print(f\"Step {step} | {action} | Section: {section}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generates a final summary for the client based on the agent's memory of section insights and cross-section observations.\n",
    "\n",
    "    Purpose:\n",
    "    This function constructs a prompt using the agent's memory of section insights and cross-section observations to generate a final summary for the client. It uses the OpenAI API to create a concise summary covering strengths, issues, and overall alignment with goals.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be used for generating the summary.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves section notes and cross-section observations from the agent's memory.\n",
    "    2. Constructs a prompt that includes the section insights and cross-section observations.\n",
    "    3. Adds instructions to write a short, clear 4-6 sentence final summary covering strengths, issues, and overall alignment with goals.\n",
    "    4. Creates a list of messages with the constructed prompt.\n",
    "    5. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    6. If the API call is successful, returns the generated summary.\n",
    "    7. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated final summary or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    # Build a summary prompt using memory\n",
    "    notes_by_section = agent.memory.get(\"section_notes\", {})\n",
    "    cross_section = agent.memory.get(\"cross_section_flags\", [])\n",
    "\n",
    "    prompt = \"You are a senior consultant wrapping up your review of an IT strategy report.\\n\"\n",
    "    prompt += \"Use the following section insights and cross-section observations to write a final summary for the client.\\n\\n\"\n",
    "\n",
    "    for section, notes in notes_by_section.items():\n",
    "        prompt += f\"Section: {section}\\n\"\n",
    "        for note in notes:\n",
    "            prompt += f\"- {note}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    if cross_section:\n",
    "        prompt += \"Cross-Section Findings:\\n\"\n",
    "        for a, b, obs in cross_section:\n",
    "            prompt += f\"- {a} vs. {b}: {obs}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    # Include overall score summary\n",
    "    if \"section_scores\" in agent.memory:\n",
    "        prompt += \"\\nOverall Ratings:\\n\"\n",
    "        for section, score_text in agent.memory[\"section_scores\"].items():\n",
    "            prompt += f\"{section}:\\n{score_text}\\n\\n\"\n",
    "            \n",
    "    prompt += \"Write a short, clear 4-6 sentence final summary covering strengths, issues, and overall alignment with goals.\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        return call_openai_with_tracking(messages, model=model, temperature=temperature).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate final summary: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_report_to_markdown(agent, filename=\"consultant_ai_report.md\"):\n",
    "    \"\"\"\n",
    "    Exports the consulting report review to a markdown file.\n",
    "\n",
    "    Purpose:\n",
    "    This function generates a markdown file summarizing the consulting report review, including the final summary, section insights, and cross-section findings.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be exported.\n",
    "    filename (str): The name of the markdown file to save. Default is \"consultant_ai_report.md\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Creates an output directory if it does not exist.\n",
    "    2. Opens the specified file in write mode.\n",
    "    3. Writes the report title to the file.\n",
    "    4. Retrieves the final summary from the agent's memory and writes it to the file.\n",
    "    5. Iterates through the section notes in the agent's memory and writes each section's insights to the file.\n",
    "    6. If there are cross-section findings, writes them to the file.\n",
    "    7. Prints a confirmation message indicating the file has been saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"# 🧾 AI-Powered Consulting Report\\n\\n\")\n",
    "\n",
    "        # Final summary\n",
    "        final = agent.memory.get(\"final_summary\", \"No final summary generated.\")\n",
    "        f.write(\"## Final Summary\\n\\n\")\n",
    "        f.write(final + \"\\n\\n\")\n",
    "\n",
    "        # Top Issues\n",
    "        if \"top_issues\" in agent.memory:\n",
    "            f.write(\"\\n## Top 3 Issues\\n\")\n",
    "            f.write(agent.memory[\"top_issues\"] + \"\\n\")\n",
    "\n",
    "        # Section Insights and Ratings\n",
    "        f.write(\"## Section Insights, Ratings, Fixes, and Confidence Levels\\n\")\n",
    "        for section in agent.memory[\"section_notes\"].keys():\n",
    "            f.write(f\"\\n### {section}\\n\")\n",
    "            \n",
    "            # Write section notes\n",
    "            notes = agent.memory[\"section_notes\"].get(section, [])\n",
    "            if notes:\n",
    "                f.write(\"\\n**Notes:**\\n\")\n",
    "                for note in notes:\n",
    "                    f.write(f\"- {note}\\n\")\n",
    "            \n",
    "            # Write section scores\n",
    "            score_text = agent.memory[\"section_scores\"].get(section, \"\")\n",
    "            if score_text:\n",
    "                f.write(\"\\n**Ratings:**\\n\")\n",
    "                f.write(f\"{format_score_block(score_text)}\\n\")\n",
    "                \n",
    "            # Write section fixes\n",
    "            fixes = agent.memory[\"section_fixes\"].get(section, \"\")\n",
    "            if fixes:\n",
    "                f.write(\"\\n**Fix Recommendations:**\\n\")\n",
    "                f.write(f\"{fixes}\\n\")\n",
    "                \n",
    "            # Write confidence levels\n",
    "            confidence = agent.memory[\"confidence_levels\"].get(section, \"\")\n",
    "            if confidence:\n",
    "                f.write(f\"\\n**Confidence Level:** {confidence}/10\\n\")\n",
    "\n",
    "        # Cross-section\n",
    "        if agent.memory[\"cross_section_flags\"]:\n",
    "            f.write(\"\\n## Cross-Section Findings\\n\")\n",
    "            for a, b, obs in agent.memory[\"cross_section_flags\"]:\n",
    "                f.write(f\"- **{a} vs. {b}**: {obs}\\n\")\n",
    "        \n",
    "        # Missing Sections\n",
    "        if \"highlight_missing\" in agent.memory:\n",
    "            f.write(\"\\n## Missing Sections Check\\n\")\n",
    "            f.write(agent.memory[\"highlight_missing\"] + \"\\n\")\n",
    "        \n",
    "        if \"missing_analysis\" in agent.memory:\n",
    "            f.write(\"\\n## Analysis of Missing Sections\\n\")\n",
    "            f.write(agent.memory[\"missing_analysis\"] + \"\\n\")\n",
    "\n",
    "\n",
    "    print(f\"✅ Markdown report saved as: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def export_report_to_markdown_and_pdf(agent, markdown_file=\"consultant_ai_report.md\", pdf_file=\"consultant_ai_report.pdf\"):\n",
    "    \"\"\"\n",
    "    Exports the consulting report review to both markdown and PDF formats.\n",
    "\n",
    "    Purpose:\n",
    "    This function generates a markdown file summarizing the consulting report review and then converts it to a PDF file. It ensures the output directory exists, exports the report to markdown, converts the markdown to HTML, and finally renders the HTML to a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be exported.\n",
    "    markdown_file (str): The name of the markdown file to save. Default is \"consultant_ai_report.md\".\n",
    "    pdf_file (str): The name of the PDF file to save. Default is \"consultant_ai_report.pdf\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Ensures the output directory exists.\n",
    "    2. Exports the report to a markdown file using the `export_report_to_markdown` function.\n",
    "    3. Reads the markdown file and converts its content to HTML.\n",
    "    4. Wraps the HTML content in a basic HTML page structure.\n",
    "    5. Saves the HTML content to a temporary file.\n",
    "    6. Uses Playwright to render the HTML file to a PDF.\n",
    "    7. Handles any exceptions that occur during the PDF rendering process and prints appropriate messages.\n",
    "\n",
    "    Asynchronous Workflow:\n",
    "    1. Opens an async Playwright session.\n",
    "    2. Launches a Chromium browser.\n",
    "    3. Opens a new page in the browser.\n",
    "    4. Navigates to the local HTML file.\n",
    "    5. Generates a PDF from the HTML content.\n",
    "    6. Closes the browser.\n",
    "    7. Prints a success message if the PDF is saved successfully.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_dir = \"../outputs/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    markdown_file = os.path.join(output_dir, markdown_file)\n",
    "    pdf_file = os.path.join(output_dir, pdf_file)\n",
    "    \n",
    "    # Step 1: Export to Markdown\n",
    "    export_report_to_markdown(agent, filename=markdown_file)\n",
    "\n",
    "    # Step 2: Convert to HTML\n",
    "    with open(markdown_file, \"r\") as f:\n",
    "        md_text = f.read()\n",
    "    html_content = markdown(md_text)\n",
    "\n",
    "    # Optional: wrap in basic HTML page\n",
    "    full_html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Consulting Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "            h1, h2, h3 {{ color: #003366; }}\n",
    "            ul {{ margin-top: 0; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    {html_content}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Save HTML and render to PDF\n",
    "    temp_html_path = os.path.join(output_dir, \"temp_report.html\")\n",
    "    with open(temp_html_path, \"w\") as f:\n",
    "        f.write(full_html)\n",
    "\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(\"file://\" + os.path.abspath(temp_html_path))\n",
    "            await page.pdf(path=pdf_file, format=\"A4\")\n",
    "            await browser.close()\n",
    "            print(f\"✅ PDF report saved as: {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ PDF export failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_section(section_name, section_text, goals_text=None, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Scores a section of a consulting report based on clarity, alignment, and completeness.\n",
    "\n",
    "    Purpose:\n",
    "    This function evaluates a specified section of a consulting report using three criteria: clarity, alignment with goals, and completeness. It generates a score out of 10 for each criterion along with a one-line explanation for each score.\n",
    "\n",
    "    Parameters:\n",
    "    section_name (str): The name of the section to evaluate.\n",
    "    section_text (str): The text of the section to evaluate.\n",
    "    goals_text (str, optional): The text of the report's goals to evaluate alignment. Default is None.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Constructs a prompt that instructs the model to evaluate the section based on clarity, alignment, and completeness.\n",
    "    2. If goals_text is provided, includes it in the prompt for evaluating alignment.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the generated scores and explanations.\n",
    "    6. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated scores and explanations for clarity, alignment, and completeness, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Evaluate the section '{section_name}' of a consulting report using the following 3 criteria:\\n\"\n",
    "        \"1. Clarity (Is the writing clear, well-structured, and understandable?)\\n\"\n",
    "        \"2. Alignment (Does it align with the report’s goals?)\\n\"\n",
    "        \"3. Completeness (Does it cover the necessary topics?)\\n\\n\"\n",
    "    )\n",
    "\n",
    "    if goals_text:\n",
    "        prompt += f\"Report Goals:\\n{goals_text}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Section:\\n{section_text}\\n\\n\"\n",
    "    prompt += (\n",
    "        \"Provide a score out of 10 for each criterion with a one-line explanation per score. Format:\\n\"\n",
    "        \"Clarity: [score]/10 – [reason]\\n\"\n",
    "        \"Alignment: [score]/10 – [reason]\\n\"\n",
    "        \"Completeness: [score]/10 – [reason]\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = call_openai_with_tracking(messages, model=model, temperature=temperature)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to score section: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_section_insights(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Summarize key insights from this section's reasoning steps into a short, client-friendly paragraph.\n",
    "    \"\"\"\n",
    "    steps = agent.history[-5:]  # Last 5 steps, or you could use all\n",
    "    thoughts = \"\\n\".join([f\"Thought: {s['thought']}\\nAction: {s['action']}\\nObservation: {s['observation']}\" for s in steps])\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are summarizing an internal AI review of the consulting report section '{agent.section_name}'.\\n\"\n",
    "        \"Write a concise, client-facing summary of the most important observations based on the review.\\n\"\n",
    "        \"Focus on useful insights, potential risks, and gaps. Avoid tool names or internal system messages.\\n\\n\"\n",
    "        f\"Review Log:\\n{thoughts}\\n\\n\"\n",
    "        \"Summary:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(messages, model=model, temperature=temperature).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to summarize section insights: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_issues(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Extracts the top 3 most important issues or gaps from the section summaries in the agent's memory.\n",
    "\n",
    "    Purpose:\n",
    "    This function reviews feedback across multiple sections of a report and identifies the top 3 most important issues or gaps that should be addressed. It prioritizes issues that impact clarity, alignment, or completeness across the report.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be reviewed.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the section summaries from the agent's memory.\n",
    "    2. Concatenates the section summaries into a single string.\n",
    "    3. Constructs a prompt instructing the model to identify the top 3 most important issues or gaps based on the section summaries.\n",
    "    4. Creates a list of messages with the constructed prompt.\n",
    "    5. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    6. If the API call is successful, returns the extracted top issues.\n",
    "    7. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted top 3 issues or gaps, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    summaries = agent.memory.get(\"section_notes\", {})\n",
    "    all_text = \"\\n\".join([f\"{sec}: {obs[0]}\" for sec, obs in summaries.items()])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an AI consultant reviewing feedback across multiple sections of a report.\\n\"\n",
    "        \"Based on the following section summaries, identify the top 3 most important issues or gaps that should be addressed.\\n\"\n",
    "        \"Be concise, specific, and prioritize issues that impact clarity, alignment, or completeness across the report.\\n\\n\"\n",
    "        f\"Section Summaries:\\n{all_text}\\n\\n\"\n",
    "        \"Top 3 Issues:\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(messages, model=model, temperature=temperature).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to extract top issues: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_score_block(score_text):\n",
    "    \"\"\"\n",
    "    Formats a block of score text by adding icons based on the score.\n",
    "\n",
    "    Purpose:\n",
    "    This function processes a block of text containing scores out of 10 and adds icons to each line based on the score. The icons indicate the quality of the score: green for high scores, yellow for medium scores, and red for low scores.\n",
    "\n",
    "    Parameters:\n",
    "    score_text (str): A block of text containing scores out of 10.\n",
    "\n",
    "    Workflow:\n",
    "    1. Splits the input score_text into individual lines.\n",
    "    2. For each line, searches for a score in the format \"X/10\".\n",
    "    3. If a score is found:\n",
    "       - Adds a green icon (🟢) for scores 8 and above.\n",
    "       - Adds a yellow icon (🟡) for scores between 6 and 7.\n",
    "       - Adds a red icon (🔴) for scores below 6.\n",
    "    4. If no score is found, the line remains unchanged.\n",
    "    5. Joins the processed lines back into a single block of text.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted block of text with icons added based on the scores.\n",
    "    \"\"\"\n",
    "    def add_icons(line):\n",
    "        match = re.search(r\"(\\d+)/10\", line)\n",
    "        if not match:\n",
    "            return line\n",
    "        score = int(match.group(1))\n",
    "        if score >= 8:\n",
    "            icon = \"🟢\"\n",
    "        elif score >= 6:\n",
    "            icon = \"🟡\"\n",
    "        else:\n",
    "            icon = \"🔴\"\n",
    "        return f\"{icon} {line}\"\n",
    "    \n",
    "    return \"\".join([add_icons(line) + \"  \\n\" for line in score_text.splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_level(agent, model=\"gpt-3.5-turbo\", temperature=0.6):\n",
    "    \"\"\"\n",
    "    Evaluates the confidence level of the AI consultant's reasoning steps.\n",
    "\n",
    "    Purpose:\n",
    "    This function assesses the confidence level of the AI consultant's reasoning steps based on clarity, consistency, and support of the analysis. It uses the OpenAI API to rate the confidence level from 1 to 10.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the reasoning history to be evaluated.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.6.\n",
    "\n",
    "    Workflow:\n",
    "    1. Extracts the last 5 reasoning steps from the agent's history.\n",
    "    2. Constructs a prompt that includes the reasoning steps and asks for a confidence level rating from 1 to 10.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the confidence level rating.\n",
    "    6. If an exception occurs, returns a warning message.\n",
    "\n",
    "    Returns:\n",
    "    str: The confidence level rating from 1 to 10, or a warning message if the API call fails.\n",
    "    \"\"\"\n",
    "    reasoning_log = \"\\n\".join([f\"{s['thought']} → {s['action']} → {s['observation']}\" for s in agent.history[-5:]])\n",
    "    prompt = (\n",
    "        \"Given the following reasoning steps from an AI consultant reviewing a report section, rate the confidence \"\n",
    "        \"level from 1 to 10 based on how clear, consistent, and well-supported the analysis appears. Just return a number (1–10).\\n\\n\"\n",
    "        f\"{reasoning_log}\\n\\nConfidence Level:\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(messages, model=model, temperature=temperature).strip()\n",
    "    except Exception as e:\n",
    "        return \"⚠️\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fixes(agent, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generates specific fixes or improvements for a consulting report section.\n",
    "\n",
    "    Purpose:\n",
    "    This function reviews the notes for a specific section of a consulting report and generates 2-3 specific fixes or improvements to make the section stronger.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, which contains the memory data to be reviewed.\n",
    "    model (str): The model to use for the API call. Default is \"gpt-3.5-turbo\".\n",
    "    temperature (float): The sampling temperature to use. Higher values mean the model will take more risks. Default is 0.7.\n",
    "\n",
    "    Workflow:\n",
    "    1. Retrieves the notes for the current section from the agent's memory.\n",
    "    2. Constructs a prompt that includes the section notes and asks for 2-3 specific fixes or improvements.\n",
    "    3. Creates a list of messages with the constructed prompt.\n",
    "    4. Calls the OpenAI API with tracking using the call_openai_with_tracking function.\n",
    "    5. If the API call is successful, returns the generated fixes or improvements.\n",
    "    6. If an exception occurs, returns a failure message with the exception details.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated fixes or improvements, or a failure message if the API call fails.\n",
    "    \"\"\"\n",
    "    notes = agent.memory[\"section_notes\"].get(agent.section_name, [\"\"])[0]\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are reviewing this consulting section:\\n\\n{notes}\\n\\n\"\n",
    "        \"List 2–3 specific fixes or improvements to make this section stronger.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return call_openai_with_tracking(messages, model=model, temperature=temperature).strip()\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to generate fixes: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_summary_support(summary_text, other_sections):\n",
    "    \"\"\"\n",
    "    Check if the summary reflects content from the rest of the report.\n",
    "    \"\"\"\n",
    "    combined = \"\\n\".join(other_sections.values())\n",
    "    prompt = (\n",
    "        \"Does the following summary accurately reflect the details and points made in the full report?\\n\\n\"\n",
    "        f\"Summary:\\n{summary_text}\\n\\n\"\n",
    "        f\"Report Body:\\n{combined}\\n\\n\"\n",
    "        \"Answer in 3–5 sentences highlighting any gaps, overstatements, or missing support.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_smart_goals(section_text):\n",
    "    \"\"\"\n",
    "    Evaluate whether the section's goals follow the SMART criteria.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Evaluate the following goals using the SMART criteria:\\n\"\n",
    "        \"Specific, Measurable, Achievable, Relevant, Time-bound.\\n\\n\"\n",
    "        f\"Goals:\\n{section_text}\\n\\n\"\n",
    "        \"Rate each of the SMART attributes and explain briefly.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recommendation_alignment(recommendation_text, goals_text):\n",
    "    \"\"\"\n",
    "    Check if recommendations align with the stated goals.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Here are the goals of the report:\\n{goals_text}\\n\\n\"\n",
    "        f\"And here are the key recommendations:\\n{recommendation_text}\\n\\n\"\n",
    "        \"Do the recommendations clearly align with the goals? Identify any mismatches or missing connections.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_sections(report_sections):\n",
    "    \"\"\"\n",
    "    Analyzes missing sections in an IT consulting report and evaluates their impact.\n",
    "\n",
    "    Purpose:\n",
    "    This function identifies any missing sections in an IT consulting report based on a predefined set of expected sections. It then evaluates how the absence of these sections might affect the report's effectiveness, highlighting any critical omissions.\n",
    "\n",
    "    Parameters:\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "\n",
    "    Workflow:\n",
    "    1. Identifies the missing sections by comparing the keys in `report_sections` with the expected sections in `canonical_section_map`.\n",
    "    2. If no sections are missing, returns a message indicating that no critical sections are missing.\n",
    "    3. If there are missing sections, constructs a prompt listing the missing sections and asking how their absence might affect the report's effectiveness.\n",
    "    4. Calls the OpenAI API with tracking using the `call_openai_with_tracking` function to get the evaluation.\n",
    "    5. Returns the evaluation from the OpenAI API.\n",
    "\n",
    "    Returns:\n",
    "    str: The evaluation of the impact of the missing sections on the report's effectiveness, or a message indicating that no critical sections are missing.\n",
    "    \"\"\"\n",
    "    missing = list(set(canonical_section_map.keys()) - set(report_sections.keys()))\n",
    "    if not missing:\n",
    "        return \"✅ No critical sections appear to be missing from the report.\"\n",
    "\n",
    "    missing_str = \"\\n\".join(f\"- {s}\" for s in sorted(missing))\n",
    "\n",
    "    prompt = (\n",
    "        \"You're reviewing an IT consulting report.\\n\"\n",
    "        \"The following expected sections are missing:\\n\"\n",
    "        f\"{missing_str}\\n\\n\"\n",
    "        \"Based on standard consulting practices, how might these missing sections affect the report's effectiveness?\\n\"\n",
    "        \"Highlight if any of these are critical omissions, and explain why.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return call_openai_with_tracking(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_report_review(agent, report_sections, max_steps=5):\n",
    "    \"\"\"\n",
    "    Conducts a full review of an IT consulting report using the ReAct framework.\n",
    "\n",
    "    Purpose:\n",
    "    This function iterates through all sections of an IT consulting report, performing a reasoning and action loop for each section using the ReAct framework. It then performs post-processing steps to highlight missing sections, generate a final summary, and extract the top issues.\n",
    "\n",
    "    Parameters:\n",
    "    agent (ReActConsultantAgent): An instance of the ReActConsultantAgent class, initialized with the section name and text.\n",
    "    report_sections (dict): A dictionary where keys are section headers and values are the corresponding section contents.\n",
    "    max_steps (int): The maximum number of steps to run the loop for each section. Default is 5.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates through each section in the report_sections dictionary.\n",
    "    2. For each section:\n",
    "       - Sets the agent's section_name and section_text attributes.\n",
    "       - Calls the run_react_loop_check_withTool function to perform the reasoning and action loop.\n",
    "    3. After processing all sections, performs post-processing steps:\n",
    "       - Calls highlight_missing_sections to identify any missing sections in the report.\n",
    "       - Calls generate_final_summary to generate a final summary of the report.\n",
    "       - Calls extract_top_issues to identify the top issues or gaps in the report.\n",
    "    4. Stores the results of the post-processing steps in the agent's memory.\n",
    "\n",
    "    Returns:\n",
    "    ReActConsultantAgent: The agent instance with updated memory containing the results of the full report review.\n",
    "    \"\"\"\n",
    "    # Loop through all sections\n",
    "    for section_name, section_text in report_sections.items():\n",
    "        agent.section_name = section_name\n",
    "        agent.section_text = section_text\n",
    "        run_react_loop_check_withTool(agent, max_steps=max_steps)\n",
    "\n",
    "    # Post-processing steps\n",
    "    agent.memory[\"highlight_missing\"] = highlight_missing_sections(report_sections)\n",
    "    agent.memory[\"missing_analysis\"] = analyze_missing_sections(report_sections)\n",
    "    agent.memory[\"final_summary\"] = generate_final_summary(agent)\n",
    "    agent.memory[\"top_issues\"] = extract_top_issues(agent)\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to assess readability of a section with textstat scores\n",
    "# Flesch Reading Ease: higher score indicates easier readability\n",
    "# Reading Level Estimate: grade level of text\n",
    "# Difficult Words Count: number of difficult words in the text\n",
    "def check_readability(section_text):\n",
    "    score = textstat.flesch_reading_ease(section_text)\n",
    "    level = textstat.text_standard(section_text)\n",
    "    difficult = textstat.difficult_words(section_text)\n",
    "\n",
    "    summary = (\n",
    "        f\"📖 **Flesch Reading Ease**: {score:.1f} (higher = easier)\\n\"\n",
    "        f\"🏫 **Reading Level Estimate**: {level}\\n\"\n",
    "        f\"🧠 **Difficult Words Count**: {difficult}\"\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_react_step(agent, thought, action, step_num=0):\n",
    "    \"\"\"\n",
    "    Simulates a single ReAct step using a given thought and action.\n",
    "    Delegates action execution to dispatch_tool_action().\n",
    "\n",
    "    Returns:\n",
    "    - observation: result of executing the tool\n",
    "    \"\"\"\n",
    "    # Log tool usage\n",
    "    if hasattr(agent, \"tool_usage\"):\n",
    "        agent.tool_usage[action] = agent.tool_usage.get(action, 0) + 1\n",
    "\n",
    "    # Execute the action using shared dispatch function\n",
    "    observation = dispatch_tool_action(agent, action)\n",
    "\n",
    "    # Store step in history\n",
    "    agent.history.append({\n",
    "        \"thought\": thought,\n",
    "        \"action\": action,\n",
    "        \"observation\": observation\n",
    "    })\n",
    "\n",
    "    # Track tool usage in memory\n",
    "    agent.memory[\"tool_history\"].append((step_num, action, agent.section_name))\n",
    "\n",
    "    return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tool_action(action_string, text=\"Default text\", name=\"Test Section\"):\n",
    "    \"\"\"\n",
    "    Tests a specific tool action by simulating a single ReAct step.\n",
    "\n",
    "    Purpose:\n",
    "    This function creates an instance of the ReActConsultantAgent with a given section name and text, then simulates a single ReAct step using the specified action string. It prints the action being tested and the resulting observation.\n",
    "\n",
    "    Parameters:\n",
    "    action_string (str): The action to be tested.\n",
    "    text (str): The text of the section to be used for testing. Default is \"Default text\".\n",
    "    name (str): The name of the section to be used for testing. Default is \"Test Section\".\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes a ReActConsultantAgent instance with the given section name and text.\n",
    "    2. Calls the run_single_react_step function with the agent, a test thought, and the specified action string.\n",
    "    3. Prints the action being tested and the resulting observation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    agent = ReActConsultantAgent(section_name=name, section_text=text)\n",
    "    obs = run_single_react_step(agent, \"Test Thought\", action_string)\n",
    "    print(\"🔍 Tool:\", action_string)\n",
    "    print(\"🧠 Observation:\", obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tone_textblob(section_text):\n",
    "    \"\"\"\n",
    "    Analyzes the tone and clarity of a given text section using TextBlob.\n",
    "\n",
    "    Purpose:\n",
    "    This function uses the TextBlob library to analyze the sentiment of a given text section. It determines the tone (positive, neutral, or negative) based on the polarity score and the clarity (objective or subjective) based on the subjectivity score.\n",
    "\n",
    "    Parameters:\n",
    "    section_text (str): The text of the section to be analyzed.\n",
    "\n",
    "    Workflow:\n",
    "    1. Creates a TextBlob object from the section text.\n",
    "    2. Extracts the polarity and subjectivity scores from the TextBlob object.\n",
    "    3. Determines the tone based on the polarity score:\n",
    "       - If polarity > 0.2, the tone is positive.\n",
    "       - If polarity < -0.2, the tone is negative.\n",
    "       - Otherwise, the tone is neutral.\n",
    "    4. Determines the clarity based on the subjectivity score:\n",
    "       - If subjectivity < 0.4, the clarity is objective.\n",
    "       - Otherwise, the clarity is subjective.\n",
    "    5. Returns a formatted string with the tone and clarity information.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string indicating the tone and clarity of the text, including the polarity and subjectivity scores.\n",
    "    \"\"\"\n",
    "    blob = TextBlob(section_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    tone = \"neutral\"\n",
    "    if polarity > 0.2:\n",
    "        tone = \"positive\"\n",
    "    elif polarity < -0.2:\n",
    "        tone = \"negative\"\n",
    "\n",
    "    clarity = \"objective\" if subjectivity < 0.4 else \"subjective\"\n",
    "\n",
    "    return (\n",
    "        f\"💬 **Tone**: {tone} (polarity: {polarity:.2f})\\n\"\n",
    "        f\"🧠 **Clarity**: {clarity} (subjectivity: {subjectivity:.2f})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Load Data** <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample IT consulting report (replace with real one later)\n",
    "# You can later replace this with a real one or load from a file.\n",
    "# We can add file upload later (open(\"report.txt\").read())\n",
    "sample_report = \"\"\"\n",
    "Client: HealthConnect Systems\n",
    "Industry: Healthcare\n",
    "Project: IT Modernization Assessment\n",
    "\n",
    "Summary:\n",
    "HealthConnect currently operates on-premise infrastructure for its core clinical systems. While some departments use SaaS tools, there is no centralized cloud strategy. Security policies are documented but not consistently followed. There is no formal data governance framework. Leadership has expressed interest in migrating systems to the cloud.\n",
    "\n",
    "Goals & Objectives:\n",
    "1. Improve scalability and flexibility.\n",
    "2. Enhance data security and privacy.\n",
    "3. Streamline operations and reduce costs.\n",
    "4. Enable remote access for healthcare providers.\n",
    "5. Ensure compliance with industry regulations.\n",
    "\n",
    "Key Recommendations:\n",
    "1. Conduct a cloud readiness assessment.\n",
    "2. Begin phased migration of CRM and EHR systems.\n",
    "3. Establish a data governance committee.\n",
    "4. Update security protocols to align with NIST standards.\n",
    "\n",
    "Timeline: \n",
    "Estimated at 6–12 months for full migration planning and execution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 4: Pre-process Data** <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Overview:\n",
      "Client: HealthConnect Systems\n",
      "Industry: Healthcare\n",
      "Project: IT Modernization Assessment\n",
      "------------------------------------------------------------\n",
      "📌 Summary:\n",
      "HealthConnect currently operates on-premise infrastructure for its core clinical systems. While some departments use SaaS tools, there is no centralized cloud strategy. Security policies are documented but not consistently followed. There is no formal data governance framework. Leadership has expressed interest in migrating systems to the cloud.\n",
      "------------------------------------------------------------\n",
      "📌 Goals & Objectives:\n",
      "1. Improve scalability and flexibility.\n",
      "2. Enhance data security and privacy.\n",
      "3. Streamline operations and reduce costs.\n",
      "4. Enable remote access for healthcare providers.\n",
      "5. Ensure compliance with industry regulations.\n",
      "------------------------------------------------------------\n",
      "📌 Key Recommendations:\n",
      "1. Conduct a cloud readiness assessment.\n",
      "2. Begin phased migration of CRM and EHR systems.\n",
      "3. Establish a data governance committee.\n",
      "4. Update security protocols to align with NIST standards.\n",
      "------------------------------------------------------------\n",
      "📌 Timeline:\n",
      "Estimated at 6–12 months for full migration planning and execution.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run it on the sample report\n",
    "report_sections = split_report_into_sections(sample_report)\n",
    "\n",
    "# Display for verification\n",
    "for section, content in report_sections.items():\n",
    "    print(f\"📌 {section}:\\n{content}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 5: Model - Basic: Single-Hop Reasoning + Static Action Loop** <a id=\"5\"></a>\n",
    "\n",
    "1. Iterate through sections of report\n",
    "2. Send each section to ChatGPT for feedback\n",
    "3. Summarize feedback \n",
    "\n",
    "## **5.1 Initialize Agent** <a id=\"5.1\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the reasoning agent\n",
    "agent = ITReportReviewer(report_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Run Agent** <a id=\"5.2\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution skipped.\n"
     ]
    }
   ],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Define the order in which we want to review sections\n",
    "    sections_to_review = list(report_sections.keys())\n",
    "\n",
    "    # Loop through each section and generate a review\n",
    "    for section in sections_to_review:\n",
    "        agent.review_section(section)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution skipped.\n"
     ]
    }
   ],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Call the summarize_full_review function to get the final summary\n",
    "    final_summary = summarize_full_review(agent)\n",
    "    print(\"📋 Final Summary of the Report Review:\\n\")\n",
    "    print(final_summary)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 6: Model - ReAct** <a id=\"6\"></a>\n",
    "\n",
    "1. **Think** about each section (with ChatGPT)\n",
    "2. Decide on and take an **action**\n",
    "3. **Observe** the results and loop back to step 1 with new reasoning\n",
    "\n",
    "## **6.1 Simple Agent - Predefined Actions** <a id=\"6.1\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution skipped.\n"
     ]
    }
   ],
   "source": [
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Choose a section to run ReAct on\n",
    "    section_name = \"Summary\"\n",
    "    section_text = report_sections.get(section_name, \"\")\n",
    "\n",
    "    # Initialize the ReAct agent\n",
    "    react_agent = ReActConsultantAgent(section_name, section_text)\n",
    "\n",
    "    # Run the ReAct loop\n",
    "    react_review_history = run_react_loop_static(react_agent)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 Agent 2 - Custom & Pre-Built Tools** <a id=\"6.2\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution skipped.\n"
     ]
    }
   ],
   "source": [
    "## ReAct Loop with Tool Usage Tracking - one section at a time\n",
    "\n",
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    # Choose a section to run ReAct on\n",
    "    section_name = \"Key Recommendations\"\n",
    "    section_text = report_sections.get(section_name, \"\")\n",
    "\n",
    "    # Initialize the ReAct agent\n",
    "    react_agent = ReActConsultantAgent(section_name, section_text)\n",
    "\n",
    "    # Run the ReAct loop\n",
    "    react_review_history = run_react_loop_check_withTool(react_agent)\n",
    "\n",
    "    # Print tool usage summary\n",
    "    print_tool_usage(react_agent)\n",
    "    plot_tool_usage(react_agent.tool_usage)\n",
    "    \n",
    "else:\n",
    "    print(\"Cell execution skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution skipped.\n"
     ]
    }
   ],
   "source": [
    "## Run ReAct on the full report of all sections\n",
    "\n",
    "# Boolean to control cell execution\n",
    "execute_cell = False\n",
    "\n",
    "if execute_cell:\n",
    "    react_agent = ReActConsultantAgent(section_name=\"Full Report\", section_text=\"\")\n",
    "    react_agent = run_full_report_review(react_agent, report_sections)\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n🧾 Final Summary:\\n\")\n",
    "    print(react_agent.memory[\"final_summary\"])\n",
    "\n",
    "    # Export\n",
    "    await export_report_to_markdown_and_pdf(react_agent)\n",
    "    print(\"✅ Report exported successfully.\")\n",
    "\n",
    "    # Tool usage summary\n",
    "    print_tool_usage(react_agent)\n",
    "    plot_tool_usage(react_agent.tool_usage)\n",
    "else:\n",
    "    print(\"Cell execution skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Tool action: search_serpapi[\"Canada digital health strategy\"]\n",
      "🔍 Tool: search_serpapi[\"Canada digital health strategy\"]\n",
      "🧠 Observation: [' entity_type: related_questions.', 'Our strategic plan is rooted in the belief that digital health technologies and a highly skilled workforce have the potential to transform the way healthcare is ...', 'Canadian digital health professionals will access, use, and contribute to professional learning pathways and resources that advance their career and their.', \"Together with FPT health data partners, we're continuing to work towards a better-connected health system with standardized health data and digital tools.\", 'In Canada, digital health is considered a broad field leveraging digital technologies ( DTs ) across clinical and community care, to optimize health outcomes ...', 'Digital Health Strategy has 4 broad goals for making a digitally integrated health system: Empowering patients; Improving provider experience ...', 'The strategy emphasizes improving healthcare quality, safety, and access by leveraging real-time data, fostering patient-centered care, and streamlining ...', 'The Future of Digital Health in Canada: Key Focus Areas for 2025 · Interoperability and Data Integration · Expansion of Virtual Care · Artificial ...', 'This chapter unpacks Canadian digital health laws, covering regulatory, technologies, data use and sharing, IP, commercial agreements, AI and more.', 'During the COVID-19 pandemic, digital health services were rapidly deployed across Canada as an adaptable and low-barrier means to improve access to health ...', 'The B.C. Digital Health Strategy lays out the path to achieving the vision of leveraging digital health services, tools, and processes to enable a connected, ...']\n"
     ]
    }
   ],
   "source": [
    "# unit test tools\n",
    "\n",
    "test_tool_action('search_serpapi[\"Canada digital health strategy\"]')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hockey_ai",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
