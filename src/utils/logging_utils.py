import logging
from collections import defaultdict
import inspect
from pathlib import Path
import os

# Initialize logger
logger = logging.getLogger("ProposalEvaluator")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s", "%H:%M:%S")
ch.setFormatter(formatter)
logger.addHandler(ch)

# Initialize global variables / custom trackers
tool_stats = defaultdict(int)
tool_failure_stats = defaultdict(int)    # failed calls
tool_failure = {}
thought_stats = defaultdict(int)
thought_score_stats = defaultdict(int)
openai_call_log = []
openai_call_counter = 0
openai_call_times = []
openai_call_sources = defaultdict(int)
openai_prompt_token_usage_by_source = defaultdict(int)
openai_completion_token_usage_by_source = defaultdict(int)
#tool_call_times = defaultdict(list)



def log_phase(message):
    logger.info(f"üìå {message}")

def log_result(vendor, criterion, score):
    logger.info(f"‚úÖ [{vendor}] '{criterion}' scored {score}/10")

def log_tool_used(tool_name):
    tool_stats[tool_name] += 1
    logger.debug(f"‚öôÔ∏è Tool used: {tool_name}")

def log_thought_score(thought, score):
    thought_stats[thought] += 1
    thought_score_stats[score] += 1
    logger.debug(f"üí≠ Thought scored: {thought} with score {score}")

def log_openai_call(prompt, response, source=None, prompt_tokens=0, completion_tokens=0, embedding=True):
    global openai_call_counter
    openai_call_counter += 1

    if not source:
        # Automatically walk the stack to find the first external caller
        for frame in inspect.stack()[1:]:
            module = inspect.getmodule(frame.frame)
            if module and not module.__name__.startswith("logging_utils"):
                source = frame.function
                break
        else:
            source = "unknown"

    openai_call_sources[source] += 1
    openai_prompt_token_usage_by_source[source] += prompt_tokens
    openai_completion_token_usage_by_source[source] += completion_tokens

    if embedding:
        openai_call_log.append({
            "source": source,
            "call_type": "embedding",
            "prompt": prompt,
            "response": response.model_dump(),
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens
        })
        log_msg = (
            f"üîÑ OpenAI call #{openai_call_counter} from {source}: "
            f"Embedding call, no response logged and no token usage stats. "
        )
    else:
        openai_call_log.append({
            "source": source,
            "call_type": "chat.completion",
            "prompt": prompt,
            "response": response.model_dump(),
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens
        })
        log_msg = (
            f"üîÑ OpenAI call #{openai_call_counter} from {source}: "
            f"{prompt[:50]}... -> {str(response)[:50]}... "
            f"(Prompt tokens: {prompt_tokens}, Completion tokens: {completion_tokens})"
        )
    logger.info(log_msg)


def print_tool_stats():
    logger.info("üìä Tool usage summary:")
    for tool, count in tool_stats.items():
        logger.info(f"   {tool}: {count} time(s)")

def print_thought_stats():
    logger.info("üìä Thought generation summary:")
    score_distribution = defaultdict(int)
    for score, count in thought_score_stats.items():
        score_distribution[score] += count
    for score, count in sorted(score_distribution.items(), reverse=True):
        logger.info(f"   Thought score {score}: {count} time(s)")

def log_tool_execution(tool_name, tool_fn, input_arg=None, agent=None):
    fn_module = inspect.getmodule(tool_fn).__name__
    input_preview = input_arg[:100] + "..." if input_arg and len(input_arg) > 100 else input_arg
    log_phase(f"üß™ Executing tool: {tool_name} from {fn_module}")
    if input_arg:
        log_phase(f"üîπ Input: {input_preview}")
    if agent and hasattr(agent, 'section_name'):
        log_phase(f"üìÑ Section: {agent.section_name}")

def log_tool_failed(tool_name, error_message):
    tool_failure[tool_name] = error_message
    tool_failure_stats[tool_name] += 1
    logger.error(f"‚ùå Tool '{tool_name}' failed: {error_message}")

def log_openai_call_time(duration_sec):
    openai_call_times.append(duration_sec)

def get_openai_call_avg_time():
    total = len(openai_call_times)
    avg = sum(openai_call_times) / total if total > 0 else 0
    return avg

def print_openai_call_stats():
    total = len(openai_call_times)
    avg = get_openai_call_avg_time()
    logger.info(f"üîÑ Total OpenAI calls: {total}, Avg time: {round(avg, 2)} sec")

def setup_logging(log_file="logs/eval.log"):
    logger = logging.getLogger("ProposalEvaluator")
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s", "%H:%M:%S")

    # Always ensure console logging
    has_console = any(isinstance(h, logging.StreamHandler) for h in logger.handlers)
    if not has_console:
        ch = logging.StreamHandler()
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    # Add file logging if not already present
    log_path = Path(log_file)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    has_file = any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_path.resolve()) for h in logger.handlers)
    if not has_file:
        fh = logging.FileHandler(log_path)
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    log_phase("Logging initialized")
    log_phase(f"Log file: {log_file}")
    return logger


def display_log(log_file="logs/eval_notebook_run.log"):
    from IPython.display import display, Markdown
    with open(log_file) as f:
        log_text = f.read()
    display(Markdown(f"```\n{log_text}\n```"))


def print_tool_success_rates():
    logger.info("üìä Tool Success Rates:")
    for tool, total_calls in tool_stats.items():
        failures = tool_failure_stats.get(tool, 0)
        success = total_calls - failures
        rate = (success / total_calls) * 100 if total_calls > 0 else 0
        logger.info(f"   {tool}: {success}/{total_calls} successful ({rate:.1f}%)")


def print_openai_call_sources():
    logger.info("üìç OpenAI Calls by Source:")
    for src, count in openai_call_sources.items():
        logger.info(f"   {src}: {count} call(s)")


def calculate_token_usage_summary(model="gpt-3.5-turbo"):
    total_prompt_tokens = sum(openai_prompt_token_usage_by_source.values())
    total_completion_tokens = sum(openai_completion_token_usage_by_source.values())
    total_tokens = total_prompt_tokens + total_completion_tokens

    # Pricing (update if you switch models)
    pricing = {
        "gpt-3.5-turbo": {"prompt": 0.0015, "completion": 0.002},
        "gpt-4": {"prompt": 0.03, "completion": 0.06},
        "gpt-4-turbo": {"prompt": 0.01, "completion": 0.03},
    }

    if model not in pricing:
        raise ValueError(f"Unknown model '{model}'. Add to pricing dict.")

    prompt_rate = pricing[model]["prompt"]
    completion_rate = pricing[model]["completion"]

    total_cost = (
        (total_prompt_tokens / 1000) * prompt_rate +
        (total_completion_tokens / 1000) * completion_rate
    )

    return {
        "model": model,
        "prompt_tokens": total_prompt_tokens,
        "completion_tokens": total_completion_tokens,
        "total_tokens": total_tokens,
        "estimated_cost_usd": round(total_cost, 4)
    }
