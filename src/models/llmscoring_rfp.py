from src.models.openai_interface import call_openai_with_tracking

def score_proposal_content_with_llm_and_tools(proposal, criterion, top_thoughts=None, triggered_tools=None, model="gpt-3.5-turbo"):
    """
    Purpose:
    Uses an LLM to score a vendor proposal based on Tree of Thought (ToT) evaluation thoughts and insights from triggered tools.

    Parameters:
    - proposal (str): The text of the vendor proposal being evaluated.
    - criterion (str): The evaluation criterion against which the proposal is being assessed.
    - top_thoughts (list of str, optional): A list of key thoughts or considerations generated during the evaluation process. Default is None.
    - tool_observations (str, optional): Observations or insights from tools that may influence the scoring. Default is None.
    - triggered_tools (list of dict, optional): A list of tool results, where each dict contains:
        - 'tool' (str): The name of the tool.
        - 'result' (str): The output or insight generated by the tool. Default is None.
    - model (str): The name of the OpenAI model to use for scoring. Default is "gpt-3.5-turbo".

    Workflow:
    1. Initializes `triggered_tools` as an empty list if not provided.
    2. Formats the tool results into a string for inclusion in the LLM prompt.
    3. Formats the top thoughts into a string for inclusion in the LLM prompt.
    4. Constructs a prompt that includes the proposal text, evaluation criterion, top thoughts, and tool insights.
    5. Sends the prompt to the OpenAI API using the `call_openai_with_tracking` function.
    6. Parses the response to extract the score and explanation.
    7. If parsing fails, defaults to a fallback score of 5 and a generic explanation.

    Returns:
    - tuple: A tuple containing:
        - score (int): The score assigned to the proposal, ranging from 1 to 10.
        - explanation (str): The reasoning behind the assigned score.
    """
    if triggered_tools is None:
        triggered_tools = []

    # Format tool output
    tool_insights = "\n".join([
        f"{tool['tool']}: {tool['result']}" for tool in triggered_tools
    ]) if triggered_tools else ""

    # Format top thoughts
    thoughts_text = ''.join(f"- {t}\n" for t in top_thoughts or [])

    prompt = f"""
You are evaluating a vendor proposal on the criterion: **{criterion}**.

Proposal:
\"\"\"
{proposal}
\"\"\"

Top Evaluation Thoughts:
{thoughts_text}

Tool Analysis:
{tool_insights}

Now, assign a score from 1 to 10 for how well the proposal addresses this criterion.
Respond in this format:
Score: X
Explanation: (your reasoning)
"""
    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_tracking(messages, model=model, temperature=0)

    try:
        lines = response.strip().split("\n")
        score_line = next((l for l in lines if "Score:" in l), "Score: 5")
        explanation = next((l for l in lines if "Explanation:" in l), "Explanation: No explanation found.")
        score = int(score_line.split(":")[1].strip())
        explanation = explanation.split(":", 1)[1].strip()
        return score, explanation
    except Exception as e:
        print(f"⚠️ Failed to parse score: {str(e)}")
        return 5, "Failed to parse explanation"
